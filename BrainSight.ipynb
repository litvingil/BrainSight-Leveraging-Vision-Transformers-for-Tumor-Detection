{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/litvingil/BrainSight-Leveraging-Vision-Transformers-for-Tumor-Detection/blob/main/BrainSight.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Install requirements"
      ],
      "metadata": {
        "id": "TeEamIVn8mps"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_EwgoY3LzYzr",
        "outputId": "a1a8e9f8-d4a7-41fb-c061-21a6358c6607"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# seed = 1234567\n",
        "seed = 9921378\n",
        "output_vit_no_aug = '/content/drive/MyDrive/colab_files/final_deep_project/vit_no_aug'\n",
        "output_vit_no_aug2 = '/content/drive/MyDrive/colab_files/final_deep_project/vit_no_aug2'\n",
        "output_vit_no_aug3 = '/content/drive/MyDrive/colab_files/final_deep_project/vit_no_aug3'\n",
        "output_vit_with_aug = '/content/drive/MyDrive/colab_files/final_deep_project/vit_with_aug'\n",
        "output_cnn_no_aug = '/content/drive/MyDrive/colab_files/final_deep_project/cnn_no_aug'\n",
        "output_cnn_with_aug = '/content/drive/MyDrive/colab_files/final_deep_project/cnn_with_aug'\n",
        "data_path = '/content/drive/MyDrive/colab_files/final_deep_project/data'"
      ],
      "metadata": {
        "id": "Is_CC0SS3vin"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Rqd722nVA6IV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f01a7873-5cec-45da-aff3-60c6a067dd69"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-2.13.1-py3-none-any.whl (486 kB)\n",
            "\u001b[?25l     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/486.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m486.2/486.2 kB\u001b[0m \u001b[31m27.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.22.4)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n",
            "Collecting dill<0.3.7,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n",
            "\u001b[?25l     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/110.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.27.1)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.65.0)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m212.5/212.5 kB\u001b[0m \u001b[31m29.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.14-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m134.3/134.3 kB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.8.4)\n",
            "Collecting huggingface-hub<1.0.0,>=0.11.0 (from datasets)\n",
            "  Downloading huggingface_hub-0.16.2-py3-none-any.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m268.5/268.5 kB\u001b[0m \u001b[31m34.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.0.12)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (4.6.3)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2023.5.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2022.7.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
            "Installing collected packages: xxhash, dill, multiprocess, huggingface-hub, datasets\n",
            "Successfully installed datasets-2.13.1 dill-0.3.6 huggingface-hub-0.16.2 multiprocess-0.70.14 xxhash-3.2.0\n",
            "Collecting transformers[torch]\n",
            "  Downloading transformers-4.30.2-py3-none-any.whl (7.2 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m101.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (3.12.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.16.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.27.1)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers[torch])\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m110.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers[torch])\n",
            "  Downloading safetensors-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m90.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (4.65.0)\n",
            "Requirement already satisfied: torch!=1.12.0,>=1.9 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.0.1+cu118)\n",
            "Collecting accelerate>=0.20.2 (from transformers[torch])\n",
            "  Downloading accelerate-0.20.3-py3-none-any.whl (227 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m227.6/227.6 kB\u001b[0m \u001b[31m33.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.20.2->transformers[torch]) (5.9.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers[torch]) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers[torch]) (4.6.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.9->transformers[torch]) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.9->transformers[torch]) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.9->transformers[torch]) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.9->transformers[torch]) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch!=1.12.0,>=1.9->transformers[torch]) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch!=1.12.0,>=1.9->transformers[torch]) (16.0.6)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch!=1.12.0,>=1.9->transformers[torch]) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch!=1.12.0,>=1.9->transformers[torch]) (1.3.0)\n",
            "Installing collected packages: tokenizers, safetensors, transformers, accelerate\n",
            "Successfully installed accelerate-0.20.3 safetensors-0.3.1 tokenizers-0.13.3 transformers-4.30.2\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (23.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.0.1+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->accelerate) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->accelerate) (4.6.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->accelerate) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->accelerate) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->accelerate) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->accelerate) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->accelerate) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->accelerate) (16.0.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6.0->accelerate) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->accelerate) (1.3.0)\n",
            "Collecting wandb\n",
            "  Downloading wandb-0.15.5-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m51.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.3)\n",
            "Collecting GitPython!=3.1.29,>=1.0.0 (from wandb)\n",
            "  Downloading GitPython-3.1.31-py3-none-any.whl (184 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m184.3/184.3 kB\u001b[0m \u001b[31m28.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.27.1)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Collecting sentry-sdk>=1.0.0 (from wandb)\n",
            "  Downloading sentry_sdk-1.27.0-py2.py3-none-any.whl (211 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m211.3/211.3 kB\u001b[0m \u001b[31m29.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docker-pycreds>=0.4.0 (from wandb)\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0)\n",
            "Collecting pathtools (from wandb)\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting setproctitle (from wandb)\n",
            "  Downloading setproctitle-1.3.2-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (67.7.2)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.4.4)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Collecting gitdb<5,>=4.0.1 (from GitPython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading gitdb-4.0.10-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.4)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Building wheels for collected packages: pathtools\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8791 sha256=ba3a81cf3e44bde7468e7b7d9ac8a15725cd3397f883105ac54f68d9a189a036\n",
            "  Stored in directory: /root/.cache/pip/wheels/e7/f3/22/152153d6eb222ee7a56ff8617d80ee5207207a8c00a7aab794\n",
            "Successfully built pathtools\n",
            "Installing collected packages: pathtools, smmap, setproctitle, sentry-sdk, docker-pycreds, gitdb, GitPython, wandb\n",
            "Successfully installed GitPython-3.1.31 docker-pycreds-0.4.0 gitdb-4.0.10 pathtools-0.1.2 sentry-sdk-1.27.0 setproctitle-1.3.2 smmap-5.0.0 wandb-0.15.5\n",
            "Requirement already satisfied: imgaug in /usr/local/lib/python3.10/dist-packages (0.4.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from imgaug) (1.16.0)\n",
            "Requirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.10/dist-packages (from imgaug) (1.22.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from imgaug) (1.10.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from imgaug) (8.4.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from imgaug) (3.7.1)\n",
            "Requirement already satisfied: scikit-image>=0.14.2 in /usr/local/lib/python3.10/dist-packages (from imgaug) (0.19.3)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (from imgaug) (4.7.0.72)\n",
            "Requirement already satisfied: imageio in /usr/local/lib/python3.10/dist-packages (from imgaug) (2.25.1)\n",
            "Requirement already satisfied: Shapely in /usr/local/lib/python3.10/dist-packages (from imgaug) (2.0.1)\n",
            "Requirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.14.2->imgaug) (3.1)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.14.2->imgaug) (2023.4.12)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.14.2->imgaug) (1.4.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.14.2->imgaug) (23.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->imgaug) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->imgaug) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->imgaug) (4.40.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->imgaug) (1.4.4)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->imgaug) (3.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->imgaug) (2.8.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets\n",
        "!pip install transformers[torch]\n",
        "!pip install accelerate -U\n",
        "!pip install wandb\n",
        "\n",
        "!pip install imgaug"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load the dataset"
      ],
      "metadata": {
        "id": "e8pOlK68_21b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "\n",
        "# Create the required directories\n",
        "os.makedirs(data_path + '/brain_tumor_dataset', exist_ok=True)\n",
        "os.makedirs(data_path + '/brain_dataset/train', exist_ok=True)\n",
        "os.makedirs(data_path +'/brain_dataset/test', exist_ok=True)\n",
        "\n",
        "# Names of tumor types\n",
        "tumor_types = ['meningioma','glioma','pituitary tumor']\n",
        "\n",
        "# Create directories for each tumor type in both train and test sets\n",
        "for tumor_type in tumor_types:\n",
        "  os.makedirs(data_path + f'/brain_dataset/test/{tumor_type}', exist_ok=True)\n",
        "  os.makedirs(data_path + f'/brain_dataset/train/{tumor_type}', exist_ok=True)\n",
        "\n",
        "# Install necessary Python libraries\n",
        "!pip install hdf5storage\n",
        "\n",
        "\n",
        "# Change directory\n",
        "%cd $data_path'/brain_tumor_dataset'\n",
        "\n",
        "# Download the dataset\n",
        "!wget https://ndownloader.figshare.com/articles/1512427/versions/5\n",
        "\n",
        "# Unzip the dataset and delete the zip\n",
        "!unzip 5 && rm 5\n",
        "\n",
        "# Concatenate the multiple zipped data in a single zip\n",
        "!cat brainTumorDataPublic_* > brainTumorDataPublic_temp.zip\n",
        "!zip -FF brainTumorDataPublic_temp.zip --out data.zip\n",
        "\n",
        "# Remove the temporary files\n",
        "!rm brainTumorDataPublic_*\n",
        "\n",
        "# Unzip the full archive and delete it\n",
        "!unzip data.zip -d data && rm data.zip\n",
        "\n",
        "# Check that \"data\" contains 3064 files\n",
        "!ls data | wc -l\n",
        "\n",
        "# Change directory to previous\n",
        "%cd ..\n",
        "\n",
        "# Clone the GitHub repository containing the script for converting the MATLAB files to NumPy\n",
        "!git clone https://github.com/guillaumefrd/brain-tumor-mri-dataset.git\n",
        "!cp brain-tumor-mri-dataset/matlab_to_numpy.py $data_path\n",
        "\n",
        "# Run the Python script to convert the MATLAB files to NumPy\n",
        "!python $data_path'/matlab_to_numpy.py' $data_path'/brain_tumor_dataset'"
      ],
      "metadata": {
        "id": "KhjuUITtAElJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load labels, images, and masks\n",
        "%cd $data_path'/brain_tumor_dataset'\n",
        "\n",
        "labels = np.load('labels.npy') - 1  # Subtracting 1 from labels to make them 0-indexed\n",
        "images = np.load('images.npy')\n",
        "masks = np.load('masks.npy')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MJ51t2JuBkuY",
        "outputId": "07926976-2d94-41f2-8b91-e4eba82992ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/colab_files/final_deep_project/data/brain_tumor_dataset\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "# Normalize images and convert to PIL format\n",
        "def normalize_and_convert_images_to_pil(images):\n",
        "  normalized_pil_images = []\n",
        "  for image in images:\n",
        "    normalized_image = (image - np.min(image)) * (255.0 / (np.max(image) - np.min(image)))\n",
        "    normalized_pil_images.append(Image.fromarray(normalized_image.astype(np.uint8)).convert('RGB'))\n",
        "  return normalized_pil_images"
      ],
      "metadata": {
        "id": "t0O4JO__Dgxo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the dataset into train and test\n",
        "train_test_split_ratio = 0.8\n",
        "num_samples = labels.shape[0]\n",
        "indices = np.random.RandomState(seed).permutation(num_samples)\n",
        "train_indices = indices[:int(num_samples * train_test_split_ratio)]\n",
        "test_indices = indices[int(num_samples * train_test_split_ratio):]"
      ],
      "metadata": {
        "id": "Z44H99xMDguP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert images to PIL format\n",
        "pil_images = normalize_and_convert_images_to_pil(images)"
      ],
      "metadata": {
        "id": "nBFH973FDgrn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save images to respective directories\n",
        "index_to_tumor_type = {i: tumor_type for i, tumor_type in enumerate(tumor_types)}\n",
        "for train_index in train_indices:\n",
        "  pil_images[train_index].save(data_path + f'/brain_dataset/train/{index_to_tumor_type[labels[train_index]]}/{train_index}.png')\n",
        "for test_index in test_indices:\n",
        "  pil_images[test_index].save(data_path + f'/brain_dataset/test/{index_to_tumor_type[labels[test_index]]}/{test_index}.png')"
      ],
      "metadata": {
        "id": "fewWPkFTDgow"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "MmThcO_M_z0_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "NC1eyP1tWN6h"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "from datasets import load_dataset, load_metric\n",
        "\n",
        "from transformers import TrainingArguments, ViTFeatureExtractor, ViTForImageClassification, Trainer\n",
        "\n",
        "import wandb\n",
        "\n",
        "# Importing required libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms\n",
        "from torchvision.transforms import Lambda\n",
        "from torchvision.datasets import ImageFolder\n",
        "from datasets import load_metric\n",
        "from transformers import TrainingArguments, Trainer, ViTForImageClassification, ViTFeatureExtractor\n",
        "from accelerate import Accelerator\n",
        "import imgaug.augmenters as iaa\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "labels_names = ['meningioma','glioma','pituitary tumor']"
      ],
      "metadata": {
        "id": "-q5MwX8ZR3qO"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Helper Functions"
      ],
      "metadata": {
        "id": "RmWpNS1r7rTj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def collate_fn(batch):\n",
        "    return {\n",
        "        'pixel_values': torch.stack([x['pixel_values'] for x in batch]),\n",
        "        'labels': torch.tensor([x['labels'] for x in batch])\n",
        "    }\n",
        "\n",
        "metric = load_metric(\"accuracy\")\n",
        "\n",
        "def compute_metrics(p):\n",
        "    return metric.compute(predictions=np.argmax(p.predictions, axis=1), references=p.label_ids)"
      ],
      "metadata": {
        "id": "bu6EiBzg8Zcg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 133,
          "referenced_widgets": [
            "c274518a78be489c8e37f2d713adce2e",
            "66ceb6d5524248d9a321cd6a6014c2ff",
            "b1fef4bc421d465caa1dbeb97a65bba3",
            "6f8e010856cd4940b859b5d1a06ad0a3",
            "a9f7f4897e95436897024426d20d324e",
            "7bfc198d6c554b219865e00e2ba5cc45",
            "058757c72182488e8e5ee9db427adc81",
            "b281060b8107458bac092186336962d9",
            "0a1492b49ca749b38f43804366a93761",
            "b6e37bdf317b4f1d82afd5cd1d276dbf",
            "376501286ad042419cd58399eef26929"
          ]
        },
        "outputId": "d178dd48-483c-4332-b701-23578eb41d2b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-25e90a6ff46d>:7: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ü§ó Evaluate: https://huggingface.co/docs/evaluate\n",
            "  metric = load_metric(\"accuracy\")\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading builder script:   0%|          | 0.00/1.65k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c274518a78be489c8e37f2d713adce2e"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#confusion matrix functions\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import pandas as pd\n",
        "from sklearn import metrics as met\n",
        "\n",
        "def compute_metrics(p):\n",
        "    preds = np.argmax(p.predictions, axis=1)\n",
        "    cm = confusion_matrix(p.label_ids, preds)\n",
        "    return cm\n",
        "\n",
        "def print_confusion_matrix(cm, labels_names):\n",
        "    df_cm = pd.DataFrame(cm, index=labels_names, columns=labels_names)\n",
        "    print(df_cm)\n"
      ],
      "metadata": {
        "id": "ZH-Zvk0dH3a9"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ViT\n"
      ],
      "metadata": {
        "id": "Usu2m9bO2ZZW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transforms"
      ],
      "metadata": {
        "id": "wJMVzli1Fs0D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the augmentation pipeline with imgaug\n",
        "augmenter = iaa.Sequential([\n",
        "    iaa.Fliplr(0.5),  # horizontally flip with probability of 0.5\n",
        "    iaa.Sometimes(0.3, iaa.GaussianBlur((0, 1.0))),  # apply Gaussian blur with probability of 0.3\n",
        "    iaa.Sometimes(0.5, iaa.SaltAndPepper(0.05)),  # Add Salt and Pepper noise with probability of 0.5\n",
        "    iaa.Sometimes(0.3, iaa.AdditivePoissonNoise(lam=(0, 30))),  # Add Poisson noise with probability of 0.3\n",
        "])\n",
        "\n",
        "# imgaug works with numpy images (HWC) but torchvision with PIL images, so we have to convert\n",
        "imgaug_transform = Lambda(lambda img: augmenter.augment_image(np.array(img)))"
      ],
      "metadata": {
        "id": "sK4IDdTqF8hz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define transformations for the train dataset (without augmentation)\n",
        "train_transforms_without_aug = transforms.Compose([\n",
        "    transforms.Resize((224, 224),interpolation=Image.BILINEAR),  # Resize 512x512 images to 224x224 using bilinear interpolation\n",
        "    transforms.ToTensor(),  # Convert image to PyTorch tensor and automatically rescale to [0,1] by dividing by 255\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize image tensor to range [-1,1]\n",
        "])\n",
        "\n",
        "# Define transformations for the train dataset (including augmentation)\n",
        "train_transforms_with_aug = transforms.Compose([\n",
        "    transforms.Resize((224, 224),interpolation=Image.BILINEAR),  # Resize 512x512 images to 224x224 using bilinear interpolation\n",
        "    imgaug_transform,\n",
        "    transforms.ToTensor(),  # Convert image to PyTorch tensor and automatically rescale to [0,1] by dividing by 255\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize image tensor to range [-1,1]\n",
        "])\n",
        "\n",
        "# Define transformations for the test dataset\n",
        "test_transforms = transforms.Compose([\n",
        "    transforms.Resize((224, 224),interpolation=Image.BILINEAR),  # Resize 512x512 images to 224x224 using bilinear interpolation\n",
        "    transforms.ToTensor(),  # Convert image to PyTorch tensor and automatically rescale to [0,1] by dividing by 255\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize image tensor to range [-1,1]\n",
        "])"
      ],
      "metadata": {
        "id": "KDlpGaAUFwW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Without augmentation"
      ],
      "metadata": {
        "id": "_rH5oLzJP7sK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ViT_base = 'google/vit-base-patch16-224-in21k'\n",
        "\n",
        "# Load the datasets from folders and process the dataset with the transform\n",
        "train_dataset = ImageFolder(data_path + '/brain_dataset/train', transform=train_transforms_without_aug)\n",
        "test_dataset = ImageFolder(data_path + '/brain_dataset/test', transform=test_transforms)"
      ],
      "metadata": {
        "id": "J0pZnj1BQLn7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "1xY3XqFfZQ2t",
        "outputId": "c95dc161-80b5-4ae1-c15f-0ea48b8ac2d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at google/vit-base-patch16-224-in21k were not used when initializing ViTForImageClassification: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "- This IS expected if you are initializing ViTForImageClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing ViTForImageClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mlitvingil\u001b[0m (\u001b[33mdeep_bekeif\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.15.4"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20230702_101819-vis2i7xd</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/deep_bekeif/vit_no_aug/runs/vis2i7xd' target=\"_blank\">polished-flower-13</a></strong> to <a href='https://wandb.ai/deep_bekeif/vit_no_aug' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/deep_bekeif/vit_no_aug' target=\"_blank\">https://wandb.ai/deep_bekeif/vit_no_aug</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/deep_bekeif/vit_no_aug/runs/vis2i7xd' target=\"_blank\">https://wandb.ai/deep_bekeif/vit_no_aug/runs/vis2i7xd</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at google/vit-base-patch16-224-in21k were not used when initializing ViTForImageClassification: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "- This IS expected if you are initializing ViTForImageClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing ViTForImageClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "# define the model\n",
        "def model_init():\n",
        "  return ViTForImageClassification.from_pretrained(\n",
        "    ViT_base,\n",
        "    num_labels=len(labels_names),\n",
        "    id2label={str(i): tumor_type for i, tumor_type in enumerate(labels_names)},\n",
        "    label2id={tumor_type: i for i, tumor_type in enumerate(labels_names)},\n",
        ")\n",
        "\n",
        "model = model_init()\n",
        "\n",
        "# Initialize a new run\n",
        "wandb.init(project=\"vit_no_aug\")\n",
        "\n",
        "# Define the TrainingArguments\n",
        "training_args = TrainingArguments(\n",
        "  output_dir=output_vit_no_aug,\n",
        "  per_device_train_batch_size=16,\n",
        "  evaluation_strategy=\"steps\",\n",
        "  num_train_epochs=5,\n",
        "  fp16=True,\n",
        "  save_steps=100,#100\n",
        "  eval_steps=50,#100\n",
        "  logging_steps=10,\n",
        "  learning_rate=2e-4,\n",
        "  save_total_limit=2,\n",
        "  remove_unused_columns=False,\n",
        "  push_to_hub=False,\n",
        "  report_to='wandb',\n",
        "  load_best_model_at_end=True,\n",
        "  seed = seed\n",
        ")\n",
        "\n",
        "\n",
        "trainer = Trainer(\n",
        "    model_init=model_init,\n",
        "    args=training_args,\n",
        "    compute_metrics=compute_metrics,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset,\n",
        "    data_collator= lambda samples: {'pixel_values': torch.stack([sample[0] for sample in samples]),\n",
        "                                   'labels': torch.tensor([sample[1] for sample in samples])},\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#final\n",
        "train_results = trainer.train()\n",
        "trainer.save_model()\n",
        "trainer.log_metrics(\"train\", train_results.metrics)\n",
        "trainer.save_metrics(\"train\", train_results.metrics)\n",
        "trainer.save_state()\n",
        "\n",
        "wandb.finish()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "34eo5Uu-HIe8",
        "outputId": "f481a6f3-bc5e-40fc-acdf-7c16afa4304a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at google/vit-base-patch16-224-in21k were not used when initializing ViTForImageClassification: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "- This IS expected if you are initializing ViTForImageClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing ViTForImageClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='770' max='770' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [770/770 17:33, Epoch 5/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.208500</td>\n",
              "      <td>0.362267</td>\n",
              "      <td>0.872757</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.173500</td>\n",
              "      <td>0.135880</td>\n",
              "      <td>0.949429</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>0.123100</td>\n",
              "      <td>0.141044</td>\n",
              "      <td>0.952692</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.076000</td>\n",
              "      <td>0.142741</td>\n",
              "      <td>0.947798</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>0.100600</td>\n",
              "      <td>0.249677</td>\n",
              "      <td>0.941272</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>0.086400</td>\n",
              "      <td>0.120581</td>\n",
              "      <td>0.964111</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>350</td>\n",
              "      <td>0.070100</td>\n",
              "      <td>0.150766</td>\n",
              "      <td>0.955954</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>0.056400</td>\n",
              "      <td>0.152167</td>\n",
              "      <td>0.960848</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>450</td>\n",
              "      <td>0.032600</td>\n",
              "      <td>0.143290</td>\n",
              "      <td>0.962480</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.004700</td>\n",
              "      <td>0.135229</td>\n",
              "      <td>0.967374</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>550</td>\n",
              "      <td>0.015200</td>\n",
              "      <td>0.139668</td>\n",
              "      <td>0.967374</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>0.004900</td>\n",
              "      <td>0.119744</td>\n",
              "      <td>0.967374</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>650</td>\n",
              "      <td>0.003600</td>\n",
              "      <td>0.133235</td>\n",
              "      <td>0.967374</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>0.003300</td>\n",
              "      <td>0.117723</td>\n",
              "      <td>0.969005</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>750</td>\n",
              "      <td>0.003200</td>\n",
              "      <td>0.113195</td>\n",
              "      <td>0.972268</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "***** train metrics *****\n",
            "  epoch                    =         5.0\n",
            "  total_flos               = 884451758GF\n",
            "  train_loss               =      0.0985\n",
            "  train_runtime            =  0:17:38.38\n",
            "  train_samples_per_second =      11.579\n",
            "  train_steps_per_second   =       0.728\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>‚ñÅ‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>eval/loss</td><td>‚ñà‚ñÇ‚ñÇ‚ñÇ‚ñÖ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ</td></tr><tr><td>eval/runtime</td><td>‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>eval/samples_per_second</td><td>‚ñÅ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>eval/steps_per_second</td><td>‚ñÅ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>train/epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>train/global_step</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>train/learning_rate</td><td>‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>train/loss</td><td>‚ñà‚ñÜ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>train/total_flos</td><td>‚ñÅ</td></tr><tr><td>train/train_loss</td><td>‚ñÅ</td></tr><tr><td>train/train_runtime</td><td>‚ñÅ</td></tr><tr><td>train/train_samples_per_second</td><td>‚ñÅ</td></tr><tr><td>train/train_steps_per_second</td><td>‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>0.97227</td></tr><tr><td>eval/loss</td><td>0.1132</td></tr><tr><td>eval/runtime</td><td>8.3325</td></tr><tr><td>eval/samples_per_second</td><td>73.568</td></tr><tr><td>eval/steps_per_second</td><td>9.241</td></tr><tr><td>train/epoch</td><td>5.0</td></tr><tr><td>train/global_step</td><td>770</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.0031</td></tr><tr><td>train/total_flos</td><td>9.496728442678579e+17</td></tr><tr><td>train/train_loss</td><td>0.09847</td></tr><tr><td>train/train_runtime</td><td>1058.384</td></tr><tr><td>train/train_samples_per_second</td><td>11.579</td></tr><tr><td>train/train_steps_per_second</td><td>0.728</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">polished-flower-13</strong> at: <a href='https://wandb.ai/deep_bekeif/vit_no_aug/runs/vis2i7xd' target=\"_blank\">https://wandb.ai/deep_bekeif/vit_no_aug/runs/vis2i7xd</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20230702_101819-vis2i7xd/logs</code>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#display Confusion Matrix\n",
        "pred = trainer.predict(test_dataset)\n",
        "\n",
        "cm_display = met.ConfusionMatrixDisplay(confusion_matrix = compute_metrics(pred), display_labels = labels_names)\n",
        "\n",
        "cm_display.plot()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "id": "-5lJDHQYIDyT",
        "outputId": "b7c601a6-4f10-4ccb-b789-8ce07b66a6ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## With augmentation"
      ],
      "metadata": {
        "id": "GtJNGP6yS9I8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ViT_base = 'google/vit-base-patch16-224-in21k'\n",
        "\n",
        "# Load the datasets from folders and process the dataset with the transform\n",
        "train_dataset = ImageFolder(data_path + '/brain_dataset/train', transform=train_transforms_with_aug)\n",
        "test_dataset = ImageFolder(data_path + '/brain_dataset/test', transform=test_transforms)"
      ],
      "metadata": {
        "id": "6z16QIvITNFZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define the model\n",
        "model = ViTForImageClassification.from_pretrained(\n",
        "    ViT_base,\n",
        "    num_labels=len(labels_names),\n",
        "    id2label={str(i): c for i, c in enumerate(labels_names)},\n",
        "    label2id={c: str(i) for i, c in enumerate(labels_names)}\n",
        ")\n",
        "\n",
        "# Initialize a new run\n",
        "wandb.init(project=\"vit_with_aug\")\n",
        "\n",
        "# Define the TrainingArguments\n",
        "training_args = TrainingArguments(\n",
        "  output_dir=output_vit_with_aug,\n",
        "  per_device_train_batch_size=16,\n",
        "  evaluation_strategy=\"steps\",\n",
        "  num_train_epochs=5,\n",
        "  fp16=True,\n",
        "  save_steps=100,#100\n",
        "  eval_steps=50,#100\n",
        "  logging_steps=10,\n",
        "  learning_rate=2e-4,\n",
        "  save_total_limit=2,\n",
        "  remove_unused_columns=False,\n",
        "  push_to_hub=False,\n",
        "  report_to='wandb',\n",
        "  load_best_model_at_end=True,\n",
        "  seed = seed\n",
        ")\n",
        "\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    compute_metrics=compute_metrics,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset,\n",
        "    data_collator= lambda samples: {'pixel_values': torch.stack([sample[0] for sample in samples]),\n",
        "                                   'labels': torch.tensor([sample[1] for sample in samples])},\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 214
        },
        "id": "02IbTxwrTIkD",
        "outputId": "2ebfb171-6d80-4a2f-e7b0-73907ede86f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at google/vit-base-patch16-224-in21k were not used when initializing ViTForImageClassification: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "- This IS expected if you are initializing ViTForImageClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing ViTForImageClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.15.4"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20230702_104241-nk1vo8nc</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/deep_bekeif/vit_with_aug/runs/nk1vo8nc' target=\"_blank\">fine-flower-2</a></strong> to <a href='https://wandb.ai/deep_bekeif/vit_with_aug' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/deep_bekeif/vit_with_aug' target=\"_blank\">https://wandb.ai/deep_bekeif/vit_with_aug</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/deep_bekeif/vit_with_aug/runs/nk1vo8nc' target=\"_blank\">https://wandb.ai/deep_bekeif/vit_with_aug/runs/nk1vo8nc</a>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_results = trainer.train()\n",
        "trainer.save_model()\n",
        "trainer.log_metrics(\"train\", train_results.metrics)\n",
        "trainer.save_metrics(\"train\", train_results.metrics)\n",
        "trainer.save_state()\n",
        "\n",
        "wandb.finish()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "7b0b645b32144ccf810f3a15796ef66c",
            "1e4a25437b634ef894b21972431432b4",
            "e936fd6952fe41dca9ffe96a6450787b",
            "06ce33f8bd3048248a7076bd1515d46b",
            "7a130b04a5ce4dce99eb61334816e362",
            "a08c3126a6684d97a0c2f6517ade5eed",
            "19061d6836ed4fa697b1f8697e7d46d9",
            "9b5bf20b83cc4d7496466d045034ea9d"
          ]
        },
        "id": "595CF1SN-J5g",
        "outputId": "cc42d525-53d7-470b-9bb6-bc16d4258295"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='770' max='770' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [770/770 06:15, Epoch 5/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.349200</td>\n",
              "      <td>0.279296</td>\n",
              "      <td>0.913540</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.283300</td>\n",
              "      <td>0.200849</td>\n",
              "      <td>0.929853</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>0.303900</td>\n",
              "      <td>0.235043</td>\n",
              "      <td>0.929853</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.166300</td>\n",
              "      <td>0.137872</td>\n",
              "      <td>0.959217</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>0.174900</td>\n",
              "      <td>0.143445</td>\n",
              "      <td>0.955954</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>0.112500</td>\n",
              "      <td>0.131100</td>\n",
              "      <td>0.957586</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>350</td>\n",
              "      <td>0.087700</td>\n",
              "      <td>0.138253</td>\n",
              "      <td>0.959217</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>0.154600</td>\n",
              "      <td>0.116634</td>\n",
              "      <td>0.960848</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>450</td>\n",
              "      <td>0.084500</td>\n",
              "      <td>0.106484</td>\n",
              "      <td>0.964111</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.057300</td>\n",
              "      <td>0.093910</td>\n",
              "      <td>0.965742</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>550</td>\n",
              "      <td>0.096400</td>\n",
              "      <td>0.097435</td>\n",
              "      <td>0.972268</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>0.015100</td>\n",
              "      <td>0.093249</td>\n",
              "      <td>0.969005</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>650</td>\n",
              "      <td>0.039400</td>\n",
              "      <td>0.098203</td>\n",
              "      <td>0.970636</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>0.033700</td>\n",
              "      <td>0.092190</td>\n",
              "      <td>0.969005</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>750</td>\n",
              "      <td>0.008000</td>\n",
              "      <td>0.098421</td>\n",
              "      <td>0.969005</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "***** train metrics *****\n",
            "  epoch                    =         5.0\n",
            "  total_flos               = 884451758GF\n",
            "  train_loss               =      0.1763\n",
            "  train_runtime            =  0:06:15.88\n",
            "  train_samples_per_second =      32.603\n",
            "  train_steps_per_second   =       2.048\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(Label(value='0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max‚Ä¶"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7b0b645b32144ccf810f3a15796ef66c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>‚ñÅ‚ñÉ‚ñÉ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>eval/loss</td><td>‚ñà‚ñÖ‚ñÜ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>eval/runtime</td><td>‚ñÑ‚ñÇ‚ñÉ‚ñÉ‚ñà‚ñÅ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÑ</td></tr><tr><td>eval/samples_per_second</td><td>‚ñÖ‚ñÜ‚ñÖ‚ñÖ‚ñÅ‚ñà‚ñá‚ñÜ‚ñá‚ñá‚ñÜ‚ñá‚ñá‚ñÜ‚ñÖ</td></tr><tr><td>eval/steps_per_second</td><td>‚ñÖ‚ñÜ‚ñÖ‚ñÖ‚ñÅ‚ñà‚ñá‚ñÜ‚ñá‚ñá‚ñÜ‚ñá‚ñá‚ñÜ‚ñÖ</td></tr><tr><td>train/epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>train/global_step</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>train/learning_rate</td><td>‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>train/loss</td><td>‚ñà‚ñÜ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>train/total_flos</td><td>‚ñÅ</td></tr><tr><td>train/train_loss</td><td>‚ñÅ</td></tr><tr><td>train/train_runtime</td><td>‚ñÅ</td></tr><tr><td>train/train_samples_per_second</td><td>‚ñÅ</td></tr><tr><td>train/train_steps_per_second</td><td>‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>0.969</td></tr><tr><td>eval/loss</td><td>0.09842</td></tr><tr><td>eval/runtime</td><td>8.3177</td></tr><tr><td>eval/samples_per_second</td><td>73.698</td></tr><tr><td>eval/steps_per_second</td><td>9.257</td></tr><tr><td>train/epoch</td><td>5.0</td></tr><tr><td>train/global_step</td><td>770</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.0336</td></tr><tr><td>train/total_flos</td><td>9.496728442678579e+17</td></tr><tr><td>train/train_loss</td><td>0.17629</td></tr><tr><td>train/train_runtime</td><td>375.8868</td></tr><tr><td>train/train_samples_per_second</td><td>32.603</td></tr><tr><td>train/train_steps_per_second</td><td>2.048</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">fine-flower-2</strong> at: <a href='https://wandb.ai/deep_bekeif/vit_with_aug/runs/nk1vo8nc' target=\"_blank\">https://wandb.ai/deep_bekeif/vit_with_aug/runs/nk1vo8nc</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20230702_104241-nk1vo8nc/logs</code>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#display Confusion Matrix\n",
        "pred = trainer.predict(test_dataset)\n",
        "\n",
        "cm_display = met.ConfusionMatrixDisplay(confusion_matrix = compute_metrics(pred), display_labels = labels_names)\n",
        "\n",
        "cm_display.plot()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "id": "wruMfxToIKyP",
        "outputId": "85a70048-7019-4d55-b932-966e1ce61a45"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load pretrain\n"
      ],
      "metadata": {
        "id": "5C9ZT2fPUbqc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import ViTForImageClassification\n",
        "model = ViTForImageClassification.from_pretrained('/content/drive/MyDrive/colab_files/brain_tumor_dataset/vit-model')"
      ],
      "metadata": {
        "id": "2SKRUkjkT4GC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4cZvzMmILYsi"
      },
      "source": [
        "# CNN\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transforms"
      ],
      "metadata": {
        "id": "rpZxcrVvFepQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "mXq6QZZwOsso"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the augmentation pipeline with imgaug\n",
        "augmenter = iaa.Sequential([\n",
        "    iaa.Fliplr(0.5),  # horizontally flip with probability of 0.5\n",
        "    iaa.Sometimes(0.3, iaa.GaussianBlur((0, 1.0))),  # apply Gaussian blur with probability of 0.3\n",
        "    iaa.Sometimes(0.5, iaa.SaltAndPepper(0.05)),  # Add Salt and Pepper noise with probability of 0.5\n",
        "    iaa.Sometimes(0.3, iaa.AdditivePoissonNoise(lam=(0, 30))),  # Add Poisson noise with probability of 0.3\n",
        "])\n",
        "\n",
        "# imgaug works with numpy images (HWC) but torchvision with PIL images, so we have to convert\n",
        "imgaug_transform = Lambda(lambda img: augmenter.augment_image(np.array(img)))"
      ],
      "metadata": {
        "id": "BVTUV3wuGBkj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define transformations for the train and test datasets\n",
        "# Define the cropping percentage\n",
        "crop_percentage = 0.875\n",
        "\n",
        "# Define transformations for the train dataset\n",
        "train_transforms_without_aug = transforms.Compose([\n",
        "    transforms.CenterCrop((int(512 * crop_percentage),int(512 * crop_percentage))),\n",
        "    transforms.Resize((224, 224),interpolation=Image.BICUBIC), # Resize 508x508 images to 224x224 using BICUBIC interpolation\n",
        "    transforms.ToTensor(), # Convert image to PyTorch tensor and automatically rescale to [0,1] by dividing by 255\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "# Define transformations for the train dataset (including augmentation)\n",
        "train_transforms_with_aug = transforms.Compose([\n",
        "    transforms.CenterCrop((int(512 * crop_percentage),int(512 * crop_percentage))),\n",
        "    transforms.Resize((224, 224),interpolation=Image.BICUBIC), # Resize 508x508 images to 224x224 using BICUBIC interpolation\n",
        "    imgaug_transform,\n",
        "    transforms.ToTensor(), # Convert image to PyTorch tensor and automatically rescale to [0,1] by dividing by 255\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "\n",
        "# Define transformations for the test dataset\n",
        "\n",
        "test_transforms_without_aug = transforms.Compose([\n",
        "    transforms.CenterCrop((int(512 * crop_percentage),int(512 * crop_percentage))),\n",
        "    transforms.Resize((224, 224),interpolation=Image.BICUBIC), # Resize 508x508 images to 224x224 using BICUBIC interpolation\n",
        "    transforms.ToTensor(), # Convert image to PyTorch tensor and automatically rescale to [0,1] by dividing by 255\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n"
      ],
      "metadata": {
        "id": "-tN_vpAcFhFS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Without augmentation"
      ],
      "metadata": {
        "id": "krzSz6X_UuhZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X5JCFIgER-Do"
      },
      "outputs": [],
      "source": [
        "CNN_base = 'microsoft/resnet-50'\n",
        "\n",
        "# Load the datasets from folders and process the dataset with the transform\n",
        "train_dataset = ImageFolder(data_path + '/brain_dataset/train', transform=train_transforms_without_aug)\n",
        "test_dataset = ImageFolder(data_path + '/brain_dataset/test', transform=test_transforms_without_aug)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "id": "cqPxTR5DQTT7",
        "outputId": "cc83f746-afc0-4c63-e060-cacd6d4df551"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You are using a model of type resnet to instantiate a model of type vit. This is not supported for all configurations of models and can yield errors.\n",
            "Some weights of the model checkpoint at microsoft/resnet-50 were not used when initializing ViTForImageClassification: ['resnet.embedder.embedder.normalization.running_var', 'resnet.encoder.stages.3.layers.2.layer.1.normalization.running_mean', 'resnet.encoder.stages.1.layers.0.layer.0.normalization.num_batches_tracked', 'resnet.encoder.stages.2.layers.3.layer.0.normalization.running_mean', 'resnet.encoder.stages.1.layers.3.layer.2.normalization.weight', 'resnet.encoder.stages.1.layers.3.layer.1.convolution.weight', 'resnet.encoder.stages.2.layers.1.layer.2.normalization.num_batches_tracked', 'resnet.encoder.stages.2.layers.0.layer.1.normalization.running_var', 'resnet.encoder.stages.2.layers.2.layer.2.normalization.running_mean', 'resnet.encoder.stages.3.layers.2.layer.0.normalization.bias', 'resnet.encoder.stages.1.layers.1.layer.0.normalization.running_var', 'resnet.embedder.embedder.convolution.weight', 'resnet.embedder.embedder.normalization.weight', 'resnet.encoder.stages.3.layers.2.layer.0.normalization.weight', 'resnet.encoder.stages.2.layers.0.layer.1.normalization.bias', 'resnet.encoder.stages.2.layers.4.layer.0.normalization.running_mean', 'resnet.encoder.stages.0.layers.2.layer.0.normalization.running_mean', 'resnet.encoder.stages.0.layers.0.layer.2.normalization.running_mean', 'resnet.encoder.stages.1.layers.1.layer.1.normalization.running_var', 'resnet.encoder.stages.1.layers.0.layer.0.convolution.weight', 'resnet.encoder.stages.1.layers.3.layer.0.normalization.bias', 'resnet.encoder.stages.3.layers.2.layer.2.normalization.bias', 'resnet.encoder.stages.2.layers.4.layer.1.normalization.running_mean', 'resnet.encoder.stages.3.layers.0.layer.1.normalization.num_batches_tracked', 'resnet.encoder.stages.3.layers.2.layer.1.normalization.running_var', 'resnet.encoder.stages.0.layers.1.layer.1.normalization.weight', 'resnet.encoder.stages.3.layers.1.layer.1.normalization.bias', 'resnet.encoder.stages.2.layers.5.layer.0.convolution.weight', 'resnet.encoder.stages.0.layers.2.layer.1.normalization.running_mean', 'resnet.encoder.stages.0.layers.1.layer.2.normalization.running_mean', 'resnet.encoder.stages.0.layers.0.layer.0.normalization.num_batches_tracked', 'resnet.encoder.stages.1.layers.1.layer.1.normalization.bias', 'resnet.encoder.stages.3.layers.0.layer.0.normalization.bias', 'resnet.encoder.stages.2.layers.4.layer.2.convolution.weight', 'resnet.encoder.stages.0.layers.1.layer.2.normalization.bias', 'resnet.encoder.stages.2.layers.5.layer.2.normalization.num_batches_tracked', 'resnet.encoder.stages.0.layers.0.layer.1.normalization.num_batches_tracked', 'resnet.encoder.stages.3.layers.0.layer.2.normalization.num_batches_tracked', 'resnet.encoder.stages.2.layers.1.layer.0.normalization.num_batches_tracked', 'resnet.encoder.stages.3.layers.1.layer.1.normalization.running_mean', 'resnet.encoder.stages.1.layers.0.layer.1.normalization.bias', 'resnet.encoder.stages.2.layers.3.layer.1.convolution.weight', 'resnet.encoder.stages.2.layers.3.layer.0.normalization.running_var', 'resnet.encoder.stages.3.layers.0.shortcut.convolution.weight', 'resnet.encoder.stages.0.layers.0.layer.2.convolution.weight', 'resnet.encoder.stages.1.layers.3.layer.2.normalization.num_batches_tracked', 'resnet.encoder.stages.3.layers.0.layer.1.convolution.weight', 'resnet.encoder.stages.3.layers.0.layer.2.normalization.running_mean', 'resnet.encoder.stages.3.layers.0.layer.0.normalization.weight', 'resnet.encoder.stages.2.layers.5.layer.0.normalization.weight', 'resnet.encoder.stages.1.layers.1.layer.2.normalization.bias', 'resnet.encoder.stages.2.layers.0.layer.0.normalization.num_batches_tracked', 'resnet.embedder.embedder.normalization.num_batches_tracked', 'resnet.encoder.stages.1.layers.2.layer.1.normalization.weight', 'resnet.encoder.stages.1.layers.3.layer.1.normalization.weight', 'resnet.encoder.stages.2.layers.5.layer.2.normalization.running_mean', 'resnet.encoder.stages.1.layers.0.layer.1.normalization.running_var', 'resnet.encoder.stages.3.layers.2.layer.0.normalization.running_var', 'resnet.encoder.stages.0.layers.0.layer.1.normalization.running_mean', 'resnet.encoder.stages.2.layers.0.layer.2.convolution.weight', 'resnet.encoder.stages.1.layers.2.layer.1.normalization.running_mean', 'resnet.encoder.stages.3.layers.1.layer.1.normalization.num_batches_tracked', 'resnet.encoder.stages.1.layers.0.shortcut.normalization.running_var', 'resnet.encoder.stages.2.layers.2.layer.2.normalization.num_batches_tracked', 'resnet.encoder.stages.3.layers.2.layer.1.normalization.bias', 'resnet.encoder.stages.1.layers.3.layer.0.normalization.running_var', 'resnet.encoder.stages.2.layers.1.layer.1.normalization.num_batches_tracked', 'resnet.encoder.stages.1.layers.1.layer.0.convolution.weight', 'resnet.encoder.stages.3.layers.0.layer.0.normalization.running_var', 'resnet.encoder.stages.2.layers.0.layer.1.normalization.num_batches_tracked', 'resnet.encoder.stages.1.layers.2.layer.2.normalization.running_mean', 'resnet.encoder.stages.2.layers.4.layer.0.normalization.bias', 'resnet.encoder.stages.2.layers.3.layer.0.convolution.weight', 'resnet.encoder.stages.2.layers.4.layer.1.normalization.num_batches_tracked', 'resnet.encoder.stages.3.layers.1.layer.2.normalization.weight', 'resnet.encoder.stages.3.layers.0.layer.2.normalization.running_var', 'resnet.encoder.stages.3.layers.0.layer.0.normalization.running_mean', 'resnet.encoder.stages.1.layers.0.layer.0.normalization.weight', 'resnet.encoder.stages.0.layers.0.layer.1.normalization.running_var', 'resnet.encoder.stages.2.layers.4.layer.2.normalization.num_batches_tracked', 'resnet.encoder.stages.2.layers.0.layer.1.normalization.weight', 'resnet.encoder.stages.2.layers.3.layer.0.normalization.bias', 'resnet.encoder.stages.0.layers.0.shortcut.normalization.num_batches_tracked', 'resnet.encoder.stages.0.layers.0.shortcut.normalization.running_var', 'resnet.encoder.stages.3.layers.1.layer.2.normalization.running_var', 'resnet.encoder.stages.0.layers.2.layer.1.convolution.weight', 'resnet.encoder.stages.0.layers.0.layer.2.normalization.bias', 'resnet.encoder.stages.1.layers.1.layer.2.normalization.running_var', 'resnet.encoder.stages.2.layers.4.layer.1.normalization.weight', 'resnet.encoder.stages.3.layers.0.layer.0.convolution.weight', 'resnet.encoder.stages.1.layers.0.shortcut.normalization.weight', 'resnet.encoder.stages.2.layers.2.layer.1.normalization.weight', 'resnet.encoder.stages.3.layers.2.layer.1.convolution.weight', 'resnet.encoder.stages.3.layers.1.layer.0.normalization.num_batches_tracked', 'resnet.encoder.stages.0.layers.2.layer.2.normalization.weight', 'resnet.encoder.stages.2.layers.4.layer.0.convolution.weight', 'resnet.encoder.stages.3.layers.2.layer.0.normalization.num_batches_tracked', 'resnet.encoder.stages.2.layers.2.layer.1.normalization.running_var', 'resnet.encoder.stages.2.layers.4.layer.2.normalization.running_mean', 'resnet.encoder.stages.3.layers.2.layer.1.normalization.weight', 'resnet.encoder.stages.0.layers.1.layer.1.convolution.weight', 'resnet.encoder.stages.2.layers.2.layer.1.normalization.bias', 'resnet.encoder.stages.1.layers.1.layer.0.normalization.num_batches_tracked', 'resnet.encoder.stages.1.layers.2.layer.0.convolution.weight', 'resnet.encoder.stages.2.layers.3.layer.1.normalization.running_var', 'resnet.encoder.stages.1.layers.2.layer.2.convolution.weight', 'resnet.encoder.stages.1.layers.3.layer.1.normalization.num_batches_tracked', 'resnet.encoder.stages.1.layers.0.layer.2.normalization.weight', 'resnet.encoder.stages.1.layers.2.layer.0.normalization.bias', 'resnet.encoder.stages.0.layers.1.layer.0.normalization.running_var', 'resnet.encoder.stages.1.layers.0.layer.1.normalization.running_mean', 'resnet.encoder.stages.3.layers.2.layer.1.normalization.num_batches_tracked', 'resnet.encoder.stages.2.layers.5.layer.1.normalization.weight', 'resnet.encoder.stages.3.layers.2.layer.2.normalization.weight', 'resnet.encoder.stages.3.layers.1.layer.0.normalization.bias', 'resnet.encoder.stages.3.layers.1.layer.0.normalization.running_mean', 'resnet.encoder.stages.2.layers.1.layer.2.convolution.weight', 'resnet.encoder.stages.0.layers.2.layer.0.normalization.bias', 'resnet.encoder.stages.2.layers.4.layer.2.normalization.running_var', 'resnet.encoder.stages.1.layers.3.layer.0.convolution.weight', 'resnet.encoder.stages.2.layers.0.layer.1.normalization.running_mean', 'resnet.encoder.stages.2.layers.0.shortcut.normalization.running_mean', 'resnet.encoder.stages.0.layers.2.layer.0.normalization.running_var', 'resnet.encoder.stages.1.layers.2.layer.0.normalization.weight', 'resnet.encoder.stages.2.layers.3.layer.2.convolution.weight', 'resnet.encoder.stages.2.layers.4.layer.1.normalization.bias', 'resnet.encoder.stages.0.layers.0.layer.0.normalization.running_mean', 'resnet.encoder.stages.2.layers.1.layer.2.normalization.running_mean', 'resnet.encoder.stages.1.layers.0.layer.2.normalization.num_batches_tracked', 'resnet.encoder.stages.2.layers.0.shortcut.normalization.running_var', 'resnet.encoder.stages.2.layers.4.layer.2.normalization.weight', 'resnet.encoder.stages.2.layers.2.layer.1.normalization.num_batches_tracked', 'resnet.encoder.stages.1.layers.1.layer.1.convolution.weight', 'resnet.encoder.stages.0.layers.1.layer.2.convolution.weight', 'resnet.encoder.stages.1.layers.3.layer.0.normalization.num_batches_tracked', 'resnet.encoder.stages.2.layers.3.layer.0.normalization.weight', 'resnet.encoder.stages.0.layers.1.layer.2.normalization.num_batches_tracked', 'resnet.encoder.stages.2.layers.2.layer.0.normalization.weight', 'resnet.encoder.stages.2.layers.0.layer.0.normalization.running_var', 'resnet.encoder.stages.1.layers.2.layer.0.normalization.running_mean', 'resnet.encoder.stages.0.layers.1.layer.2.normalization.running_var', 'resnet.encoder.stages.0.layers.0.layer.0.normalization.bias', 'resnet.encoder.stages.3.layers.0.layer.0.normalization.num_batches_tracked', 'resnet.encoder.stages.0.layers.0.layer.2.normalization.weight', 'resnet.encoder.stages.3.layers.0.shortcut.normalization.bias', 'resnet.encoder.stages.2.layers.3.layer.2.normalization.running_mean', 'resnet.encoder.stages.1.layers.3.layer.1.normalization.running_var', 'resnet.encoder.stages.2.layers.3.layer.2.normalization.running_var', 'resnet.encoder.stages.1.layers.2.layer.0.normalization.num_batches_tracked', 'resnet.encoder.stages.3.layers.1.layer.2.normalization.running_mean', 'resnet.encoder.stages.2.layers.0.layer.0.normalization.bias', 'resnet.encoder.stages.2.layers.1.layer.0.normalization.running_var', 'resnet.encoder.stages.2.layers.3.layer.2.normalization.num_batches_tracked', 'resnet.encoder.stages.0.layers.1.layer.1.normalization.running_var', 'resnet.encoder.stages.2.layers.0.shortcut.normalization.num_batches_tracked', 'resnet.encoder.stages.0.layers.2.layer.1.normalization.running_var', 'resnet.encoder.stages.3.layers.0.layer.1.normalization.bias', 'resnet.encoder.stages.0.layers.0.layer.2.normalization.running_var', 'resnet.encoder.stages.2.layers.1.layer.2.normalization.running_var', 'resnet.encoder.stages.1.layers.3.layer.0.normalization.running_mean', 'resnet.encoder.stages.3.layers.0.shortcut.normalization.weight', 'resnet.encoder.stages.1.layers.2.layer.1.normalization.bias', 'resnet.encoder.stages.3.layers.0.layer.1.normalization.running_mean', 'resnet.encoder.stages.2.layers.3.layer.1.normalization.bias', 'resnet.encoder.stages.3.layers.0.layer.2.normalization.weight', 'resnet.encoder.stages.2.layers.4.layer.2.normalization.bias', 'resnet.encoder.stages.2.layers.0.layer.0.normalization.weight', 'resnet.encoder.stages.0.layers.1.layer.0.convolution.weight', 'resnet.encoder.stages.3.layers.1.layer.1.convolution.weight', 'resnet.encoder.stages.3.layers.1.layer.2.normalization.bias', 'resnet.encoder.stages.0.layers.1.layer.0.normalization.weight', 'resnet.encoder.stages.2.layers.2.layer.2.convolution.weight', 'resnet.encoder.stages.2.layers.5.layer.2.normalization.bias', 'resnet.encoder.stages.0.layers.2.layer.0.convolution.weight', 'resnet.encoder.stages.2.layers.5.layer.2.normalization.weight', 'resnet.encoder.stages.2.layers.4.layer.0.normalization.num_batches_tracked', 'resnet.encoder.stages.1.layers.2.layer.0.normalization.running_var', 'resnet.encoder.stages.3.layers.2.layer.2.normalization.running_var', 'resnet.encoder.stages.2.layers.5.layer.1.normalization.num_batches_tracked', 'resnet.encoder.stages.2.layers.5.layer.2.convolution.weight', 'resnet.encoder.stages.2.layers.1.layer.1.normalization.running_var', 'resnet.encoder.stages.1.layers.1.layer.0.normalization.running_mean', 'resnet.encoder.stages.2.layers.2.layer.1.convolution.weight', 'resnet.encoder.stages.2.layers.0.shortcut.normalization.weight', 'resnet.encoder.stages.2.layers.0.layer.2.normalization.running_mean', 'resnet.encoder.stages.0.layers.0.shortcut.normalization.weight', 'resnet.encoder.stages.0.layers.2.layer.1.normalization.bias', 'resnet.encoder.stages.3.layers.0.shortcut.normalization.num_batches_tracked', 'resnet.encoder.stages.3.layers.1.layer.2.convolution.weight', 'resnet.encoder.stages.1.layers.0.layer.1.convolution.weight', 'resnet.encoder.stages.3.layers.0.layer.1.normalization.running_var', 'resnet.encoder.stages.3.layers.1.layer.1.normalization.running_var', 'resnet.encoder.stages.3.layers.2.layer.2.convolution.weight', 'resnet.encoder.stages.2.layers.0.layer.2.normalization.bias', 'resnet.encoder.stages.1.layers.2.layer.1.normalization.num_batches_tracked', 'resnet.encoder.stages.1.layers.0.shortcut.normalization.num_batches_tracked', 'resnet.encoder.stages.1.layers.2.layer.2.normalization.weight', 'resnet.encoder.stages.2.layers.1.layer.1.normalization.running_mean', 'resnet.encoder.stages.1.layers.1.layer.0.normalization.weight', 'resnet.encoder.stages.3.layers.1.layer.1.normalization.weight', 'resnet.encoder.stages.2.layers.4.layer.1.normalization.running_var', 'resnet.encoder.stages.2.layers.5.layer.0.normalization.running_var', 'resnet.encoder.stages.2.layers.5.layer.1.normalization.bias', 'resnet.encoder.stages.2.layers.1.layer.1.convolution.weight', 'resnet.encoder.stages.2.layers.2.layer.2.normalization.bias', 'resnet.encoder.stages.1.layers.0.layer.2.normalization.running_var', 'resnet.encoder.stages.1.layers.0.layer.0.normalization.running_mean', 'resnet.encoder.stages.3.layers.2.layer.0.convolution.weight', 'resnet.encoder.stages.1.layers.0.layer.1.normalization.num_batches_tracked', 'resnet.encoder.stages.1.layers.2.layer.1.convolution.weight', 'resnet.encoder.stages.0.layers.0.layer.2.normalization.num_batches_tracked', 'resnet.encoder.stages.2.layers.1.layer.2.normalization.bias', 'resnet.encoder.stages.2.layers.5.layer.2.normalization.running_var', 'resnet.encoder.stages.1.layers.0.layer.2.convolution.weight', 'resnet.encoder.stages.3.layers.0.layer.2.convolution.weight', 'resnet.encoder.stages.2.layers.1.layer.0.normalization.running_mean', 'resnet.encoder.stages.1.layers.3.layer.2.normalization.bias', 'resnet.encoder.stages.2.layers.5.layer.0.normalization.running_mean', 'resnet.encoder.stages.1.layers.3.layer.2.convolution.weight', 'classifier.1.bias', 'resnet.encoder.stages.1.layers.2.layer.2.normalization.num_batches_tracked', 'resnet.encoder.stages.1.layers.0.layer.0.normalization.running_var', 'resnet.encoder.stages.2.layers.5.layer.1.convolution.weight', 'resnet.encoder.stages.0.layers.2.layer.2.normalization.running_var', 'resnet.encoder.stages.1.layers.0.layer.0.normalization.bias', 'resnet.encoder.stages.2.layers.0.layer.2.normalization.num_batches_tracked', 'resnet.encoder.stages.2.layers.5.layer.0.normalization.bias', 'resnet.encoder.stages.0.layers.2.layer.2.normalization.bias', 'resnet.encoder.stages.2.layers.2.layer.0.normalization.num_batches_tracked', 'resnet.encoder.stages.0.layers.1.layer.0.normalization.running_mean', 'resnet.encoder.stages.0.layers.2.layer.0.normalization.num_batches_tracked', 'classifier.1.weight', 'resnet.encoder.stages.0.layers.1.layer.1.normalization.bias', 'resnet.encoder.stages.0.layers.1.layer.2.normalization.weight', 'resnet.encoder.stages.0.layers.0.layer.1.normalization.bias', 'resnet.encoder.stages.0.layers.0.layer.0.normalization.running_var', 'resnet.encoder.stages.2.layers.2.layer.0.normalization.bias', 'resnet.encoder.stages.3.layers.2.layer.2.normalization.num_batches_tracked', 'resnet.encoder.stages.2.layers.4.layer.1.convolution.weight', 'resnet.encoder.stages.2.layers.3.layer.1.normalization.running_mean', 'resnet.encoder.stages.0.layers.2.layer.1.normalization.weight', 'resnet.encoder.stages.1.layers.2.layer.2.normalization.running_var', 'resnet.encoder.stages.0.layers.0.shortcut.normalization.bias', 'resnet.encoder.stages.1.layers.0.shortcut.normalization.running_mean', 'resnet.encoder.stages.1.layers.1.layer.1.normalization.running_mean', 'resnet.encoder.stages.0.layers.1.layer.0.normalization.num_batches_tracked', 'resnet.embedder.embedder.normalization.running_mean', 'resnet.encoder.stages.2.layers.2.layer.0.convolution.weight', 'resnet.encoder.stages.0.layers.0.layer.0.convolution.weight', 'resnet.encoder.stages.2.layers.3.layer.2.normalization.bias', 'resnet.encoder.stages.2.layers.5.layer.0.normalization.num_batches_tracked', 'resnet.encoder.stages.3.layers.1.layer.0.normalization.weight', 'resnet.encoder.stages.2.layers.0.shortcut.normalization.bias', 'resnet.encoder.stages.2.layers.3.layer.2.normalization.weight', 'resnet.encoder.stages.0.layers.0.shortcut.normalization.running_mean', 'resnet.encoder.stages.3.layers.0.layer.1.normalization.weight', 'resnet.encoder.stages.0.layers.2.layer.0.normalization.weight', 'resnet.encoder.stages.2.layers.4.layer.0.normalization.weight', 'resnet.encoder.stages.2.layers.2.layer.0.normalization.running_var', 'resnet.encoder.stages.0.layers.2.layer.2.convolution.weight', 'resnet.encoder.stages.2.layers.1.layer.2.normalization.weight', 'resnet.encoder.stages.1.layers.0.layer.2.normalization.bias', 'resnet.encoder.stages.1.layers.1.layer.0.normalization.bias', 'resnet.encoder.stages.2.layers.1.layer.1.normalization.weight', 'resnet.encoder.stages.2.layers.3.layer.0.normalization.num_batches_tracked', 'resnet.encoder.stages.3.layers.1.layer.0.normalization.running_var', 'resnet.encoder.stages.2.layers.1.layer.0.normalization.bias', 'resnet.encoder.stages.1.layers.2.layer.1.normalization.running_var', 'resnet.encoder.stages.0.layers.1.layer.0.normalization.bias', 'resnet.encoder.stages.0.layers.0.layer.1.convolution.weight', 'resnet.encoder.stages.3.layers.1.layer.2.normalization.num_batches_tracked', 'resnet.encoder.stages.0.layers.0.layer.0.normalization.weight', 'resnet.encoder.stages.1.layers.1.layer.1.normalization.num_batches_tracked', 'resnet.encoder.stages.1.layers.1.layer.2.normalization.running_mean', 'resnet.encoder.stages.2.layers.2.layer.1.normalization.running_mean', 'resnet.encoder.stages.2.layers.5.layer.1.normalization.running_var', 'resnet.encoder.stages.1.layers.2.layer.2.normalization.bias', 'resnet.encoder.stages.1.layers.0.layer.1.normalization.weight', 'resnet.encoder.stages.1.layers.1.layer.2.convolution.weight', 'resnet.encoder.stages.2.layers.1.layer.0.convolution.weight', 'resnet.encoder.stages.3.layers.1.layer.0.convolution.weight', 'resnet.encoder.stages.2.layers.5.layer.1.normalization.running_mean', 'resnet.encoder.stages.3.layers.0.shortcut.normalization.running_var', 'resnet.encoder.stages.0.layers.2.layer.2.normalization.num_batches_tracked', 'resnet.encoder.stages.1.layers.1.layer.1.normalization.weight', 'resnet.encoder.stages.0.layers.1.layer.1.normalization.num_batches_tracked', 'resnet.encoder.stages.3.layers.0.layer.2.normalization.bias', 'resnet.encoder.stages.2.layers.0.layer.0.normalization.running_mean', 'resnet.encoder.stages.0.layers.2.layer.1.normalization.num_batches_tracked', 'resnet.encoder.stages.2.layers.3.layer.1.normalization.weight', 'resnet.encoder.stages.1.layers.3.layer.1.normalization.running_mean', 'resnet.encoder.stages.1.layers.0.layer.2.normalization.running_mean', 'resnet.encoder.stages.2.layers.0.layer.2.normalization.weight', 'resnet.encoder.stages.2.layers.0.layer.0.convolution.weight', 'resnet.encoder.stages.1.layers.0.shortcut.normalization.bias', 'resnet.encoder.stages.2.layers.2.layer.2.normalization.weight', 'resnet.encoder.stages.0.layers.2.layer.2.normalization.running_mean', 'resnet.encoder.stages.1.layers.1.layer.2.normalization.num_batches_tracked', 'resnet.encoder.stages.2.layers.0.layer.1.convolution.weight', 'resnet.encoder.stages.1.layers.3.layer.2.normalization.running_mean', 'resnet.encoder.stages.3.layers.2.layer.2.normalization.running_mean', 'resnet.encoder.stages.2.layers.4.layer.0.normalization.running_var', 'resnet.encoder.stages.1.layers.1.layer.2.normalization.weight', 'resnet.embedder.embedder.normalization.bias', 'resnet.encoder.stages.1.layers.3.layer.0.normalization.weight', 'resnet.encoder.stages.3.layers.0.shortcut.normalization.running_mean', 'resnet.encoder.stages.1.layers.3.layer.1.normalization.bias', 'resnet.encoder.stages.0.layers.0.shortcut.convolution.weight', 'resnet.encoder.stages.0.layers.1.layer.1.normalization.running_mean', 'resnet.encoder.stages.2.layers.2.layer.2.normalization.running_var', 'resnet.encoder.stages.3.layers.2.layer.0.normalization.running_mean', 'resnet.encoder.stages.2.layers.1.layer.1.normalization.bias', 'resnet.encoder.stages.0.layers.0.layer.1.normalization.weight', 'resnet.encoder.stages.2.layers.0.layer.2.normalization.running_var', 'resnet.encoder.stages.2.layers.0.shortcut.convolution.weight', 'resnet.encoder.stages.1.layers.0.shortcut.convolution.weight', 'resnet.encoder.stages.2.layers.3.layer.1.normalization.num_batches_tracked', 'resnet.encoder.stages.2.layers.1.layer.0.normalization.weight', 'resnet.encoder.stages.1.layers.3.layer.2.normalization.running_var', 'resnet.encoder.stages.2.layers.2.layer.0.normalization.running_mean']\n",
            "- This IS expected if you are initializing ViTForImageClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing ViTForImageClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of ViTForImageClassification were not initialized from the model checkpoint at microsoft/resnet-50 and are newly initialized: ['encoder.layer.10.attention.output.dense.bias', 'encoder.layer.11.layernorm_after.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.3.layernorm_before.bias', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.4.output.dense.bias', 'encoder.layer.7.attention.attention.value.weight', 'encoder.layer.11.layernorm_before.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.8.attention.attention.query.bias', 'encoder.layer.1.attention.attention.key.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.8.attention.attention.value.bias', 'encoder.layer.11.layernorm_before.weight', 'encoder.layer.7.attention.attention.query.bias', 'encoder.layer.4.attention.attention.key.bias', 'encoder.layer.1.attention.attention.query.bias', 'encoder.layer.9.layernorm_before.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.10.attention.attention.value.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.9.attention.attention.key.bias', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.9.layernorm_before.bias', 'encoder.layer.8.attention.attention.query.weight', 'encoder.layer.9.output.dense.weight', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.5.attention.attention.query.weight', 'encoder.layer.7.attention.attention.query.weight', 'encoder.layer.4.layernorm_before.weight', 'embeddings.patch_embeddings.projection.weight', 'encoder.layer.0.layernorm_before.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.3.layernorm_before.weight', 'encoder.layer.6.attention.attention.query.weight', 'encoder.layer.3.attention.attention.query.weight', 'encoder.layer.7.output.dense.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.5.attention.attention.value.bias', 'encoder.layer.10.attention.attention.key.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.7.layernorm_after.weight', 'embeddings.position_embeddings', 'encoder.layer.2.layernorm_after.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.11.attention.attention.query.weight', 'encoder.layer.5.attention.attention.query.bias', 'encoder.layer.5.layernorm_after.bias', 'encoder.layer.9.attention.attention.query.weight', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.2.layernorm_before.bias', 'encoder.layer.2.output.dense.bias', 'encoder.layer.5.attention.attention.key.bias', 'encoder.layer.8.output.dense.bias', 'encoder.layer.1.layernorm_after.weight', 'encoder.layer.5.attention.attention.value.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.10.attention.attention.key.weight', 'encoder.layer.2.attention.attention.value.bias', 'encoder.layer.11.output.dense.bias', 'encoder.layer.9.attention.attention.value.weight', 'encoder.layer.5.layernorm_before.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.2.attention.attention.query.bias', 'encoder.layer.9.attention.attention.key.weight', 'encoder.layer.6.layernorm_after.weight', 'encoder.layer.6.attention.attention.value.bias', 'encoder.layer.11.attention.attention.value.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.6.output.dense.weight', 'encoder.layer.0.layernorm_after.weight', 'encoder.layer.4.attention.attention.key.weight', 'classifier.weight', 'encoder.layer.4.attention.attention.query.bias', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.9.attention.attention.query.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.4.layernorm_after.weight', 'encoder.layer.0.attention.attention.query.bias', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.11.layernorm_after.bias', 'encoder.layer.9.layernorm_after.bias', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.0.output.dense.bias', 'encoder.layer.8.layernorm_after.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.3.attention.attention.value.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.6.layernorm_before.weight', 'encoder.layer.8.attention.attention.key.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.3.layernorm_after.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.4.attention.attention.value.bias', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.9.output.dense.bias', 'encoder.layer.1.attention.attention.query.weight', 'encoder.layer.2.attention.attention.key.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.6.attention.attention.query.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.9.layernorm_after.weight', 'encoder.layer.3.attention.attention.key.bias', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.0.attention.attention.query.weight', 'encoder.layer.1.attention.attention.value.weight', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.8.layernorm_before.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.6.layernorm_before.bias', 'encoder.layer.2.attention.attention.value.weight', 'encoder.layer.1.attention.attention.value.bias', 'encoder.layer.7.attention.attention.key.bias', 'encoder.layer.6.output.dense.bias', 'layernorm.weight', 'encoder.layer.3.layernorm_after.bias', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.3.attention.attention.query.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.11.attention.attention.key.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.2.attention.attention.query.weight', 'encoder.layer.0.layernorm_after.bias', 'encoder.layer.6.layernorm_after.bias', 'encoder.layer.7.output.dense.bias', 'encoder.layer.6.attention.attention.key.weight', 'encoder.layer.5.layernorm_after.weight', 'embeddings.cls_token', 'encoder.layer.7.layernorm_before.weight', 'encoder.layer.8.layernorm_before.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.0.attention.attention.value.bias', 'classifier.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.1.attention.attention.key.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.11.attention.attention.value.bias', 'encoder.layer.10.layernorm_after.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.2.layernorm_before.weight', 'encoder.layer.10.layernorm_before.bias', 'encoder.layer.5.attention.attention.key.weight', 'encoder.layer.3.attention.attention.value.weight', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.7.attention.attention.key.weight', 'encoder.layer.0.layernorm_before.weight', 'encoder.layer.4.attention.attention.query.weight', 'embeddings.patch_embeddings.projection.bias', 'encoder.layer.7.layernorm_before.bias', 'encoder.layer.7.layernorm_after.bias', 'encoder.layer.3.output.dense.bias', 'encoder.layer.7.attention.attention.value.bias', 'encoder.layer.6.attention.attention.value.weight', 'encoder.layer.2.attention.attention.key.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.10.layernorm_after.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.4.layernorm_after.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.4.attention.attention.value.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.2.layernorm_after.weight', 'encoder.layer.0.attention.attention.value.weight', 'encoder.layer.2.intermediate.dense.bias', 'layernorm.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.5.output.dense.weight', 'encoder.layer.9.attention.attention.value.bias', 'encoder.layer.8.attention.attention.value.weight', 'encoder.layer.8.layernorm_after.bias', 'encoder.layer.1.layernorm_after.bias', 'encoder.layer.10.layernorm_before.weight', 'encoder.layer.0.attention.attention.key.bias', 'encoder.layer.11.attention.attention.key.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.10.attention.attention.query.weight', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.10.attention.attention.query.bias', 'encoder.layer.3.attention.attention.key.weight', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.8.attention.attention.key.bias', 'encoder.layer.10.attention.attention.value.bias', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.4.layernorm_before.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.1.layernorm_before.weight', 'encoder.layer.6.attention.attention.key.bias', 'encoder.layer.1.layernorm_before.bias', 'encoder.layer.0.attention.attention.key.weight', 'encoder.layer.11.attention.attention.query.bias', 'encoder.layer.5.layernorm_before.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.15.4"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20230702_111551-uv2ku8z2</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/deep_bekeif/cnn_no_aug/runs/uv2ku8z2' target=\"_blank\">lunar-shape-5</a></strong> to <a href='https://wandb.ai/deep_bekeif/cnn_no_aug' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/deep_bekeif/cnn_no_aug' target=\"_blank\">https://wandb.ai/deep_bekeif/cnn_no_aug</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/deep_bekeif/cnn_no_aug/runs/uv2ku8z2' target=\"_blank\">https://wandb.ai/deep_bekeif/cnn_no_aug/runs/uv2ku8z2</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You are using a model of type resnet to instantiate a model of type vit. This is not supported for all configurations of models and can yield errors.\n",
            "Some weights of the model checkpoint at microsoft/resnet-50 were not used when initializing ViTForImageClassification: ['resnet.embedder.embedder.normalization.running_var', 'resnet.encoder.stages.3.layers.2.layer.1.normalization.running_mean', 'resnet.encoder.stages.1.layers.0.layer.0.normalization.num_batches_tracked', 'resnet.encoder.stages.2.layers.3.layer.0.normalization.running_mean', 'resnet.encoder.stages.1.layers.3.layer.2.normalization.weight', 'resnet.encoder.stages.1.layers.3.layer.1.convolution.weight', 'resnet.encoder.stages.2.layers.1.layer.2.normalization.num_batches_tracked', 'resnet.encoder.stages.2.layers.0.layer.1.normalization.running_var', 'resnet.encoder.stages.2.layers.2.layer.2.normalization.running_mean', 'resnet.encoder.stages.3.layers.2.layer.0.normalization.bias', 'resnet.encoder.stages.1.layers.1.layer.0.normalization.running_var', 'resnet.embedder.embedder.convolution.weight', 'resnet.embedder.embedder.normalization.weight', 'resnet.encoder.stages.3.layers.2.layer.0.normalization.weight', 'resnet.encoder.stages.2.layers.0.layer.1.normalization.bias', 'resnet.encoder.stages.2.layers.4.layer.0.normalization.running_mean', 'resnet.encoder.stages.0.layers.2.layer.0.normalization.running_mean', 'resnet.encoder.stages.0.layers.0.layer.2.normalization.running_mean', 'resnet.encoder.stages.1.layers.1.layer.1.normalization.running_var', 'resnet.encoder.stages.1.layers.0.layer.0.convolution.weight', 'resnet.encoder.stages.1.layers.3.layer.0.normalization.bias', 'resnet.encoder.stages.3.layers.2.layer.2.normalization.bias', 'resnet.encoder.stages.2.layers.4.layer.1.normalization.running_mean', 'resnet.encoder.stages.3.layers.0.layer.1.normalization.num_batches_tracked', 'resnet.encoder.stages.3.layers.2.layer.1.normalization.running_var', 'resnet.encoder.stages.0.layers.1.layer.1.normalization.weight', 'resnet.encoder.stages.3.layers.1.layer.1.normalization.bias', 'resnet.encoder.stages.2.layers.5.layer.0.convolution.weight', 'resnet.encoder.stages.0.layers.2.layer.1.normalization.running_mean', 'resnet.encoder.stages.0.layers.1.layer.2.normalization.running_mean', 'resnet.encoder.stages.0.layers.0.layer.0.normalization.num_batches_tracked', 'resnet.encoder.stages.1.layers.1.layer.1.normalization.bias', 'resnet.encoder.stages.3.layers.0.layer.0.normalization.bias', 'resnet.encoder.stages.2.layers.4.layer.2.convolution.weight', 'resnet.encoder.stages.0.layers.1.layer.2.normalization.bias', 'resnet.encoder.stages.2.layers.5.layer.2.normalization.num_batches_tracked', 'resnet.encoder.stages.0.layers.0.layer.1.normalization.num_batches_tracked', 'resnet.encoder.stages.3.layers.0.layer.2.normalization.num_batches_tracked', 'resnet.encoder.stages.2.layers.1.layer.0.normalization.num_batches_tracked', 'resnet.encoder.stages.3.layers.1.layer.1.normalization.running_mean', 'resnet.encoder.stages.1.layers.0.layer.1.normalization.bias', 'resnet.encoder.stages.2.layers.3.layer.1.convolution.weight', 'resnet.encoder.stages.2.layers.3.layer.0.normalization.running_var', 'resnet.encoder.stages.3.layers.0.shortcut.convolution.weight', 'resnet.encoder.stages.0.layers.0.layer.2.convolution.weight', 'resnet.encoder.stages.1.layers.3.layer.2.normalization.num_batches_tracked', 'resnet.encoder.stages.3.layers.0.layer.1.convolution.weight', 'resnet.encoder.stages.3.layers.0.layer.2.normalization.running_mean', 'resnet.encoder.stages.3.layers.0.layer.0.normalization.weight', 'resnet.encoder.stages.2.layers.5.layer.0.normalization.weight', 'resnet.encoder.stages.1.layers.1.layer.2.normalization.bias', 'resnet.encoder.stages.2.layers.0.layer.0.normalization.num_batches_tracked', 'resnet.embedder.embedder.normalization.num_batches_tracked', 'resnet.encoder.stages.1.layers.2.layer.1.normalization.weight', 'resnet.encoder.stages.1.layers.3.layer.1.normalization.weight', 'resnet.encoder.stages.2.layers.5.layer.2.normalization.running_mean', 'resnet.encoder.stages.1.layers.0.layer.1.normalization.running_var', 'resnet.encoder.stages.3.layers.2.layer.0.normalization.running_var', 'resnet.encoder.stages.0.layers.0.layer.1.normalization.running_mean', 'resnet.encoder.stages.2.layers.0.layer.2.convolution.weight', 'resnet.encoder.stages.1.layers.2.layer.1.normalization.running_mean', 'resnet.encoder.stages.3.layers.1.layer.1.normalization.num_batches_tracked', 'resnet.encoder.stages.1.layers.0.shortcut.normalization.running_var', 'resnet.encoder.stages.2.layers.2.layer.2.normalization.num_batches_tracked', 'resnet.encoder.stages.3.layers.2.layer.1.normalization.bias', 'resnet.encoder.stages.1.layers.3.layer.0.normalization.running_var', 'resnet.encoder.stages.2.layers.1.layer.1.normalization.num_batches_tracked', 'resnet.encoder.stages.1.layers.1.layer.0.convolution.weight', 'resnet.encoder.stages.3.layers.0.layer.0.normalization.running_var', 'resnet.encoder.stages.2.layers.0.layer.1.normalization.num_batches_tracked', 'resnet.encoder.stages.1.layers.2.layer.2.normalization.running_mean', 'resnet.encoder.stages.2.layers.4.layer.0.normalization.bias', 'resnet.encoder.stages.2.layers.3.layer.0.convolution.weight', 'resnet.encoder.stages.2.layers.4.layer.1.normalization.num_batches_tracked', 'resnet.encoder.stages.3.layers.1.layer.2.normalization.weight', 'resnet.encoder.stages.3.layers.0.layer.2.normalization.running_var', 'resnet.encoder.stages.3.layers.0.layer.0.normalization.running_mean', 'resnet.encoder.stages.1.layers.0.layer.0.normalization.weight', 'resnet.encoder.stages.0.layers.0.layer.1.normalization.running_var', 'resnet.encoder.stages.2.layers.4.layer.2.normalization.num_batches_tracked', 'resnet.encoder.stages.2.layers.0.layer.1.normalization.weight', 'resnet.encoder.stages.2.layers.3.layer.0.normalization.bias', 'resnet.encoder.stages.0.layers.0.shortcut.normalization.num_batches_tracked', 'resnet.encoder.stages.0.layers.0.shortcut.normalization.running_var', 'resnet.encoder.stages.3.layers.1.layer.2.normalization.running_var', 'resnet.encoder.stages.0.layers.2.layer.1.convolution.weight', 'resnet.encoder.stages.0.layers.0.layer.2.normalization.bias', 'resnet.encoder.stages.1.layers.1.layer.2.normalization.running_var', 'resnet.encoder.stages.2.layers.4.layer.1.normalization.weight', 'resnet.encoder.stages.3.layers.0.layer.0.convolution.weight', 'resnet.encoder.stages.1.layers.0.shortcut.normalization.weight', 'resnet.encoder.stages.2.layers.2.layer.1.normalization.weight', 'resnet.encoder.stages.3.layers.2.layer.1.convolution.weight', 'resnet.encoder.stages.3.layers.1.layer.0.normalization.num_batches_tracked', 'resnet.encoder.stages.0.layers.2.layer.2.normalization.weight', 'resnet.encoder.stages.2.layers.4.layer.0.convolution.weight', 'resnet.encoder.stages.3.layers.2.layer.0.normalization.num_batches_tracked', 'resnet.encoder.stages.2.layers.2.layer.1.normalization.running_var', 'resnet.encoder.stages.2.layers.4.layer.2.normalization.running_mean', 'resnet.encoder.stages.3.layers.2.layer.1.normalization.weight', 'resnet.encoder.stages.0.layers.1.layer.1.convolution.weight', 'resnet.encoder.stages.2.layers.2.layer.1.normalization.bias', 'resnet.encoder.stages.1.layers.1.layer.0.normalization.num_batches_tracked', 'resnet.encoder.stages.1.layers.2.layer.0.convolution.weight', 'resnet.encoder.stages.2.layers.3.layer.1.normalization.running_var', 'resnet.encoder.stages.1.layers.2.layer.2.convolution.weight', 'resnet.encoder.stages.1.layers.3.layer.1.normalization.num_batches_tracked', 'resnet.encoder.stages.1.layers.0.layer.2.normalization.weight', 'resnet.encoder.stages.1.layers.2.layer.0.normalization.bias', 'resnet.encoder.stages.0.layers.1.layer.0.normalization.running_var', 'resnet.encoder.stages.1.layers.0.layer.1.normalization.running_mean', 'resnet.encoder.stages.3.layers.2.layer.1.normalization.num_batches_tracked', 'resnet.encoder.stages.2.layers.5.layer.1.normalization.weight', 'resnet.encoder.stages.3.layers.2.layer.2.normalization.weight', 'resnet.encoder.stages.3.layers.1.layer.0.normalization.bias', 'resnet.encoder.stages.3.layers.1.layer.0.normalization.running_mean', 'resnet.encoder.stages.2.layers.1.layer.2.convolution.weight', 'resnet.encoder.stages.0.layers.2.layer.0.normalization.bias', 'resnet.encoder.stages.2.layers.4.layer.2.normalization.running_var', 'resnet.encoder.stages.1.layers.3.layer.0.convolution.weight', 'resnet.encoder.stages.2.layers.0.layer.1.normalization.running_mean', 'resnet.encoder.stages.2.layers.0.shortcut.normalization.running_mean', 'resnet.encoder.stages.0.layers.2.layer.0.normalization.running_var', 'resnet.encoder.stages.1.layers.2.layer.0.normalization.weight', 'resnet.encoder.stages.2.layers.3.layer.2.convolution.weight', 'resnet.encoder.stages.2.layers.4.layer.1.normalization.bias', 'resnet.encoder.stages.0.layers.0.layer.0.normalization.running_mean', 'resnet.encoder.stages.2.layers.1.layer.2.normalization.running_mean', 'resnet.encoder.stages.1.layers.0.layer.2.normalization.num_batches_tracked', 'resnet.encoder.stages.2.layers.0.shortcut.normalization.running_var', 'resnet.encoder.stages.2.layers.4.layer.2.normalization.weight', 'resnet.encoder.stages.2.layers.2.layer.1.normalization.num_batches_tracked', 'resnet.encoder.stages.1.layers.1.layer.1.convolution.weight', 'resnet.encoder.stages.0.layers.1.layer.2.convolution.weight', 'resnet.encoder.stages.1.layers.3.layer.0.normalization.num_batches_tracked', 'resnet.encoder.stages.2.layers.3.layer.0.normalization.weight', 'resnet.encoder.stages.0.layers.1.layer.2.normalization.num_batches_tracked', 'resnet.encoder.stages.2.layers.2.layer.0.normalization.weight', 'resnet.encoder.stages.2.layers.0.layer.0.normalization.running_var', 'resnet.encoder.stages.1.layers.2.layer.0.normalization.running_mean', 'resnet.encoder.stages.0.layers.1.layer.2.normalization.running_var', 'resnet.encoder.stages.0.layers.0.layer.0.normalization.bias', 'resnet.encoder.stages.3.layers.0.layer.0.normalization.num_batches_tracked', 'resnet.encoder.stages.0.layers.0.layer.2.normalization.weight', 'resnet.encoder.stages.3.layers.0.shortcut.normalization.bias', 'resnet.encoder.stages.2.layers.3.layer.2.normalization.running_mean', 'resnet.encoder.stages.1.layers.3.layer.1.normalization.running_var', 'resnet.encoder.stages.2.layers.3.layer.2.normalization.running_var', 'resnet.encoder.stages.1.layers.2.layer.0.normalization.num_batches_tracked', 'resnet.encoder.stages.3.layers.1.layer.2.normalization.running_mean', 'resnet.encoder.stages.2.layers.0.layer.0.normalization.bias', 'resnet.encoder.stages.2.layers.1.layer.0.normalization.running_var', 'resnet.encoder.stages.2.layers.3.layer.2.normalization.num_batches_tracked', 'resnet.encoder.stages.0.layers.1.layer.1.normalization.running_var', 'resnet.encoder.stages.2.layers.0.shortcut.normalization.num_batches_tracked', 'resnet.encoder.stages.0.layers.2.layer.1.normalization.running_var', 'resnet.encoder.stages.3.layers.0.layer.1.normalization.bias', 'resnet.encoder.stages.0.layers.0.layer.2.normalization.running_var', 'resnet.encoder.stages.2.layers.1.layer.2.normalization.running_var', 'resnet.encoder.stages.1.layers.3.layer.0.normalization.running_mean', 'resnet.encoder.stages.3.layers.0.shortcut.normalization.weight', 'resnet.encoder.stages.1.layers.2.layer.1.normalization.bias', 'resnet.encoder.stages.3.layers.0.layer.1.normalization.running_mean', 'resnet.encoder.stages.2.layers.3.layer.1.normalization.bias', 'resnet.encoder.stages.3.layers.0.layer.2.normalization.weight', 'resnet.encoder.stages.2.layers.4.layer.2.normalization.bias', 'resnet.encoder.stages.2.layers.0.layer.0.normalization.weight', 'resnet.encoder.stages.0.layers.1.layer.0.convolution.weight', 'resnet.encoder.stages.3.layers.1.layer.1.convolution.weight', 'resnet.encoder.stages.3.layers.1.layer.2.normalization.bias', 'resnet.encoder.stages.0.layers.1.layer.0.normalization.weight', 'resnet.encoder.stages.2.layers.2.layer.2.convolution.weight', 'resnet.encoder.stages.2.layers.5.layer.2.normalization.bias', 'resnet.encoder.stages.0.layers.2.layer.0.convolution.weight', 'resnet.encoder.stages.2.layers.5.layer.2.normalization.weight', 'resnet.encoder.stages.2.layers.4.layer.0.normalization.num_batches_tracked', 'resnet.encoder.stages.1.layers.2.layer.0.normalization.running_var', 'resnet.encoder.stages.3.layers.2.layer.2.normalization.running_var', 'resnet.encoder.stages.2.layers.5.layer.1.normalization.num_batches_tracked', 'resnet.encoder.stages.2.layers.5.layer.2.convolution.weight', 'resnet.encoder.stages.2.layers.1.layer.1.normalization.running_var', 'resnet.encoder.stages.1.layers.1.layer.0.normalization.running_mean', 'resnet.encoder.stages.2.layers.2.layer.1.convolution.weight', 'resnet.encoder.stages.2.layers.0.shortcut.normalization.weight', 'resnet.encoder.stages.2.layers.0.layer.2.normalization.running_mean', 'resnet.encoder.stages.0.layers.0.shortcut.normalization.weight', 'resnet.encoder.stages.0.layers.2.layer.1.normalization.bias', 'resnet.encoder.stages.3.layers.0.shortcut.normalization.num_batches_tracked', 'resnet.encoder.stages.3.layers.1.layer.2.convolution.weight', 'resnet.encoder.stages.1.layers.0.layer.1.convolution.weight', 'resnet.encoder.stages.3.layers.0.layer.1.normalization.running_var', 'resnet.encoder.stages.3.layers.1.layer.1.normalization.running_var', 'resnet.encoder.stages.3.layers.2.layer.2.convolution.weight', 'resnet.encoder.stages.2.layers.0.layer.2.normalization.bias', 'resnet.encoder.stages.1.layers.2.layer.1.normalization.num_batches_tracked', 'resnet.encoder.stages.1.layers.0.shortcut.normalization.num_batches_tracked', 'resnet.encoder.stages.1.layers.2.layer.2.normalization.weight', 'resnet.encoder.stages.2.layers.1.layer.1.normalization.running_mean', 'resnet.encoder.stages.1.layers.1.layer.0.normalization.weight', 'resnet.encoder.stages.3.layers.1.layer.1.normalization.weight', 'resnet.encoder.stages.2.layers.4.layer.1.normalization.running_var', 'resnet.encoder.stages.2.layers.5.layer.0.normalization.running_var', 'resnet.encoder.stages.2.layers.5.layer.1.normalization.bias', 'resnet.encoder.stages.2.layers.1.layer.1.convolution.weight', 'resnet.encoder.stages.2.layers.2.layer.2.normalization.bias', 'resnet.encoder.stages.1.layers.0.layer.2.normalization.running_var', 'resnet.encoder.stages.1.layers.0.layer.0.normalization.running_mean', 'resnet.encoder.stages.3.layers.2.layer.0.convolution.weight', 'resnet.encoder.stages.1.layers.0.layer.1.normalization.num_batches_tracked', 'resnet.encoder.stages.1.layers.2.layer.1.convolution.weight', 'resnet.encoder.stages.0.layers.0.layer.2.normalization.num_batches_tracked', 'resnet.encoder.stages.2.layers.1.layer.2.normalization.bias', 'resnet.encoder.stages.2.layers.5.layer.2.normalization.running_var', 'resnet.encoder.stages.1.layers.0.layer.2.convolution.weight', 'resnet.encoder.stages.3.layers.0.layer.2.convolution.weight', 'resnet.encoder.stages.2.layers.1.layer.0.normalization.running_mean', 'resnet.encoder.stages.1.layers.3.layer.2.normalization.bias', 'resnet.encoder.stages.2.layers.5.layer.0.normalization.running_mean', 'resnet.encoder.stages.1.layers.3.layer.2.convolution.weight', 'classifier.1.bias', 'resnet.encoder.stages.1.layers.2.layer.2.normalization.num_batches_tracked', 'resnet.encoder.stages.1.layers.0.layer.0.normalization.running_var', 'resnet.encoder.stages.2.layers.5.layer.1.convolution.weight', 'resnet.encoder.stages.0.layers.2.layer.2.normalization.running_var', 'resnet.encoder.stages.1.layers.0.layer.0.normalization.bias', 'resnet.encoder.stages.2.layers.0.layer.2.normalization.num_batches_tracked', 'resnet.encoder.stages.2.layers.5.layer.0.normalization.bias', 'resnet.encoder.stages.0.layers.2.layer.2.normalization.bias', 'resnet.encoder.stages.2.layers.2.layer.0.normalization.num_batches_tracked', 'resnet.encoder.stages.0.layers.1.layer.0.normalization.running_mean', 'resnet.encoder.stages.0.layers.2.layer.0.normalization.num_batches_tracked', 'classifier.1.weight', 'resnet.encoder.stages.0.layers.1.layer.1.normalization.bias', 'resnet.encoder.stages.0.layers.1.layer.2.normalization.weight', 'resnet.encoder.stages.0.layers.0.layer.1.normalization.bias', 'resnet.encoder.stages.0.layers.0.layer.0.normalization.running_var', 'resnet.encoder.stages.2.layers.2.layer.0.normalization.bias', 'resnet.encoder.stages.3.layers.2.layer.2.normalization.num_batches_tracked', 'resnet.encoder.stages.2.layers.4.layer.1.convolution.weight', 'resnet.encoder.stages.2.layers.3.layer.1.normalization.running_mean', 'resnet.encoder.stages.0.layers.2.layer.1.normalization.weight', 'resnet.encoder.stages.1.layers.2.layer.2.normalization.running_var', 'resnet.encoder.stages.0.layers.0.shortcut.normalization.bias', 'resnet.encoder.stages.1.layers.0.shortcut.normalization.running_mean', 'resnet.encoder.stages.1.layers.1.layer.1.normalization.running_mean', 'resnet.encoder.stages.0.layers.1.layer.0.normalization.num_batches_tracked', 'resnet.embedder.embedder.normalization.running_mean', 'resnet.encoder.stages.2.layers.2.layer.0.convolution.weight', 'resnet.encoder.stages.0.layers.0.layer.0.convolution.weight', 'resnet.encoder.stages.2.layers.3.layer.2.normalization.bias', 'resnet.encoder.stages.2.layers.5.layer.0.normalization.num_batches_tracked', 'resnet.encoder.stages.3.layers.1.layer.0.normalization.weight', 'resnet.encoder.stages.2.layers.0.shortcut.normalization.bias', 'resnet.encoder.stages.2.layers.3.layer.2.normalization.weight', 'resnet.encoder.stages.0.layers.0.shortcut.normalization.running_mean', 'resnet.encoder.stages.3.layers.0.layer.1.normalization.weight', 'resnet.encoder.stages.0.layers.2.layer.0.normalization.weight', 'resnet.encoder.stages.2.layers.4.layer.0.normalization.weight', 'resnet.encoder.stages.2.layers.2.layer.0.normalization.running_var', 'resnet.encoder.stages.0.layers.2.layer.2.convolution.weight', 'resnet.encoder.stages.2.layers.1.layer.2.normalization.weight', 'resnet.encoder.stages.1.layers.0.layer.2.normalization.bias', 'resnet.encoder.stages.1.layers.1.layer.0.normalization.bias', 'resnet.encoder.stages.2.layers.1.layer.1.normalization.weight', 'resnet.encoder.stages.2.layers.3.layer.0.normalization.num_batches_tracked', 'resnet.encoder.stages.3.layers.1.layer.0.normalization.running_var', 'resnet.encoder.stages.2.layers.1.layer.0.normalization.bias', 'resnet.encoder.stages.1.layers.2.layer.1.normalization.running_var', 'resnet.encoder.stages.0.layers.1.layer.0.normalization.bias', 'resnet.encoder.stages.0.layers.0.layer.1.convolution.weight', 'resnet.encoder.stages.3.layers.1.layer.2.normalization.num_batches_tracked', 'resnet.encoder.stages.0.layers.0.layer.0.normalization.weight', 'resnet.encoder.stages.1.layers.1.layer.1.normalization.num_batches_tracked', 'resnet.encoder.stages.1.layers.1.layer.2.normalization.running_mean', 'resnet.encoder.stages.2.layers.2.layer.1.normalization.running_mean', 'resnet.encoder.stages.2.layers.5.layer.1.normalization.running_var', 'resnet.encoder.stages.1.layers.2.layer.2.normalization.bias', 'resnet.encoder.stages.1.layers.0.layer.1.normalization.weight', 'resnet.encoder.stages.1.layers.1.layer.2.convolution.weight', 'resnet.encoder.stages.2.layers.1.layer.0.convolution.weight', 'resnet.encoder.stages.3.layers.1.layer.0.convolution.weight', 'resnet.encoder.stages.2.layers.5.layer.1.normalization.running_mean', 'resnet.encoder.stages.3.layers.0.shortcut.normalization.running_var', 'resnet.encoder.stages.0.layers.2.layer.2.normalization.num_batches_tracked', 'resnet.encoder.stages.1.layers.1.layer.1.normalization.weight', 'resnet.encoder.stages.0.layers.1.layer.1.normalization.num_batches_tracked', 'resnet.encoder.stages.3.layers.0.layer.2.normalization.bias', 'resnet.encoder.stages.2.layers.0.layer.0.normalization.running_mean', 'resnet.encoder.stages.0.layers.2.layer.1.normalization.num_batches_tracked', 'resnet.encoder.stages.2.layers.3.layer.1.normalization.weight', 'resnet.encoder.stages.1.layers.3.layer.1.normalization.running_mean', 'resnet.encoder.stages.1.layers.0.layer.2.normalization.running_mean', 'resnet.encoder.stages.2.layers.0.layer.2.normalization.weight', 'resnet.encoder.stages.2.layers.0.layer.0.convolution.weight', 'resnet.encoder.stages.1.layers.0.shortcut.normalization.bias', 'resnet.encoder.stages.2.layers.2.layer.2.normalization.weight', 'resnet.encoder.stages.0.layers.2.layer.2.normalization.running_mean', 'resnet.encoder.stages.1.layers.1.layer.2.normalization.num_batches_tracked', 'resnet.encoder.stages.2.layers.0.layer.1.convolution.weight', 'resnet.encoder.stages.1.layers.3.layer.2.normalization.running_mean', 'resnet.encoder.stages.3.layers.2.layer.2.normalization.running_mean', 'resnet.encoder.stages.2.layers.4.layer.0.normalization.running_var', 'resnet.encoder.stages.1.layers.1.layer.2.normalization.weight', 'resnet.embedder.embedder.normalization.bias', 'resnet.encoder.stages.1.layers.3.layer.0.normalization.weight', 'resnet.encoder.stages.3.layers.0.shortcut.normalization.running_mean', 'resnet.encoder.stages.1.layers.3.layer.1.normalization.bias', 'resnet.encoder.stages.0.layers.0.shortcut.convolution.weight', 'resnet.encoder.stages.0.layers.1.layer.1.normalization.running_mean', 'resnet.encoder.stages.2.layers.2.layer.2.normalization.running_var', 'resnet.encoder.stages.3.layers.2.layer.0.normalization.running_mean', 'resnet.encoder.stages.2.layers.1.layer.1.normalization.bias', 'resnet.encoder.stages.0.layers.0.layer.1.normalization.weight', 'resnet.encoder.stages.2.layers.0.layer.2.normalization.running_var', 'resnet.encoder.stages.2.layers.0.shortcut.convolution.weight', 'resnet.encoder.stages.1.layers.0.shortcut.convolution.weight', 'resnet.encoder.stages.2.layers.3.layer.1.normalization.num_batches_tracked', 'resnet.encoder.stages.2.layers.1.layer.0.normalization.weight', 'resnet.encoder.stages.1.layers.3.layer.2.normalization.running_var', 'resnet.encoder.stages.2.layers.2.layer.0.normalization.running_mean']\n",
            "- This IS expected if you are initializing ViTForImageClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing ViTForImageClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of ViTForImageClassification were not initialized from the model checkpoint at microsoft/resnet-50 and are newly initialized: ['encoder.layer.10.attention.output.dense.bias', 'encoder.layer.11.layernorm_after.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.3.layernorm_before.bias', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.4.output.dense.bias', 'encoder.layer.7.attention.attention.value.weight', 'encoder.layer.11.layernorm_before.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.8.attention.attention.query.bias', 'encoder.layer.1.attention.attention.key.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.8.attention.attention.value.bias', 'encoder.layer.11.layernorm_before.weight', 'encoder.layer.7.attention.attention.query.bias', 'encoder.layer.4.attention.attention.key.bias', 'encoder.layer.1.attention.attention.query.bias', 'encoder.layer.9.layernorm_before.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.10.attention.attention.value.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.9.attention.attention.key.bias', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.9.layernorm_before.bias', 'encoder.layer.8.attention.attention.query.weight', 'encoder.layer.9.output.dense.weight', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.5.attention.attention.query.weight', 'encoder.layer.7.attention.attention.query.weight', 'encoder.layer.4.layernorm_before.weight', 'embeddings.patch_embeddings.projection.weight', 'encoder.layer.0.layernorm_before.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.3.layernorm_before.weight', 'encoder.layer.6.attention.attention.query.weight', 'encoder.layer.3.attention.attention.query.weight', 'encoder.layer.7.output.dense.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.5.attention.attention.value.bias', 'encoder.layer.10.attention.attention.key.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.7.layernorm_after.weight', 'embeddings.position_embeddings', 'encoder.layer.2.layernorm_after.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.11.attention.attention.query.weight', 'encoder.layer.5.attention.attention.query.bias', 'encoder.layer.5.layernorm_after.bias', 'encoder.layer.9.attention.attention.query.weight', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.2.layernorm_before.bias', 'encoder.layer.2.output.dense.bias', 'encoder.layer.5.attention.attention.key.bias', 'encoder.layer.8.output.dense.bias', 'encoder.layer.1.layernorm_after.weight', 'encoder.layer.5.attention.attention.value.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.10.attention.attention.key.weight', 'encoder.layer.2.attention.attention.value.bias', 'encoder.layer.11.output.dense.bias', 'encoder.layer.9.attention.attention.value.weight', 'encoder.layer.5.layernorm_before.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.2.attention.attention.query.bias', 'encoder.layer.9.attention.attention.key.weight', 'encoder.layer.6.layernorm_after.weight', 'encoder.layer.6.attention.attention.value.bias', 'encoder.layer.11.attention.attention.value.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.6.output.dense.weight', 'encoder.layer.0.layernorm_after.weight', 'encoder.layer.4.attention.attention.key.weight', 'classifier.weight', 'encoder.layer.4.attention.attention.query.bias', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.9.attention.attention.query.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.4.layernorm_after.weight', 'encoder.layer.0.attention.attention.query.bias', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.11.layernorm_after.bias', 'encoder.layer.9.layernorm_after.bias', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.0.output.dense.bias', 'encoder.layer.8.layernorm_after.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.3.attention.attention.value.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.6.layernorm_before.weight', 'encoder.layer.8.attention.attention.key.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.3.layernorm_after.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.4.attention.attention.value.bias', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.9.output.dense.bias', 'encoder.layer.1.attention.attention.query.weight', 'encoder.layer.2.attention.attention.key.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.6.attention.attention.query.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.9.layernorm_after.weight', 'encoder.layer.3.attention.attention.key.bias', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.0.attention.attention.query.weight', 'encoder.layer.1.attention.attention.value.weight', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.8.layernorm_before.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.6.layernorm_before.bias', 'encoder.layer.2.attention.attention.value.weight', 'encoder.layer.1.attention.attention.value.bias', 'encoder.layer.7.attention.attention.key.bias', 'encoder.layer.6.output.dense.bias', 'layernorm.weight', 'encoder.layer.3.layernorm_after.bias', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.3.attention.attention.query.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.11.attention.attention.key.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.2.attention.attention.query.weight', 'encoder.layer.0.layernorm_after.bias', 'encoder.layer.6.layernorm_after.bias', 'encoder.layer.7.output.dense.bias', 'encoder.layer.6.attention.attention.key.weight', 'encoder.layer.5.layernorm_after.weight', 'embeddings.cls_token', 'encoder.layer.7.layernorm_before.weight', 'encoder.layer.8.layernorm_before.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.0.attention.attention.value.bias', 'classifier.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.1.attention.attention.key.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.11.attention.attention.value.bias', 'encoder.layer.10.layernorm_after.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.2.layernorm_before.weight', 'encoder.layer.10.layernorm_before.bias', 'encoder.layer.5.attention.attention.key.weight', 'encoder.layer.3.attention.attention.value.weight', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.7.attention.attention.key.weight', 'encoder.layer.0.layernorm_before.weight', 'encoder.layer.4.attention.attention.query.weight', 'embeddings.patch_embeddings.projection.bias', 'encoder.layer.7.layernorm_before.bias', 'encoder.layer.7.layernorm_after.bias', 'encoder.layer.3.output.dense.bias', 'encoder.layer.7.attention.attention.value.bias', 'encoder.layer.6.attention.attention.value.weight', 'encoder.layer.2.attention.attention.key.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.10.layernorm_after.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.4.layernorm_after.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.4.attention.attention.value.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.2.layernorm_after.weight', 'encoder.layer.0.attention.attention.value.weight', 'encoder.layer.2.intermediate.dense.bias', 'layernorm.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.5.output.dense.weight', 'encoder.layer.9.attention.attention.value.bias', 'encoder.layer.8.attention.attention.value.weight', 'encoder.layer.8.layernorm_after.bias', 'encoder.layer.1.layernorm_after.bias', 'encoder.layer.10.layernorm_before.weight', 'encoder.layer.0.attention.attention.key.bias', 'encoder.layer.11.attention.attention.key.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.10.attention.attention.query.weight', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.10.attention.attention.query.bias', 'encoder.layer.3.attention.attention.key.weight', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.8.attention.attention.key.bias', 'encoder.layer.10.attention.attention.value.bias', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.4.layernorm_before.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.1.layernorm_before.weight', 'encoder.layer.6.attention.attention.key.bias', 'encoder.layer.1.layernorm_before.bias', 'encoder.layer.0.attention.attention.key.weight', 'encoder.layer.11.attention.attention.query.bias', 'encoder.layer.5.layernorm_before.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "# define the model\n",
        "def model_init():\n",
        "  return ViTForImageClassification.from_pretrained(\n",
        "    CNN_base,\n",
        "    num_labels=len(labels_names),\n",
        "    id2label={str(i): tumor_type for i, tumor_type in enumerate(labels_names)},\n",
        "    label2id={tumor_type: i for i, tumor_type in enumerate(labels_names)},\n",
        ")\n",
        "\n",
        "model = model_init()\n",
        "# Initialize a new run\n",
        "wandb.init(project=\"cnn_no_aug\")\n",
        "\n",
        "# Define the TrainingArguments\n",
        "training_args = TrainingArguments(\n",
        "  output_dir=output_cnn_no_aug,\n",
        "  per_device_train_batch_size=16,\n",
        "  evaluation_strategy=\"steps\",\n",
        "  num_train_epochs=5, #4\n",
        "  fp16=True,\n",
        "  save_steps=100,#100\n",
        "  eval_steps=50,#100\n",
        "  logging_steps=10,\n",
        "  learning_rate=2e-4,\n",
        "  save_total_limit=2,\n",
        "  remove_unused_columns=False,\n",
        "  push_to_hub=False,\n",
        "  report_to='wandb',\n",
        "  load_best_model_at_end=True,\n",
        "  seed = seed\n",
        ")\n",
        "\n",
        "\n",
        "trainer = Trainer(\n",
        "    model_init=model_init,\n",
        "    args=training_args,\n",
        "    compute_metrics=compute_metrics,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset,\n",
        "    data_collator= lambda samples: {'pixel_values': torch.stack([sample[0] for sample in samples]),\n",
        "                                   'labels': torch.tensor([sample[1] for sample in samples])},\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "SjE4QFApSppD",
        "outputId": "04e4b2b1-f684-43eb-859d-51964124fa12"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You are using a model of type resnet to instantiate a model of type vit. This is not supported for all configurations of models and can yield errors.\n",
            "Some weights of the model checkpoint at microsoft/resnet-50 were not used when initializing ViTForImageClassification: ['resnet.embedder.embedder.normalization.running_var', 'resnet.encoder.stages.3.layers.2.layer.1.normalization.running_mean', 'resnet.encoder.stages.1.layers.0.layer.0.normalization.num_batches_tracked', 'resnet.encoder.stages.2.layers.3.layer.0.normalization.running_mean', 'resnet.encoder.stages.1.layers.3.layer.2.normalization.weight', 'resnet.encoder.stages.1.layers.3.layer.1.convolution.weight', 'resnet.encoder.stages.2.layers.1.layer.2.normalization.num_batches_tracked', 'resnet.encoder.stages.2.layers.0.layer.1.normalization.running_var', 'resnet.encoder.stages.2.layers.2.layer.2.normalization.running_mean', 'resnet.encoder.stages.3.layers.2.layer.0.normalization.bias', 'resnet.encoder.stages.1.layers.1.layer.0.normalization.running_var', 'resnet.embedder.embedder.convolution.weight', 'resnet.embedder.embedder.normalization.weight', 'resnet.encoder.stages.3.layers.2.layer.0.normalization.weight', 'resnet.encoder.stages.2.layers.0.layer.1.normalization.bias', 'resnet.encoder.stages.2.layers.4.layer.0.normalization.running_mean', 'resnet.encoder.stages.0.layers.2.layer.0.normalization.running_mean', 'resnet.encoder.stages.0.layers.0.layer.2.normalization.running_mean', 'resnet.encoder.stages.1.layers.1.layer.1.normalization.running_var', 'resnet.encoder.stages.1.layers.0.layer.0.convolution.weight', 'resnet.encoder.stages.1.layers.3.layer.0.normalization.bias', 'resnet.encoder.stages.3.layers.2.layer.2.normalization.bias', 'resnet.encoder.stages.2.layers.4.layer.1.normalization.running_mean', 'resnet.encoder.stages.3.layers.0.layer.1.normalization.num_batches_tracked', 'resnet.encoder.stages.3.layers.2.layer.1.normalization.running_var', 'resnet.encoder.stages.0.layers.1.layer.1.normalization.weight', 'resnet.encoder.stages.3.layers.1.layer.1.normalization.bias', 'resnet.encoder.stages.2.layers.5.layer.0.convolution.weight', 'resnet.encoder.stages.0.layers.2.layer.1.normalization.running_mean', 'resnet.encoder.stages.0.layers.1.layer.2.normalization.running_mean', 'resnet.encoder.stages.0.layers.0.layer.0.normalization.num_batches_tracked', 'resnet.encoder.stages.1.layers.1.layer.1.normalization.bias', 'resnet.encoder.stages.3.layers.0.layer.0.normalization.bias', 'resnet.encoder.stages.2.layers.4.layer.2.convolution.weight', 'resnet.encoder.stages.0.layers.1.layer.2.normalization.bias', 'resnet.encoder.stages.2.layers.5.layer.2.normalization.num_batches_tracked', 'resnet.encoder.stages.0.layers.0.layer.1.normalization.num_batches_tracked', 'resnet.encoder.stages.3.layers.0.layer.2.normalization.num_batches_tracked', 'resnet.encoder.stages.2.layers.1.layer.0.normalization.num_batches_tracked', 'resnet.encoder.stages.3.layers.1.layer.1.normalization.running_mean', 'resnet.encoder.stages.1.layers.0.layer.1.normalization.bias', 'resnet.encoder.stages.2.layers.3.layer.1.convolution.weight', 'resnet.encoder.stages.2.layers.3.layer.0.normalization.running_var', 'resnet.encoder.stages.3.layers.0.shortcut.convolution.weight', 'resnet.encoder.stages.0.layers.0.layer.2.convolution.weight', 'resnet.encoder.stages.1.layers.3.layer.2.normalization.num_batches_tracked', 'resnet.encoder.stages.3.layers.0.layer.1.convolution.weight', 'resnet.encoder.stages.3.layers.0.layer.2.normalization.running_mean', 'resnet.encoder.stages.3.layers.0.layer.0.normalization.weight', 'resnet.encoder.stages.2.layers.5.layer.0.normalization.weight', 'resnet.encoder.stages.1.layers.1.layer.2.normalization.bias', 'resnet.encoder.stages.2.layers.0.layer.0.normalization.num_batches_tracked', 'resnet.embedder.embedder.normalization.num_batches_tracked', 'resnet.encoder.stages.1.layers.2.layer.1.normalization.weight', 'resnet.encoder.stages.1.layers.3.layer.1.normalization.weight', 'resnet.encoder.stages.2.layers.5.layer.2.normalization.running_mean', 'resnet.encoder.stages.1.layers.0.layer.1.normalization.running_var', 'resnet.encoder.stages.3.layers.2.layer.0.normalization.running_var', 'resnet.encoder.stages.0.layers.0.layer.1.normalization.running_mean', 'resnet.encoder.stages.2.layers.0.layer.2.convolution.weight', 'resnet.encoder.stages.1.layers.2.layer.1.normalization.running_mean', 'resnet.encoder.stages.3.layers.1.layer.1.normalization.num_batches_tracked', 'resnet.encoder.stages.1.layers.0.shortcut.normalization.running_var', 'resnet.encoder.stages.2.layers.2.layer.2.normalization.num_batches_tracked', 'resnet.encoder.stages.3.layers.2.layer.1.normalization.bias', 'resnet.encoder.stages.1.layers.3.layer.0.normalization.running_var', 'resnet.encoder.stages.2.layers.1.layer.1.normalization.num_batches_tracked', 'resnet.encoder.stages.1.layers.1.layer.0.convolution.weight', 'resnet.encoder.stages.3.layers.0.layer.0.normalization.running_var', 'resnet.encoder.stages.2.layers.0.layer.1.normalization.num_batches_tracked', 'resnet.encoder.stages.1.layers.2.layer.2.normalization.running_mean', 'resnet.encoder.stages.2.layers.4.layer.0.normalization.bias', 'resnet.encoder.stages.2.layers.3.layer.0.convolution.weight', 'resnet.encoder.stages.2.layers.4.layer.1.normalization.num_batches_tracked', 'resnet.encoder.stages.3.layers.1.layer.2.normalization.weight', 'resnet.encoder.stages.3.layers.0.layer.2.normalization.running_var', 'resnet.encoder.stages.3.layers.0.layer.0.normalization.running_mean', 'resnet.encoder.stages.1.layers.0.layer.0.normalization.weight', 'resnet.encoder.stages.0.layers.0.layer.1.normalization.running_var', 'resnet.encoder.stages.2.layers.4.layer.2.normalization.num_batches_tracked', 'resnet.encoder.stages.2.layers.0.layer.1.normalization.weight', 'resnet.encoder.stages.2.layers.3.layer.0.normalization.bias', 'resnet.encoder.stages.0.layers.0.shortcut.normalization.num_batches_tracked', 'resnet.encoder.stages.0.layers.0.shortcut.normalization.running_var', 'resnet.encoder.stages.3.layers.1.layer.2.normalization.running_var', 'resnet.encoder.stages.0.layers.2.layer.1.convolution.weight', 'resnet.encoder.stages.0.layers.0.layer.2.normalization.bias', 'resnet.encoder.stages.1.layers.1.layer.2.normalization.running_var', 'resnet.encoder.stages.2.layers.4.layer.1.normalization.weight', 'resnet.encoder.stages.3.layers.0.layer.0.convolution.weight', 'resnet.encoder.stages.1.layers.0.shortcut.normalization.weight', 'resnet.encoder.stages.2.layers.2.layer.1.normalization.weight', 'resnet.encoder.stages.3.layers.2.layer.1.convolution.weight', 'resnet.encoder.stages.3.layers.1.layer.0.normalization.num_batches_tracked', 'resnet.encoder.stages.0.layers.2.layer.2.normalization.weight', 'resnet.encoder.stages.2.layers.4.layer.0.convolution.weight', 'resnet.encoder.stages.3.layers.2.layer.0.normalization.num_batches_tracked', 'resnet.encoder.stages.2.layers.2.layer.1.normalization.running_var', 'resnet.encoder.stages.2.layers.4.layer.2.normalization.running_mean', 'resnet.encoder.stages.3.layers.2.layer.1.normalization.weight', 'resnet.encoder.stages.0.layers.1.layer.1.convolution.weight', 'resnet.encoder.stages.2.layers.2.layer.1.normalization.bias', 'resnet.encoder.stages.1.layers.1.layer.0.normalization.num_batches_tracked', 'resnet.encoder.stages.1.layers.2.layer.0.convolution.weight', 'resnet.encoder.stages.2.layers.3.layer.1.normalization.running_var', 'resnet.encoder.stages.1.layers.2.layer.2.convolution.weight', 'resnet.encoder.stages.1.layers.3.layer.1.normalization.num_batches_tracked', 'resnet.encoder.stages.1.layers.0.layer.2.normalization.weight', 'resnet.encoder.stages.1.layers.2.layer.0.normalization.bias', 'resnet.encoder.stages.0.layers.1.layer.0.normalization.running_var', 'resnet.encoder.stages.1.layers.0.layer.1.normalization.running_mean', 'resnet.encoder.stages.3.layers.2.layer.1.normalization.num_batches_tracked', 'resnet.encoder.stages.2.layers.5.layer.1.normalization.weight', 'resnet.encoder.stages.3.layers.2.layer.2.normalization.weight', 'resnet.encoder.stages.3.layers.1.layer.0.normalization.bias', 'resnet.encoder.stages.3.layers.1.layer.0.normalization.running_mean', 'resnet.encoder.stages.2.layers.1.layer.2.convolution.weight', 'resnet.encoder.stages.0.layers.2.layer.0.normalization.bias', 'resnet.encoder.stages.2.layers.4.layer.2.normalization.running_var', 'resnet.encoder.stages.1.layers.3.layer.0.convolution.weight', 'resnet.encoder.stages.2.layers.0.layer.1.normalization.running_mean', 'resnet.encoder.stages.2.layers.0.shortcut.normalization.running_mean', 'resnet.encoder.stages.0.layers.2.layer.0.normalization.running_var', 'resnet.encoder.stages.1.layers.2.layer.0.normalization.weight', 'resnet.encoder.stages.2.layers.3.layer.2.convolution.weight', 'resnet.encoder.stages.2.layers.4.layer.1.normalization.bias', 'resnet.encoder.stages.0.layers.0.layer.0.normalization.running_mean', 'resnet.encoder.stages.2.layers.1.layer.2.normalization.running_mean', 'resnet.encoder.stages.1.layers.0.layer.2.normalization.num_batches_tracked', 'resnet.encoder.stages.2.layers.0.shortcut.normalization.running_var', 'resnet.encoder.stages.2.layers.4.layer.2.normalization.weight', 'resnet.encoder.stages.2.layers.2.layer.1.normalization.num_batches_tracked', 'resnet.encoder.stages.1.layers.1.layer.1.convolution.weight', 'resnet.encoder.stages.0.layers.1.layer.2.convolution.weight', 'resnet.encoder.stages.1.layers.3.layer.0.normalization.num_batches_tracked', 'resnet.encoder.stages.2.layers.3.layer.0.normalization.weight', 'resnet.encoder.stages.0.layers.1.layer.2.normalization.num_batches_tracked', 'resnet.encoder.stages.2.layers.2.layer.0.normalization.weight', 'resnet.encoder.stages.2.layers.0.layer.0.normalization.running_var', 'resnet.encoder.stages.1.layers.2.layer.0.normalization.running_mean', 'resnet.encoder.stages.0.layers.1.layer.2.normalization.running_var', 'resnet.encoder.stages.0.layers.0.layer.0.normalization.bias', 'resnet.encoder.stages.3.layers.0.layer.0.normalization.num_batches_tracked', 'resnet.encoder.stages.0.layers.0.layer.2.normalization.weight', 'resnet.encoder.stages.3.layers.0.shortcut.normalization.bias', 'resnet.encoder.stages.2.layers.3.layer.2.normalization.running_mean', 'resnet.encoder.stages.1.layers.3.layer.1.normalization.running_var', 'resnet.encoder.stages.2.layers.3.layer.2.normalization.running_var', 'resnet.encoder.stages.1.layers.2.layer.0.normalization.num_batches_tracked', 'resnet.encoder.stages.3.layers.1.layer.2.normalization.running_mean', 'resnet.encoder.stages.2.layers.0.layer.0.normalization.bias', 'resnet.encoder.stages.2.layers.1.layer.0.normalization.running_var', 'resnet.encoder.stages.2.layers.3.layer.2.normalization.num_batches_tracked', 'resnet.encoder.stages.0.layers.1.layer.1.normalization.running_var', 'resnet.encoder.stages.2.layers.0.shortcut.normalization.num_batches_tracked', 'resnet.encoder.stages.0.layers.2.layer.1.normalization.running_var', 'resnet.encoder.stages.3.layers.0.layer.1.normalization.bias', 'resnet.encoder.stages.0.layers.0.layer.2.normalization.running_var', 'resnet.encoder.stages.2.layers.1.layer.2.normalization.running_var', 'resnet.encoder.stages.1.layers.3.layer.0.normalization.running_mean', 'resnet.encoder.stages.3.layers.0.shortcut.normalization.weight', 'resnet.encoder.stages.1.layers.2.layer.1.normalization.bias', 'resnet.encoder.stages.3.layers.0.layer.1.normalization.running_mean', 'resnet.encoder.stages.2.layers.3.layer.1.normalization.bias', 'resnet.encoder.stages.3.layers.0.layer.2.normalization.weight', 'resnet.encoder.stages.2.layers.4.layer.2.normalization.bias', 'resnet.encoder.stages.2.layers.0.layer.0.normalization.weight', 'resnet.encoder.stages.0.layers.1.layer.0.convolution.weight', 'resnet.encoder.stages.3.layers.1.layer.1.convolution.weight', 'resnet.encoder.stages.3.layers.1.layer.2.normalization.bias', 'resnet.encoder.stages.0.layers.1.layer.0.normalization.weight', 'resnet.encoder.stages.2.layers.2.layer.2.convolution.weight', 'resnet.encoder.stages.2.layers.5.layer.2.normalization.bias', 'resnet.encoder.stages.0.layers.2.layer.0.convolution.weight', 'resnet.encoder.stages.2.layers.5.layer.2.normalization.weight', 'resnet.encoder.stages.2.layers.4.layer.0.normalization.num_batches_tracked', 'resnet.encoder.stages.1.layers.2.layer.0.normalization.running_var', 'resnet.encoder.stages.3.layers.2.layer.2.normalization.running_var', 'resnet.encoder.stages.2.layers.5.layer.1.normalization.num_batches_tracked', 'resnet.encoder.stages.2.layers.5.layer.2.convolution.weight', 'resnet.encoder.stages.2.layers.1.layer.1.normalization.running_var', 'resnet.encoder.stages.1.layers.1.layer.0.normalization.running_mean', 'resnet.encoder.stages.2.layers.2.layer.1.convolution.weight', 'resnet.encoder.stages.2.layers.0.shortcut.normalization.weight', 'resnet.encoder.stages.2.layers.0.layer.2.normalization.running_mean', 'resnet.encoder.stages.0.layers.0.shortcut.normalization.weight', 'resnet.encoder.stages.0.layers.2.layer.1.normalization.bias', 'resnet.encoder.stages.3.layers.0.shortcut.normalization.num_batches_tracked', 'resnet.encoder.stages.3.layers.1.layer.2.convolution.weight', 'resnet.encoder.stages.1.layers.0.layer.1.convolution.weight', 'resnet.encoder.stages.3.layers.0.layer.1.normalization.running_var', 'resnet.encoder.stages.3.layers.1.layer.1.normalization.running_var', 'resnet.encoder.stages.3.layers.2.layer.2.convolution.weight', 'resnet.encoder.stages.2.layers.0.layer.2.normalization.bias', 'resnet.encoder.stages.1.layers.2.layer.1.normalization.num_batches_tracked', 'resnet.encoder.stages.1.layers.0.shortcut.normalization.num_batches_tracked', 'resnet.encoder.stages.1.layers.2.layer.2.normalization.weight', 'resnet.encoder.stages.2.layers.1.layer.1.normalization.running_mean', 'resnet.encoder.stages.1.layers.1.layer.0.normalization.weight', 'resnet.encoder.stages.3.layers.1.layer.1.normalization.weight', 'resnet.encoder.stages.2.layers.4.layer.1.normalization.running_var', 'resnet.encoder.stages.2.layers.5.layer.0.normalization.running_var', 'resnet.encoder.stages.2.layers.5.layer.1.normalization.bias', 'resnet.encoder.stages.2.layers.1.layer.1.convolution.weight', 'resnet.encoder.stages.2.layers.2.layer.2.normalization.bias', 'resnet.encoder.stages.1.layers.0.layer.2.normalization.running_var', 'resnet.encoder.stages.1.layers.0.layer.0.normalization.running_mean', 'resnet.encoder.stages.3.layers.2.layer.0.convolution.weight', 'resnet.encoder.stages.1.layers.0.layer.1.normalization.num_batches_tracked', 'resnet.encoder.stages.1.layers.2.layer.1.convolution.weight', 'resnet.encoder.stages.0.layers.0.layer.2.normalization.num_batches_tracked', 'resnet.encoder.stages.2.layers.1.layer.2.normalization.bias', 'resnet.encoder.stages.2.layers.5.layer.2.normalization.running_var', 'resnet.encoder.stages.1.layers.0.layer.2.convolution.weight', 'resnet.encoder.stages.3.layers.0.layer.2.convolution.weight', 'resnet.encoder.stages.2.layers.1.layer.0.normalization.running_mean', 'resnet.encoder.stages.1.layers.3.layer.2.normalization.bias', 'resnet.encoder.stages.2.layers.5.layer.0.normalization.running_mean', 'resnet.encoder.stages.1.layers.3.layer.2.convolution.weight', 'classifier.1.bias', 'resnet.encoder.stages.1.layers.2.layer.2.normalization.num_batches_tracked', 'resnet.encoder.stages.1.layers.0.layer.0.normalization.running_var', 'resnet.encoder.stages.2.layers.5.layer.1.convolution.weight', 'resnet.encoder.stages.0.layers.2.layer.2.normalization.running_var', 'resnet.encoder.stages.1.layers.0.layer.0.normalization.bias', 'resnet.encoder.stages.2.layers.0.layer.2.normalization.num_batches_tracked', 'resnet.encoder.stages.2.layers.5.layer.0.normalization.bias', 'resnet.encoder.stages.0.layers.2.layer.2.normalization.bias', 'resnet.encoder.stages.2.layers.2.layer.0.normalization.num_batches_tracked', 'resnet.encoder.stages.0.layers.1.layer.0.normalization.running_mean', 'resnet.encoder.stages.0.layers.2.layer.0.normalization.num_batches_tracked', 'classifier.1.weight', 'resnet.encoder.stages.0.layers.1.layer.1.normalization.bias', 'resnet.encoder.stages.0.layers.1.layer.2.normalization.weight', 'resnet.encoder.stages.0.layers.0.layer.1.normalization.bias', 'resnet.encoder.stages.0.layers.0.layer.0.normalization.running_var', 'resnet.encoder.stages.2.layers.2.layer.0.normalization.bias', 'resnet.encoder.stages.3.layers.2.layer.2.normalization.num_batches_tracked', 'resnet.encoder.stages.2.layers.4.layer.1.convolution.weight', 'resnet.encoder.stages.2.layers.3.layer.1.normalization.running_mean', 'resnet.encoder.stages.0.layers.2.layer.1.normalization.weight', 'resnet.encoder.stages.1.layers.2.layer.2.normalization.running_var', 'resnet.encoder.stages.0.layers.0.shortcut.normalization.bias', 'resnet.encoder.stages.1.layers.0.shortcut.normalization.running_mean', 'resnet.encoder.stages.1.layers.1.layer.1.normalization.running_mean', 'resnet.encoder.stages.0.layers.1.layer.0.normalization.num_batches_tracked', 'resnet.embedder.embedder.normalization.running_mean', 'resnet.encoder.stages.2.layers.2.layer.0.convolution.weight', 'resnet.encoder.stages.0.layers.0.layer.0.convolution.weight', 'resnet.encoder.stages.2.layers.3.layer.2.normalization.bias', 'resnet.encoder.stages.2.layers.5.layer.0.normalization.num_batches_tracked', 'resnet.encoder.stages.3.layers.1.layer.0.normalization.weight', 'resnet.encoder.stages.2.layers.0.shortcut.normalization.bias', 'resnet.encoder.stages.2.layers.3.layer.2.normalization.weight', 'resnet.encoder.stages.0.layers.0.shortcut.normalization.running_mean', 'resnet.encoder.stages.3.layers.0.layer.1.normalization.weight', 'resnet.encoder.stages.0.layers.2.layer.0.normalization.weight', 'resnet.encoder.stages.2.layers.4.layer.0.normalization.weight', 'resnet.encoder.stages.2.layers.2.layer.0.normalization.running_var', 'resnet.encoder.stages.0.layers.2.layer.2.convolution.weight', 'resnet.encoder.stages.2.layers.1.layer.2.normalization.weight', 'resnet.encoder.stages.1.layers.0.layer.2.normalization.bias', 'resnet.encoder.stages.1.layers.1.layer.0.normalization.bias', 'resnet.encoder.stages.2.layers.1.layer.1.normalization.weight', 'resnet.encoder.stages.2.layers.3.layer.0.normalization.num_batches_tracked', 'resnet.encoder.stages.3.layers.1.layer.0.normalization.running_var', 'resnet.encoder.stages.2.layers.1.layer.0.normalization.bias', 'resnet.encoder.stages.1.layers.2.layer.1.normalization.running_var', 'resnet.encoder.stages.0.layers.1.layer.0.normalization.bias', 'resnet.encoder.stages.0.layers.0.layer.1.convolution.weight', 'resnet.encoder.stages.3.layers.1.layer.2.normalization.num_batches_tracked', 'resnet.encoder.stages.0.layers.0.layer.0.normalization.weight', 'resnet.encoder.stages.1.layers.1.layer.1.normalization.num_batches_tracked', 'resnet.encoder.stages.1.layers.1.layer.2.normalization.running_mean', 'resnet.encoder.stages.2.layers.2.layer.1.normalization.running_mean', 'resnet.encoder.stages.2.layers.5.layer.1.normalization.running_var', 'resnet.encoder.stages.1.layers.2.layer.2.normalization.bias', 'resnet.encoder.stages.1.layers.0.layer.1.normalization.weight', 'resnet.encoder.stages.1.layers.1.layer.2.convolution.weight', 'resnet.encoder.stages.2.layers.1.layer.0.convolution.weight', 'resnet.encoder.stages.3.layers.1.layer.0.convolution.weight', 'resnet.encoder.stages.2.layers.5.layer.1.normalization.running_mean', 'resnet.encoder.stages.3.layers.0.shortcut.normalization.running_var', 'resnet.encoder.stages.0.layers.2.layer.2.normalization.num_batches_tracked', 'resnet.encoder.stages.1.layers.1.layer.1.normalization.weight', 'resnet.encoder.stages.0.layers.1.layer.1.normalization.num_batches_tracked', 'resnet.encoder.stages.3.layers.0.layer.2.normalization.bias', 'resnet.encoder.stages.2.layers.0.layer.0.normalization.running_mean', 'resnet.encoder.stages.0.layers.2.layer.1.normalization.num_batches_tracked', 'resnet.encoder.stages.2.layers.3.layer.1.normalization.weight', 'resnet.encoder.stages.1.layers.3.layer.1.normalization.running_mean', 'resnet.encoder.stages.1.layers.0.layer.2.normalization.running_mean', 'resnet.encoder.stages.2.layers.0.layer.2.normalization.weight', 'resnet.encoder.stages.2.layers.0.layer.0.convolution.weight', 'resnet.encoder.stages.1.layers.0.shortcut.normalization.bias', 'resnet.encoder.stages.2.layers.2.layer.2.normalization.weight', 'resnet.encoder.stages.0.layers.2.layer.2.normalization.running_mean', 'resnet.encoder.stages.1.layers.1.layer.2.normalization.num_batches_tracked', 'resnet.encoder.stages.2.layers.0.layer.1.convolution.weight', 'resnet.encoder.stages.1.layers.3.layer.2.normalization.running_mean', 'resnet.encoder.stages.3.layers.2.layer.2.normalization.running_mean', 'resnet.encoder.stages.2.layers.4.layer.0.normalization.running_var', 'resnet.encoder.stages.1.layers.1.layer.2.normalization.weight', 'resnet.embedder.embedder.normalization.bias', 'resnet.encoder.stages.1.layers.3.layer.0.normalization.weight', 'resnet.encoder.stages.3.layers.0.shortcut.normalization.running_mean', 'resnet.encoder.stages.1.layers.3.layer.1.normalization.bias', 'resnet.encoder.stages.0.layers.0.shortcut.convolution.weight', 'resnet.encoder.stages.0.layers.1.layer.1.normalization.running_mean', 'resnet.encoder.stages.2.layers.2.layer.2.normalization.running_var', 'resnet.encoder.stages.3.layers.2.layer.0.normalization.running_mean', 'resnet.encoder.stages.2.layers.1.layer.1.normalization.bias', 'resnet.encoder.stages.0.layers.0.layer.1.normalization.weight', 'resnet.encoder.stages.2.layers.0.layer.2.normalization.running_var', 'resnet.encoder.stages.2.layers.0.shortcut.convolution.weight', 'resnet.encoder.stages.1.layers.0.shortcut.convolution.weight', 'resnet.encoder.stages.2.layers.3.layer.1.normalization.num_batches_tracked', 'resnet.encoder.stages.2.layers.1.layer.0.normalization.weight', 'resnet.encoder.stages.1.layers.3.layer.2.normalization.running_var', 'resnet.encoder.stages.2.layers.2.layer.0.normalization.running_mean']\n",
            "- This IS expected if you are initializing ViTForImageClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing ViTForImageClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of ViTForImageClassification were not initialized from the model checkpoint at microsoft/resnet-50 and are newly initialized: ['encoder.layer.10.attention.output.dense.bias', 'encoder.layer.11.layernorm_after.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.3.layernorm_before.bias', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.4.output.dense.bias', 'encoder.layer.7.attention.attention.value.weight', 'encoder.layer.11.layernorm_before.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.8.attention.attention.query.bias', 'encoder.layer.1.attention.attention.key.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.8.attention.attention.value.bias', 'encoder.layer.11.layernorm_before.weight', 'encoder.layer.7.attention.attention.query.bias', 'encoder.layer.4.attention.attention.key.bias', 'encoder.layer.1.attention.attention.query.bias', 'encoder.layer.9.layernorm_before.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.10.attention.attention.value.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.9.attention.attention.key.bias', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.9.layernorm_before.bias', 'encoder.layer.8.attention.attention.query.weight', 'encoder.layer.9.output.dense.weight', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.5.attention.attention.query.weight', 'encoder.layer.7.attention.attention.query.weight', 'encoder.layer.4.layernorm_before.weight', 'embeddings.patch_embeddings.projection.weight', 'encoder.layer.0.layernorm_before.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.3.layernorm_before.weight', 'encoder.layer.6.attention.attention.query.weight', 'encoder.layer.3.attention.attention.query.weight', 'encoder.layer.7.output.dense.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.5.attention.attention.value.bias', 'encoder.layer.10.attention.attention.key.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.7.layernorm_after.weight', 'embeddings.position_embeddings', 'encoder.layer.2.layernorm_after.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.11.attention.attention.query.weight', 'encoder.layer.5.attention.attention.query.bias', 'encoder.layer.5.layernorm_after.bias', 'encoder.layer.9.attention.attention.query.weight', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.2.layernorm_before.bias', 'encoder.layer.2.output.dense.bias', 'encoder.layer.5.attention.attention.key.bias', 'encoder.layer.8.output.dense.bias', 'encoder.layer.1.layernorm_after.weight', 'encoder.layer.5.attention.attention.value.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.10.attention.attention.key.weight', 'encoder.layer.2.attention.attention.value.bias', 'encoder.layer.11.output.dense.bias', 'encoder.layer.9.attention.attention.value.weight', 'encoder.layer.5.layernorm_before.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.2.attention.attention.query.bias', 'encoder.layer.9.attention.attention.key.weight', 'encoder.layer.6.layernorm_after.weight', 'encoder.layer.6.attention.attention.value.bias', 'encoder.layer.11.attention.attention.value.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.6.output.dense.weight', 'encoder.layer.0.layernorm_after.weight', 'encoder.layer.4.attention.attention.key.weight', 'classifier.weight', 'encoder.layer.4.attention.attention.query.bias', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.9.attention.attention.query.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.4.layernorm_after.weight', 'encoder.layer.0.attention.attention.query.bias', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.11.layernorm_after.bias', 'encoder.layer.9.layernorm_after.bias', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.0.output.dense.bias', 'encoder.layer.8.layernorm_after.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.3.attention.attention.value.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.6.layernorm_before.weight', 'encoder.layer.8.attention.attention.key.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.3.layernorm_after.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.4.attention.attention.value.bias', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.9.output.dense.bias', 'encoder.layer.1.attention.attention.query.weight', 'encoder.layer.2.attention.attention.key.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.6.attention.attention.query.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.9.layernorm_after.weight', 'encoder.layer.3.attention.attention.key.bias', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.0.attention.attention.query.weight', 'encoder.layer.1.attention.attention.value.weight', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.8.layernorm_before.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.6.layernorm_before.bias', 'encoder.layer.2.attention.attention.value.weight', 'encoder.layer.1.attention.attention.value.bias', 'encoder.layer.7.attention.attention.key.bias', 'encoder.layer.6.output.dense.bias', 'layernorm.weight', 'encoder.layer.3.layernorm_after.bias', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.3.attention.attention.query.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.11.attention.attention.key.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.2.attention.attention.query.weight', 'encoder.layer.0.layernorm_after.bias', 'encoder.layer.6.layernorm_after.bias', 'encoder.layer.7.output.dense.bias', 'encoder.layer.6.attention.attention.key.weight', 'encoder.layer.5.layernorm_after.weight', 'embeddings.cls_token', 'encoder.layer.7.layernorm_before.weight', 'encoder.layer.8.layernorm_before.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.0.attention.attention.value.bias', 'classifier.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.1.attention.attention.key.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.11.attention.attention.value.bias', 'encoder.layer.10.layernorm_after.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.2.layernorm_before.weight', 'encoder.layer.10.layernorm_before.bias', 'encoder.layer.5.attention.attention.key.weight', 'encoder.layer.3.attention.attention.value.weight', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.7.attention.attention.key.weight', 'encoder.layer.0.layernorm_before.weight', 'encoder.layer.4.attention.attention.query.weight', 'embeddings.patch_embeddings.projection.bias', 'encoder.layer.7.layernorm_before.bias', 'encoder.layer.7.layernorm_after.bias', 'encoder.layer.3.output.dense.bias', 'encoder.layer.7.attention.attention.value.bias', 'encoder.layer.6.attention.attention.value.weight', 'encoder.layer.2.attention.attention.key.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.10.layernorm_after.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.4.layernorm_after.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.4.attention.attention.value.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.2.layernorm_after.weight', 'encoder.layer.0.attention.attention.value.weight', 'encoder.layer.2.intermediate.dense.bias', 'layernorm.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.5.output.dense.weight', 'encoder.layer.9.attention.attention.value.bias', 'encoder.layer.8.attention.attention.value.weight', 'encoder.layer.8.layernorm_after.bias', 'encoder.layer.1.layernorm_after.bias', 'encoder.layer.10.layernorm_before.weight', 'encoder.layer.0.attention.attention.key.bias', 'encoder.layer.11.attention.attention.key.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.10.attention.attention.query.weight', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.10.attention.attention.query.bias', 'encoder.layer.3.attention.attention.key.weight', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.8.attention.attention.key.bias', 'encoder.layer.10.attention.attention.value.bias', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.4.layernorm_before.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.1.layernorm_before.weight', 'encoder.layer.6.attention.attention.key.bias', 'encoder.layer.1.layernorm_before.bias', 'encoder.layer.0.attention.attention.key.weight', 'encoder.layer.11.attention.attention.query.bias', 'encoder.layer.5.layernorm_before.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='770' max='770' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [770/770 05:56, Epoch 5/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>1.099900</td>\n",
              "      <td>1.062076</td>\n",
              "      <td>0.477977</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.937000</td>\n",
              "      <td>0.958416</td>\n",
              "      <td>0.492659</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>1.517200</td>\n",
              "      <td>0.978854</td>\n",
              "      <td>0.474715</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.887400</td>\n",
              "      <td>0.849945</td>\n",
              "      <td>0.636215</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>0.764900</td>\n",
              "      <td>0.683639</td>\n",
              "      <td>0.699837</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>0.893600</td>\n",
              "      <td>0.734032</td>\n",
              "      <td>0.655791</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>350</td>\n",
              "      <td>0.603400</td>\n",
              "      <td>0.633767</td>\n",
              "      <td>0.714519</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>0.730500</td>\n",
              "      <td>0.630683</td>\n",
              "      <td>0.722675</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>450</td>\n",
              "      <td>0.585400</td>\n",
              "      <td>0.671268</td>\n",
              "      <td>0.686786</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.549900</td>\n",
              "      <td>0.602359</td>\n",
              "      <td>0.743883</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>550</td>\n",
              "      <td>0.529600</td>\n",
              "      <td>0.587749</td>\n",
              "      <td>0.756933</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>0.511700</td>\n",
              "      <td>0.576059</td>\n",
              "      <td>0.768352</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>650</td>\n",
              "      <td>0.430300</td>\n",
              "      <td>0.577876</td>\n",
              "      <td>0.756933</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>0.526300</td>\n",
              "      <td>0.674868</td>\n",
              "      <td>0.732463</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>750</td>\n",
              "      <td>0.405500</td>\n",
              "      <td>0.560149</td>\n",
              "      <td>0.768352</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "***** train metrics *****\n",
            "  epoch                    =         5.0\n",
            "  total_flos               = 884451758GF\n",
            "  train_loss               =      0.7646\n",
            "  train_runtime            =  0:05:56.60\n",
            "  train_samples_per_second =      34.366\n",
            "  train_steps_per_second   =       2.159\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÖ‚ñÜ‚ñÖ‚ñá‚ñá‚ñÜ‚ñá‚ñà‚ñà‚ñà‚ñá‚ñà</td></tr><tr><td>eval/loss</td><td>‚ñà‚ñá‚ñá‚ñÖ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÉ‚ñÅ</td></tr><tr><td>eval/runtime</td><td>‚ñÜ‚ñÅ‚ñÜ‚ñÇ‚ñÖ‚ñÇ‚ñÇ‚ñÑ‚ñÜ‚ñÉ‚ñà‚ñÅ‚ñÅ‚ñÑ‚ñÖ</td></tr><tr><td>eval/samples_per_second</td><td>‚ñÉ‚ñà‚ñÉ‚ñá‚ñÑ‚ñá‚ñá‚ñÖ‚ñÉ‚ñÜ‚ñÅ‚ñà‚ñá‚ñÖ‚ñÑ</td></tr><tr><td>eval/steps_per_second</td><td>‚ñÉ‚ñà‚ñÉ‚ñá‚ñÑ‚ñá‚ñá‚ñÖ‚ñÉ‚ñÜ‚ñÅ‚ñà‚ñá‚ñÖ‚ñÑ</td></tr><tr><td>train/epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>train/global_step</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>train/learning_rate</td><td>‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>train/loss</td><td>‚ñà‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÇ</td></tr><tr><td>train/total_flos</td><td>‚ñÅ</td></tr><tr><td>train/train_loss</td><td>‚ñÅ</td></tr><tr><td>train/train_runtime</td><td>‚ñÅ</td></tr><tr><td>train/train_samples_per_second</td><td>‚ñÅ</td></tr><tr><td>train/train_steps_per_second</td><td>‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>0.76835</td></tr><tr><td>eval/loss</td><td>0.56015</td></tr><tr><td>eval/runtime</td><td>8.8785</td></tr><tr><td>eval/samples_per_second</td><td>69.043</td></tr><tr><td>eval/steps_per_second</td><td>8.673</td></tr><tr><td>train/epoch</td><td>5.0</td></tr><tr><td>train/global_step</td><td>770</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.5551</td></tr><tr><td>train/total_flos</td><td>9.496728442678579e+17</td></tr><tr><td>train/train_loss</td><td>0.76455</td></tr><tr><td>train/train_runtime</td><td>356.6038</td></tr><tr><td>train/train_samples_per_second</td><td>34.366</td></tr><tr><td>train/train_steps_per_second</td><td>2.159</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">lunar-shape-5</strong> at: <a href='https://wandb.ai/deep_bekeif/cnn_no_aug/runs/uv2ku8z2' target=\"_blank\">https://wandb.ai/deep_bekeif/cnn_no_aug/runs/uv2ku8z2</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20230702_111551-uv2ku8z2/logs</code>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "train_results = trainer.train()\n",
        "trainer.save_model()\n",
        "trainer.log_metrics(\"train\", train_results.metrics)\n",
        "trainer.save_metrics(\"train\", train_results.metrics)\n",
        "trainer.save_state()\n",
        "\n",
        "wandb.finish()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#display Confusion Matrix\n",
        "pred = trainer.predict(test_dataset)\n",
        "\n",
        "cm_display = met.ConfusionMatrixDisplay(confusion_matrix = compute_metrics(pred), display_labels = labels_names)\n",
        "\n",
        "cm_display.plot()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "id": "_B6YmSOoVm-B",
        "outputId": "ac0d9c12-65c4-4615-85ff-ae8ece9e7f7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## With augmentation"
      ],
      "metadata": {
        "id": "B0QUfYorUxfu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "CNN_base = 'microsoft/resnet-50'\n",
        "\n",
        "# Load the datasets from folders and process the dataset with the transform\n",
        "train_dataset = ImageFolder(data_path + '/brain_dataset/train', transform=train_transforms_with_aug)\n",
        "test_dataset = ImageFolder(data_path + '/brain_dataset/test', transform=test_transforms_without_aug)"
      ],
      "metadata": {
        "id": "kUtxy7CLVkQc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define the model\n",
        "def model_init():\n",
        "  return ViTForImageClassification.from_pretrained(\n",
        "    CNN_base,\n",
        "    num_labels=len(labels_names),\n",
        "    id2label={str(i): tumor_type for i, tumor_type in enumerate(labels_names)},\n",
        "    label2id={tumor_type: i for i, tumor_type in enumerate(labels_names)},\n",
        ")\n",
        "\n",
        "model = model_init()\n",
        "# Initialize a new run\n",
        "wandb.init(project=\"cnn_with_aug\")\n",
        "\n",
        "# Define the TrainingArguments\n",
        "training_args = TrainingArguments(\n",
        "  output_dir=output_cnn_with_aug,\n",
        "  per_device_train_batch_size=16,\n",
        "  evaluation_strategy=\"steps\",\n",
        "  num_train_epochs=5, #4\n",
        "  fp16=True,\n",
        "  save_steps=100,#100\n",
        "  eval_steps=50,#100\n",
        "  logging_steps=10,\n",
        "  learning_rate=2e-4,\n",
        "  save_total_limit=2,\n",
        "  remove_unused_columns=False,\n",
        "  push_to_hub=False,\n",
        "  report_to='wandb',\n",
        "  load_best_model_at_end=True,\n",
        "  seed = seed\n",
        ")\n",
        "\n",
        "\n",
        "trainer = Trainer(\n",
        "    model_init=model_init,\n",
        "    args=training_args,\n",
        "    compute_metrics=compute_metrics,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset,\n",
        "    data_collator= lambda samples: {'pixel_values': torch.stack([sample[0] for sample in samples]),\n",
        "                                   'labels': torch.tensor([sample[1] for sample in samples])},\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "id": "Ryzm8WNXVgTl",
        "outputId": "094809dd-4ac4-4b08-a7b9-3b8a66d3cdfd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You are using a model of type resnet to instantiate a model of type vit. This is not supported for all configurations of models and can yield errors.\n",
            "Some weights of the model checkpoint at microsoft/resnet-50 were not used when initializing ViTForImageClassification: ['resnet.embedder.embedder.normalization.running_var', 'resnet.encoder.stages.3.layers.2.layer.1.normalization.running_mean', 'resnet.encoder.stages.1.layers.0.layer.0.normalization.num_batches_tracked', 'resnet.encoder.stages.2.layers.3.layer.0.normalization.running_mean', 'resnet.encoder.stages.1.layers.3.layer.2.normalization.weight', 'resnet.encoder.stages.1.layers.3.layer.1.convolution.weight', 'resnet.encoder.stages.2.layers.1.layer.2.normalization.num_batches_tracked', 'resnet.encoder.stages.2.layers.0.layer.1.normalization.running_var', 'resnet.encoder.stages.2.layers.2.layer.2.normalization.running_mean', 'resnet.encoder.stages.3.layers.2.layer.0.normalization.bias', 'resnet.encoder.stages.1.layers.1.layer.0.normalization.running_var', 'resnet.embedder.embedder.convolution.weight', 'resnet.embedder.embedder.normalization.weight', 'resnet.encoder.stages.3.layers.2.layer.0.normalization.weight', 'resnet.encoder.stages.2.layers.0.layer.1.normalization.bias', 'resnet.encoder.stages.2.layers.4.layer.0.normalization.running_mean', 'resnet.encoder.stages.0.layers.2.layer.0.normalization.running_mean', 'resnet.encoder.stages.0.layers.0.layer.2.normalization.running_mean', 'resnet.encoder.stages.1.layers.1.layer.1.normalization.running_var', 'resnet.encoder.stages.1.layers.0.layer.0.convolution.weight', 'resnet.encoder.stages.1.layers.3.layer.0.normalization.bias', 'resnet.encoder.stages.3.layers.2.layer.2.normalization.bias', 'resnet.encoder.stages.2.layers.4.layer.1.normalization.running_mean', 'resnet.encoder.stages.3.layers.0.layer.1.normalization.num_batches_tracked', 'resnet.encoder.stages.3.layers.2.layer.1.normalization.running_var', 'resnet.encoder.stages.0.layers.1.layer.1.normalization.weight', 'resnet.encoder.stages.3.layers.1.layer.1.normalization.bias', 'resnet.encoder.stages.2.layers.5.layer.0.convolution.weight', 'resnet.encoder.stages.0.layers.2.layer.1.normalization.running_mean', 'resnet.encoder.stages.0.layers.1.layer.2.normalization.running_mean', 'resnet.encoder.stages.0.layers.0.layer.0.normalization.num_batches_tracked', 'resnet.encoder.stages.1.layers.1.layer.1.normalization.bias', 'resnet.encoder.stages.3.layers.0.layer.0.normalization.bias', 'resnet.encoder.stages.2.layers.4.layer.2.convolution.weight', 'resnet.encoder.stages.0.layers.1.layer.2.normalization.bias', 'resnet.encoder.stages.2.layers.5.layer.2.normalization.num_batches_tracked', 'resnet.encoder.stages.0.layers.0.layer.1.normalization.num_batches_tracked', 'resnet.encoder.stages.3.layers.0.layer.2.normalization.num_batches_tracked', 'resnet.encoder.stages.2.layers.1.layer.0.normalization.num_batches_tracked', 'resnet.encoder.stages.3.layers.1.layer.1.normalization.running_mean', 'resnet.encoder.stages.1.layers.0.layer.1.normalization.bias', 'resnet.encoder.stages.2.layers.3.layer.1.convolution.weight', 'resnet.encoder.stages.2.layers.3.layer.0.normalization.running_var', 'resnet.encoder.stages.3.layers.0.shortcut.convolution.weight', 'resnet.encoder.stages.0.layers.0.layer.2.convolution.weight', 'resnet.encoder.stages.1.layers.3.layer.2.normalization.num_batches_tracked', 'resnet.encoder.stages.3.layers.0.layer.1.convolution.weight', 'resnet.encoder.stages.3.layers.0.layer.2.normalization.running_mean', 'resnet.encoder.stages.3.layers.0.layer.0.normalization.weight', 'resnet.encoder.stages.2.layers.5.layer.0.normalization.weight', 'resnet.encoder.stages.1.layers.1.layer.2.normalization.bias', 'resnet.encoder.stages.2.layers.0.layer.0.normalization.num_batches_tracked', 'resnet.embedder.embedder.normalization.num_batches_tracked', 'resnet.encoder.stages.1.layers.2.layer.1.normalization.weight', 'resnet.encoder.stages.1.layers.3.layer.1.normalization.weight', 'resnet.encoder.stages.2.layers.5.layer.2.normalization.running_mean', 'resnet.encoder.stages.1.layers.0.layer.1.normalization.running_var', 'resnet.encoder.stages.3.layers.2.layer.0.normalization.running_var', 'resnet.encoder.stages.0.layers.0.layer.1.normalization.running_mean', 'resnet.encoder.stages.2.layers.0.layer.2.convolution.weight', 'resnet.encoder.stages.1.layers.2.layer.1.normalization.running_mean', 'resnet.encoder.stages.3.layers.1.layer.1.normalization.num_batches_tracked', 'resnet.encoder.stages.1.layers.0.shortcut.normalization.running_var', 'resnet.encoder.stages.2.layers.2.layer.2.normalization.num_batches_tracked', 'resnet.encoder.stages.3.layers.2.layer.1.normalization.bias', 'resnet.encoder.stages.1.layers.3.layer.0.normalization.running_var', 'resnet.encoder.stages.2.layers.1.layer.1.normalization.num_batches_tracked', 'resnet.encoder.stages.1.layers.1.layer.0.convolution.weight', 'resnet.encoder.stages.3.layers.0.layer.0.normalization.running_var', 'resnet.encoder.stages.2.layers.0.layer.1.normalization.num_batches_tracked', 'resnet.encoder.stages.1.layers.2.layer.2.normalization.running_mean', 'resnet.encoder.stages.2.layers.4.layer.0.normalization.bias', 'resnet.encoder.stages.2.layers.3.layer.0.convolution.weight', 'resnet.encoder.stages.2.layers.4.layer.1.normalization.num_batches_tracked', 'resnet.encoder.stages.3.layers.1.layer.2.normalization.weight', 'resnet.encoder.stages.3.layers.0.layer.2.normalization.running_var', 'resnet.encoder.stages.3.layers.0.layer.0.normalization.running_mean', 'resnet.encoder.stages.1.layers.0.layer.0.normalization.weight', 'resnet.encoder.stages.0.layers.0.layer.1.normalization.running_var', 'resnet.encoder.stages.2.layers.4.layer.2.normalization.num_batches_tracked', 'resnet.encoder.stages.2.layers.0.layer.1.normalization.weight', 'resnet.encoder.stages.2.layers.3.layer.0.normalization.bias', 'resnet.encoder.stages.0.layers.0.shortcut.normalization.num_batches_tracked', 'resnet.encoder.stages.0.layers.0.shortcut.normalization.running_var', 'resnet.encoder.stages.3.layers.1.layer.2.normalization.running_var', 'resnet.encoder.stages.0.layers.2.layer.1.convolution.weight', 'resnet.encoder.stages.0.layers.0.layer.2.normalization.bias', 'resnet.encoder.stages.1.layers.1.layer.2.normalization.running_var', 'resnet.encoder.stages.2.layers.4.layer.1.normalization.weight', 'resnet.encoder.stages.3.layers.0.layer.0.convolution.weight', 'resnet.encoder.stages.1.layers.0.shortcut.normalization.weight', 'resnet.encoder.stages.2.layers.2.layer.1.normalization.weight', 'resnet.encoder.stages.3.layers.2.layer.1.convolution.weight', 'resnet.encoder.stages.3.layers.1.layer.0.normalization.num_batches_tracked', 'resnet.encoder.stages.0.layers.2.layer.2.normalization.weight', 'resnet.encoder.stages.2.layers.4.layer.0.convolution.weight', 'resnet.encoder.stages.3.layers.2.layer.0.normalization.num_batches_tracked', 'resnet.encoder.stages.2.layers.2.layer.1.normalization.running_var', 'resnet.encoder.stages.2.layers.4.layer.2.normalization.running_mean', 'resnet.encoder.stages.3.layers.2.layer.1.normalization.weight', 'resnet.encoder.stages.0.layers.1.layer.1.convolution.weight', 'resnet.encoder.stages.2.layers.2.layer.1.normalization.bias', 'resnet.encoder.stages.1.layers.1.layer.0.normalization.num_batches_tracked', 'resnet.encoder.stages.1.layers.2.layer.0.convolution.weight', 'resnet.encoder.stages.2.layers.3.layer.1.normalization.running_var', 'resnet.encoder.stages.1.layers.2.layer.2.convolution.weight', 'resnet.encoder.stages.1.layers.3.layer.1.normalization.num_batches_tracked', 'resnet.encoder.stages.1.layers.0.layer.2.normalization.weight', 'resnet.encoder.stages.1.layers.2.layer.0.normalization.bias', 'resnet.encoder.stages.0.layers.1.layer.0.normalization.running_var', 'resnet.encoder.stages.1.layers.0.layer.1.normalization.running_mean', 'resnet.encoder.stages.3.layers.2.layer.1.normalization.num_batches_tracked', 'resnet.encoder.stages.2.layers.5.layer.1.normalization.weight', 'resnet.encoder.stages.3.layers.2.layer.2.normalization.weight', 'resnet.encoder.stages.3.layers.1.layer.0.normalization.bias', 'resnet.encoder.stages.3.layers.1.layer.0.normalization.running_mean', 'resnet.encoder.stages.2.layers.1.layer.2.convolution.weight', 'resnet.encoder.stages.0.layers.2.layer.0.normalization.bias', 'resnet.encoder.stages.2.layers.4.layer.2.normalization.running_var', 'resnet.encoder.stages.1.layers.3.layer.0.convolution.weight', 'resnet.encoder.stages.2.layers.0.layer.1.normalization.running_mean', 'resnet.encoder.stages.2.layers.0.shortcut.normalization.running_mean', 'resnet.encoder.stages.0.layers.2.layer.0.normalization.running_var', 'resnet.encoder.stages.1.layers.2.layer.0.normalization.weight', 'resnet.encoder.stages.2.layers.3.layer.2.convolution.weight', 'resnet.encoder.stages.2.layers.4.layer.1.normalization.bias', 'resnet.encoder.stages.0.layers.0.layer.0.normalization.running_mean', 'resnet.encoder.stages.2.layers.1.layer.2.normalization.running_mean', 'resnet.encoder.stages.1.layers.0.layer.2.normalization.num_batches_tracked', 'resnet.encoder.stages.2.layers.0.shortcut.normalization.running_var', 'resnet.encoder.stages.2.layers.4.layer.2.normalization.weight', 'resnet.encoder.stages.2.layers.2.layer.1.normalization.num_batches_tracked', 'resnet.encoder.stages.1.layers.1.layer.1.convolution.weight', 'resnet.encoder.stages.0.layers.1.layer.2.convolution.weight', 'resnet.encoder.stages.1.layers.3.layer.0.normalization.num_batches_tracked', 'resnet.encoder.stages.2.layers.3.layer.0.normalization.weight', 'resnet.encoder.stages.0.layers.1.layer.2.normalization.num_batches_tracked', 'resnet.encoder.stages.2.layers.2.layer.0.normalization.weight', 'resnet.encoder.stages.2.layers.0.layer.0.normalization.running_var', 'resnet.encoder.stages.1.layers.2.layer.0.normalization.running_mean', 'resnet.encoder.stages.0.layers.1.layer.2.normalization.running_var', 'resnet.encoder.stages.0.layers.0.layer.0.normalization.bias', 'resnet.encoder.stages.3.layers.0.layer.0.normalization.num_batches_tracked', 'resnet.encoder.stages.0.layers.0.layer.2.normalization.weight', 'resnet.encoder.stages.3.layers.0.shortcut.normalization.bias', 'resnet.encoder.stages.2.layers.3.layer.2.normalization.running_mean', 'resnet.encoder.stages.1.layers.3.layer.1.normalization.running_var', 'resnet.encoder.stages.2.layers.3.layer.2.normalization.running_var', 'resnet.encoder.stages.1.layers.2.layer.0.normalization.num_batches_tracked', 'resnet.encoder.stages.3.layers.1.layer.2.normalization.running_mean', 'resnet.encoder.stages.2.layers.0.layer.0.normalization.bias', 'resnet.encoder.stages.2.layers.1.layer.0.normalization.running_var', 'resnet.encoder.stages.2.layers.3.layer.2.normalization.num_batches_tracked', 'resnet.encoder.stages.0.layers.1.layer.1.normalization.running_var', 'resnet.encoder.stages.2.layers.0.shortcut.normalization.num_batches_tracked', 'resnet.encoder.stages.0.layers.2.layer.1.normalization.running_var', 'resnet.encoder.stages.3.layers.0.layer.1.normalization.bias', 'resnet.encoder.stages.0.layers.0.layer.2.normalization.running_var', 'resnet.encoder.stages.2.layers.1.layer.2.normalization.running_var', 'resnet.encoder.stages.1.layers.3.layer.0.normalization.running_mean', 'resnet.encoder.stages.3.layers.0.shortcut.normalization.weight', 'resnet.encoder.stages.1.layers.2.layer.1.normalization.bias', 'resnet.encoder.stages.3.layers.0.layer.1.normalization.running_mean', 'resnet.encoder.stages.2.layers.3.layer.1.normalization.bias', 'resnet.encoder.stages.3.layers.0.layer.2.normalization.weight', 'resnet.encoder.stages.2.layers.4.layer.2.normalization.bias', 'resnet.encoder.stages.2.layers.0.layer.0.normalization.weight', 'resnet.encoder.stages.0.layers.1.layer.0.convolution.weight', 'resnet.encoder.stages.3.layers.1.layer.1.convolution.weight', 'resnet.encoder.stages.3.layers.1.layer.2.normalization.bias', 'resnet.encoder.stages.0.layers.1.layer.0.normalization.weight', 'resnet.encoder.stages.2.layers.2.layer.2.convolution.weight', 'resnet.encoder.stages.2.layers.5.layer.2.normalization.bias', 'resnet.encoder.stages.0.layers.2.layer.0.convolution.weight', 'resnet.encoder.stages.2.layers.5.layer.2.normalization.weight', 'resnet.encoder.stages.2.layers.4.layer.0.normalization.num_batches_tracked', 'resnet.encoder.stages.1.layers.2.layer.0.normalization.running_var', 'resnet.encoder.stages.3.layers.2.layer.2.normalization.running_var', 'resnet.encoder.stages.2.layers.5.layer.1.normalization.num_batches_tracked', 'resnet.encoder.stages.2.layers.5.layer.2.convolution.weight', 'resnet.encoder.stages.2.layers.1.layer.1.normalization.running_var', 'resnet.encoder.stages.1.layers.1.layer.0.normalization.running_mean', 'resnet.encoder.stages.2.layers.2.layer.1.convolution.weight', 'resnet.encoder.stages.2.layers.0.shortcut.normalization.weight', 'resnet.encoder.stages.2.layers.0.layer.2.normalization.running_mean', 'resnet.encoder.stages.0.layers.0.shortcut.normalization.weight', 'resnet.encoder.stages.0.layers.2.layer.1.normalization.bias', 'resnet.encoder.stages.3.layers.0.shortcut.normalization.num_batches_tracked', 'resnet.encoder.stages.3.layers.1.layer.2.convolution.weight', 'resnet.encoder.stages.1.layers.0.layer.1.convolution.weight', 'resnet.encoder.stages.3.layers.0.layer.1.normalization.running_var', 'resnet.encoder.stages.3.layers.1.layer.1.normalization.running_var', 'resnet.encoder.stages.3.layers.2.layer.2.convolution.weight', 'resnet.encoder.stages.2.layers.0.layer.2.normalization.bias', 'resnet.encoder.stages.1.layers.2.layer.1.normalization.num_batches_tracked', 'resnet.encoder.stages.1.layers.0.shortcut.normalization.num_batches_tracked', 'resnet.encoder.stages.1.layers.2.layer.2.normalization.weight', 'resnet.encoder.stages.2.layers.1.layer.1.normalization.running_mean', 'resnet.encoder.stages.1.layers.1.layer.0.normalization.weight', 'resnet.encoder.stages.3.layers.1.layer.1.normalization.weight', 'resnet.encoder.stages.2.layers.4.layer.1.normalization.running_var', 'resnet.encoder.stages.2.layers.5.layer.0.normalization.running_var', 'resnet.encoder.stages.2.layers.5.layer.1.normalization.bias', 'resnet.encoder.stages.2.layers.1.layer.1.convolution.weight', 'resnet.encoder.stages.2.layers.2.layer.2.normalization.bias', 'resnet.encoder.stages.1.layers.0.layer.2.normalization.running_var', 'resnet.encoder.stages.1.layers.0.layer.0.normalization.running_mean', 'resnet.encoder.stages.3.layers.2.layer.0.convolution.weight', 'resnet.encoder.stages.1.layers.0.layer.1.normalization.num_batches_tracked', 'resnet.encoder.stages.1.layers.2.layer.1.convolution.weight', 'resnet.encoder.stages.0.layers.0.layer.2.normalization.num_batches_tracked', 'resnet.encoder.stages.2.layers.1.layer.2.normalization.bias', 'resnet.encoder.stages.2.layers.5.layer.2.normalization.running_var', 'resnet.encoder.stages.1.layers.0.layer.2.convolution.weight', 'resnet.encoder.stages.3.layers.0.layer.2.convolution.weight', 'resnet.encoder.stages.2.layers.1.layer.0.normalization.running_mean', 'resnet.encoder.stages.1.layers.3.layer.2.normalization.bias', 'resnet.encoder.stages.2.layers.5.layer.0.normalization.running_mean', 'resnet.encoder.stages.1.layers.3.layer.2.convolution.weight', 'classifier.1.bias', 'resnet.encoder.stages.1.layers.2.layer.2.normalization.num_batches_tracked', 'resnet.encoder.stages.1.layers.0.layer.0.normalization.running_var', 'resnet.encoder.stages.2.layers.5.layer.1.convolution.weight', 'resnet.encoder.stages.0.layers.2.layer.2.normalization.running_var', 'resnet.encoder.stages.1.layers.0.layer.0.normalization.bias', 'resnet.encoder.stages.2.layers.0.layer.2.normalization.num_batches_tracked', 'resnet.encoder.stages.2.layers.5.layer.0.normalization.bias', 'resnet.encoder.stages.0.layers.2.layer.2.normalization.bias', 'resnet.encoder.stages.2.layers.2.layer.0.normalization.num_batches_tracked', 'resnet.encoder.stages.0.layers.1.layer.0.normalization.running_mean', 'resnet.encoder.stages.0.layers.2.layer.0.normalization.num_batches_tracked', 'classifier.1.weight', 'resnet.encoder.stages.0.layers.1.layer.1.normalization.bias', 'resnet.encoder.stages.0.layers.1.layer.2.normalization.weight', 'resnet.encoder.stages.0.layers.0.layer.1.normalization.bias', 'resnet.encoder.stages.0.layers.0.layer.0.normalization.running_var', 'resnet.encoder.stages.2.layers.2.layer.0.normalization.bias', 'resnet.encoder.stages.3.layers.2.layer.2.normalization.num_batches_tracked', 'resnet.encoder.stages.2.layers.4.layer.1.convolution.weight', 'resnet.encoder.stages.2.layers.3.layer.1.normalization.running_mean', 'resnet.encoder.stages.0.layers.2.layer.1.normalization.weight', 'resnet.encoder.stages.1.layers.2.layer.2.normalization.running_var', 'resnet.encoder.stages.0.layers.0.shortcut.normalization.bias', 'resnet.encoder.stages.1.layers.0.shortcut.normalization.running_mean', 'resnet.encoder.stages.1.layers.1.layer.1.normalization.running_mean', 'resnet.encoder.stages.0.layers.1.layer.0.normalization.num_batches_tracked', 'resnet.embedder.embedder.normalization.running_mean', 'resnet.encoder.stages.2.layers.2.layer.0.convolution.weight', 'resnet.encoder.stages.0.layers.0.layer.0.convolution.weight', 'resnet.encoder.stages.2.layers.3.layer.2.normalization.bias', 'resnet.encoder.stages.2.layers.5.layer.0.normalization.num_batches_tracked', 'resnet.encoder.stages.3.layers.1.layer.0.normalization.weight', 'resnet.encoder.stages.2.layers.0.shortcut.normalization.bias', 'resnet.encoder.stages.2.layers.3.layer.2.normalization.weight', 'resnet.encoder.stages.0.layers.0.shortcut.normalization.running_mean', 'resnet.encoder.stages.3.layers.0.layer.1.normalization.weight', 'resnet.encoder.stages.0.layers.2.layer.0.normalization.weight', 'resnet.encoder.stages.2.layers.4.layer.0.normalization.weight', 'resnet.encoder.stages.2.layers.2.layer.0.normalization.running_var', 'resnet.encoder.stages.0.layers.2.layer.2.convolution.weight', 'resnet.encoder.stages.2.layers.1.layer.2.normalization.weight', 'resnet.encoder.stages.1.layers.0.layer.2.normalization.bias', 'resnet.encoder.stages.1.layers.1.layer.0.normalization.bias', 'resnet.encoder.stages.2.layers.1.layer.1.normalization.weight', 'resnet.encoder.stages.2.layers.3.layer.0.normalization.num_batches_tracked', 'resnet.encoder.stages.3.layers.1.layer.0.normalization.running_var', 'resnet.encoder.stages.2.layers.1.layer.0.normalization.bias', 'resnet.encoder.stages.1.layers.2.layer.1.normalization.running_var', 'resnet.encoder.stages.0.layers.1.layer.0.normalization.bias', 'resnet.encoder.stages.0.layers.0.layer.1.convolution.weight', 'resnet.encoder.stages.3.layers.1.layer.2.normalization.num_batches_tracked', 'resnet.encoder.stages.0.layers.0.layer.0.normalization.weight', 'resnet.encoder.stages.1.layers.1.layer.1.normalization.num_batches_tracked', 'resnet.encoder.stages.1.layers.1.layer.2.normalization.running_mean', 'resnet.encoder.stages.2.layers.2.layer.1.normalization.running_mean', 'resnet.encoder.stages.2.layers.5.layer.1.normalization.running_var', 'resnet.encoder.stages.1.layers.2.layer.2.normalization.bias', 'resnet.encoder.stages.1.layers.0.layer.1.normalization.weight', 'resnet.encoder.stages.1.layers.1.layer.2.convolution.weight', 'resnet.encoder.stages.2.layers.1.layer.0.convolution.weight', 'resnet.encoder.stages.3.layers.1.layer.0.convolution.weight', 'resnet.encoder.stages.2.layers.5.layer.1.normalization.running_mean', 'resnet.encoder.stages.3.layers.0.shortcut.normalization.running_var', 'resnet.encoder.stages.0.layers.2.layer.2.normalization.num_batches_tracked', 'resnet.encoder.stages.1.layers.1.layer.1.normalization.weight', 'resnet.encoder.stages.0.layers.1.layer.1.normalization.num_batches_tracked', 'resnet.encoder.stages.3.layers.0.layer.2.normalization.bias', 'resnet.encoder.stages.2.layers.0.layer.0.normalization.running_mean', 'resnet.encoder.stages.0.layers.2.layer.1.normalization.num_batches_tracked', 'resnet.encoder.stages.2.layers.3.layer.1.normalization.weight', 'resnet.encoder.stages.1.layers.3.layer.1.normalization.running_mean', 'resnet.encoder.stages.1.layers.0.layer.2.normalization.running_mean', 'resnet.encoder.stages.2.layers.0.layer.2.normalization.weight', 'resnet.encoder.stages.2.layers.0.layer.0.convolution.weight', 'resnet.encoder.stages.1.layers.0.shortcut.normalization.bias', 'resnet.encoder.stages.2.layers.2.layer.2.normalization.weight', 'resnet.encoder.stages.0.layers.2.layer.2.normalization.running_mean', 'resnet.encoder.stages.1.layers.1.layer.2.normalization.num_batches_tracked', 'resnet.encoder.stages.2.layers.0.layer.1.convolution.weight', 'resnet.encoder.stages.1.layers.3.layer.2.normalization.running_mean', 'resnet.encoder.stages.3.layers.2.layer.2.normalization.running_mean', 'resnet.encoder.stages.2.layers.4.layer.0.normalization.running_var', 'resnet.encoder.stages.1.layers.1.layer.2.normalization.weight', 'resnet.embedder.embedder.normalization.bias', 'resnet.encoder.stages.1.layers.3.layer.0.normalization.weight', 'resnet.encoder.stages.3.layers.0.shortcut.normalization.running_mean', 'resnet.encoder.stages.1.layers.3.layer.1.normalization.bias', 'resnet.encoder.stages.0.layers.0.shortcut.convolution.weight', 'resnet.encoder.stages.0.layers.1.layer.1.normalization.running_mean', 'resnet.encoder.stages.2.layers.2.layer.2.normalization.running_var', 'resnet.encoder.stages.3.layers.2.layer.0.normalization.running_mean', 'resnet.encoder.stages.2.layers.1.layer.1.normalization.bias', 'resnet.encoder.stages.0.layers.0.layer.1.normalization.weight', 'resnet.encoder.stages.2.layers.0.layer.2.normalization.running_var', 'resnet.encoder.stages.2.layers.0.shortcut.convolution.weight', 'resnet.encoder.stages.1.layers.0.shortcut.convolution.weight', 'resnet.encoder.stages.2.layers.3.layer.1.normalization.num_batches_tracked', 'resnet.encoder.stages.2.layers.1.layer.0.normalization.weight', 'resnet.encoder.stages.1.layers.3.layer.2.normalization.running_var', 'resnet.encoder.stages.2.layers.2.layer.0.normalization.running_mean']\n",
            "- This IS expected if you are initializing ViTForImageClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing ViTForImageClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of ViTForImageClassification were not initialized from the model checkpoint at microsoft/resnet-50 and are newly initialized: ['encoder.layer.10.attention.output.dense.bias', 'encoder.layer.11.layernorm_after.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.3.layernorm_before.bias', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.4.output.dense.bias', 'encoder.layer.7.attention.attention.value.weight', 'encoder.layer.11.layernorm_before.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.8.attention.attention.query.bias', 'encoder.layer.1.attention.attention.key.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.8.attention.attention.value.bias', 'encoder.layer.11.layernorm_before.weight', 'encoder.layer.7.attention.attention.query.bias', 'encoder.layer.4.attention.attention.key.bias', 'encoder.layer.1.attention.attention.query.bias', 'encoder.layer.9.layernorm_before.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.10.attention.attention.value.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.9.attention.attention.key.bias', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.9.layernorm_before.bias', 'encoder.layer.8.attention.attention.query.weight', 'encoder.layer.9.output.dense.weight', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.5.attention.attention.query.weight', 'encoder.layer.7.attention.attention.query.weight', 'encoder.layer.4.layernorm_before.weight', 'embeddings.patch_embeddings.projection.weight', 'encoder.layer.0.layernorm_before.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.3.layernorm_before.weight', 'encoder.layer.6.attention.attention.query.weight', 'encoder.layer.3.attention.attention.query.weight', 'encoder.layer.7.output.dense.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.5.attention.attention.value.bias', 'encoder.layer.10.attention.attention.key.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.7.layernorm_after.weight', 'embeddings.position_embeddings', 'encoder.layer.2.layernorm_after.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.11.attention.attention.query.weight', 'encoder.layer.5.attention.attention.query.bias', 'encoder.layer.5.layernorm_after.bias', 'encoder.layer.9.attention.attention.query.weight', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.2.layernorm_before.bias', 'encoder.layer.2.output.dense.bias', 'encoder.layer.5.attention.attention.key.bias', 'encoder.layer.8.output.dense.bias', 'encoder.layer.1.layernorm_after.weight', 'encoder.layer.5.attention.attention.value.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.10.attention.attention.key.weight', 'encoder.layer.2.attention.attention.value.bias', 'encoder.layer.11.output.dense.bias', 'encoder.layer.9.attention.attention.value.weight', 'encoder.layer.5.layernorm_before.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.2.attention.attention.query.bias', 'encoder.layer.9.attention.attention.key.weight', 'encoder.layer.6.layernorm_after.weight', 'encoder.layer.6.attention.attention.value.bias', 'encoder.layer.11.attention.attention.value.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.6.output.dense.weight', 'encoder.layer.0.layernorm_after.weight', 'encoder.layer.4.attention.attention.key.weight', 'classifier.weight', 'encoder.layer.4.attention.attention.query.bias', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.9.attention.attention.query.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.4.layernorm_after.weight', 'encoder.layer.0.attention.attention.query.bias', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.11.layernorm_after.bias', 'encoder.layer.9.layernorm_after.bias', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.0.output.dense.bias', 'encoder.layer.8.layernorm_after.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.3.attention.attention.value.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.6.layernorm_before.weight', 'encoder.layer.8.attention.attention.key.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.3.layernorm_after.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.4.attention.attention.value.bias', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.9.output.dense.bias', 'encoder.layer.1.attention.attention.query.weight', 'encoder.layer.2.attention.attention.key.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.6.attention.attention.query.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.9.layernorm_after.weight', 'encoder.layer.3.attention.attention.key.bias', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.0.attention.attention.query.weight', 'encoder.layer.1.attention.attention.value.weight', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.8.layernorm_before.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.6.layernorm_before.bias', 'encoder.layer.2.attention.attention.value.weight', 'encoder.layer.1.attention.attention.value.bias', 'encoder.layer.7.attention.attention.key.bias', 'encoder.layer.6.output.dense.bias', 'layernorm.weight', 'encoder.layer.3.layernorm_after.bias', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.3.attention.attention.query.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.11.attention.attention.key.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.2.attention.attention.query.weight', 'encoder.layer.0.layernorm_after.bias', 'encoder.layer.6.layernorm_after.bias', 'encoder.layer.7.output.dense.bias', 'encoder.layer.6.attention.attention.key.weight', 'encoder.layer.5.layernorm_after.weight', 'embeddings.cls_token', 'encoder.layer.7.layernorm_before.weight', 'encoder.layer.8.layernorm_before.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.0.attention.attention.value.bias', 'classifier.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.1.attention.attention.key.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.11.attention.attention.value.bias', 'encoder.layer.10.layernorm_after.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.2.layernorm_before.weight', 'encoder.layer.10.layernorm_before.bias', 'encoder.layer.5.attention.attention.key.weight', 'encoder.layer.3.attention.attention.value.weight', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.7.attention.attention.key.weight', 'encoder.layer.0.layernorm_before.weight', 'encoder.layer.4.attention.attention.query.weight', 'embeddings.patch_embeddings.projection.bias', 'encoder.layer.7.layernorm_before.bias', 'encoder.layer.7.layernorm_after.bias', 'encoder.layer.3.output.dense.bias', 'encoder.layer.7.attention.attention.value.bias', 'encoder.layer.6.attention.attention.value.weight', 'encoder.layer.2.attention.attention.key.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.10.layernorm_after.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.4.layernorm_after.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.4.attention.attention.value.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.2.layernorm_after.weight', 'encoder.layer.0.attention.attention.value.weight', 'encoder.layer.2.intermediate.dense.bias', 'layernorm.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.5.output.dense.weight', 'encoder.layer.9.attention.attention.value.bias', 'encoder.layer.8.attention.attention.value.weight', 'encoder.layer.8.layernorm_after.bias', 'encoder.layer.1.layernorm_after.bias', 'encoder.layer.10.layernorm_before.weight', 'encoder.layer.0.attention.attention.key.bias', 'encoder.layer.11.attention.attention.key.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.10.attention.attention.query.weight', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.10.attention.attention.query.bias', 'encoder.layer.3.attention.attention.key.weight', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.8.attention.attention.key.bias', 'encoder.layer.10.attention.attention.value.bias', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.4.layernorm_before.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.1.layernorm_before.weight', 'encoder.layer.6.attention.attention.key.bias', 'encoder.layer.1.layernorm_before.bias', 'encoder.layer.0.attention.attention.key.weight', 'encoder.layer.11.attention.attention.query.bias', 'encoder.layer.5.layernorm_before.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.15.4"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20230702_112647-9zzbef2a</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/deep_bekeif/cnn_with_aug/runs/9zzbef2a' target=\"_blank\">feasible-sun-2</a></strong> to <a href='https://wandb.ai/deep_bekeif/cnn_with_aug' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/deep_bekeif/cnn_with_aug' target=\"_blank\">https://wandb.ai/deep_bekeif/cnn_with_aug</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/deep_bekeif/cnn_with_aug/runs/9zzbef2a' target=\"_blank\">https://wandb.ai/deep_bekeif/cnn_with_aug/runs/9zzbef2a</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You are using a model of type resnet to instantiate a model of type vit. This is not supported for all configurations of models and can yield errors.\n",
            "Some weights of the model checkpoint at microsoft/resnet-50 were not used when initializing ViTForImageClassification: ['resnet.embedder.embedder.normalization.running_var', 'resnet.encoder.stages.3.layers.2.layer.1.normalization.running_mean', 'resnet.encoder.stages.1.layers.0.layer.0.normalization.num_batches_tracked', 'resnet.encoder.stages.2.layers.3.layer.0.normalization.running_mean', 'resnet.encoder.stages.1.layers.3.layer.2.normalization.weight', 'resnet.encoder.stages.1.layers.3.layer.1.convolution.weight', 'resnet.encoder.stages.2.layers.1.layer.2.normalization.num_batches_tracked', 'resnet.encoder.stages.2.layers.0.layer.1.normalization.running_var', 'resnet.encoder.stages.2.layers.2.layer.2.normalization.running_mean', 'resnet.encoder.stages.3.layers.2.layer.0.normalization.bias', 'resnet.encoder.stages.1.layers.1.layer.0.normalization.running_var', 'resnet.embedder.embedder.convolution.weight', 'resnet.embedder.embedder.normalization.weight', 'resnet.encoder.stages.3.layers.2.layer.0.normalization.weight', 'resnet.encoder.stages.2.layers.0.layer.1.normalization.bias', 'resnet.encoder.stages.2.layers.4.layer.0.normalization.running_mean', 'resnet.encoder.stages.0.layers.2.layer.0.normalization.running_mean', 'resnet.encoder.stages.0.layers.0.layer.2.normalization.running_mean', 'resnet.encoder.stages.1.layers.1.layer.1.normalization.running_var', 'resnet.encoder.stages.1.layers.0.layer.0.convolution.weight', 'resnet.encoder.stages.1.layers.3.layer.0.normalization.bias', 'resnet.encoder.stages.3.layers.2.layer.2.normalization.bias', 'resnet.encoder.stages.2.layers.4.layer.1.normalization.running_mean', 'resnet.encoder.stages.3.layers.0.layer.1.normalization.num_batches_tracked', 'resnet.encoder.stages.3.layers.2.layer.1.normalization.running_var', 'resnet.encoder.stages.0.layers.1.layer.1.normalization.weight', 'resnet.encoder.stages.3.layers.1.layer.1.normalization.bias', 'resnet.encoder.stages.2.layers.5.layer.0.convolution.weight', 'resnet.encoder.stages.0.layers.2.layer.1.normalization.running_mean', 'resnet.encoder.stages.0.layers.1.layer.2.normalization.running_mean', 'resnet.encoder.stages.0.layers.0.layer.0.normalization.num_batches_tracked', 'resnet.encoder.stages.1.layers.1.layer.1.normalization.bias', 'resnet.encoder.stages.3.layers.0.layer.0.normalization.bias', 'resnet.encoder.stages.2.layers.4.layer.2.convolution.weight', 'resnet.encoder.stages.0.layers.1.layer.2.normalization.bias', 'resnet.encoder.stages.2.layers.5.layer.2.normalization.num_batches_tracked', 'resnet.encoder.stages.0.layers.0.layer.1.normalization.num_batches_tracked', 'resnet.encoder.stages.3.layers.0.layer.2.normalization.num_batches_tracked', 'resnet.encoder.stages.2.layers.1.layer.0.normalization.num_batches_tracked', 'resnet.encoder.stages.3.layers.1.layer.1.normalization.running_mean', 'resnet.encoder.stages.1.layers.0.layer.1.normalization.bias', 'resnet.encoder.stages.2.layers.3.layer.1.convolution.weight', 'resnet.encoder.stages.2.layers.3.layer.0.normalization.running_var', 'resnet.encoder.stages.3.layers.0.shortcut.convolution.weight', 'resnet.encoder.stages.0.layers.0.layer.2.convolution.weight', 'resnet.encoder.stages.1.layers.3.layer.2.normalization.num_batches_tracked', 'resnet.encoder.stages.3.layers.0.layer.1.convolution.weight', 'resnet.encoder.stages.3.layers.0.layer.2.normalization.running_mean', 'resnet.encoder.stages.3.layers.0.layer.0.normalization.weight', 'resnet.encoder.stages.2.layers.5.layer.0.normalization.weight', 'resnet.encoder.stages.1.layers.1.layer.2.normalization.bias', 'resnet.encoder.stages.2.layers.0.layer.0.normalization.num_batches_tracked', 'resnet.embedder.embedder.normalization.num_batches_tracked', 'resnet.encoder.stages.1.layers.2.layer.1.normalization.weight', 'resnet.encoder.stages.1.layers.3.layer.1.normalization.weight', 'resnet.encoder.stages.2.layers.5.layer.2.normalization.running_mean', 'resnet.encoder.stages.1.layers.0.layer.1.normalization.running_var', 'resnet.encoder.stages.3.layers.2.layer.0.normalization.running_var', 'resnet.encoder.stages.0.layers.0.layer.1.normalization.running_mean', 'resnet.encoder.stages.2.layers.0.layer.2.convolution.weight', 'resnet.encoder.stages.1.layers.2.layer.1.normalization.running_mean', 'resnet.encoder.stages.3.layers.1.layer.1.normalization.num_batches_tracked', 'resnet.encoder.stages.1.layers.0.shortcut.normalization.running_var', 'resnet.encoder.stages.2.layers.2.layer.2.normalization.num_batches_tracked', 'resnet.encoder.stages.3.layers.2.layer.1.normalization.bias', 'resnet.encoder.stages.1.layers.3.layer.0.normalization.running_var', 'resnet.encoder.stages.2.layers.1.layer.1.normalization.num_batches_tracked', 'resnet.encoder.stages.1.layers.1.layer.0.convolution.weight', 'resnet.encoder.stages.3.layers.0.layer.0.normalization.running_var', 'resnet.encoder.stages.2.layers.0.layer.1.normalization.num_batches_tracked', 'resnet.encoder.stages.1.layers.2.layer.2.normalization.running_mean', 'resnet.encoder.stages.2.layers.4.layer.0.normalization.bias', 'resnet.encoder.stages.2.layers.3.layer.0.convolution.weight', 'resnet.encoder.stages.2.layers.4.layer.1.normalization.num_batches_tracked', 'resnet.encoder.stages.3.layers.1.layer.2.normalization.weight', 'resnet.encoder.stages.3.layers.0.layer.2.normalization.running_var', 'resnet.encoder.stages.3.layers.0.layer.0.normalization.running_mean', 'resnet.encoder.stages.1.layers.0.layer.0.normalization.weight', 'resnet.encoder.stages.0.layers.0.layer.1.normalization.running_var', 'resnet.encoder.stages.2.layers.4.layer.2.normalization.num_batches_tracked', 'resnet.encoder.stages.2.layers.0.layer.1.normalization.weight', 'resnet.encoder.stages.2.layers.3.layer.0.normalization.bias', 'resnet.encoder.stages.0.layers.0.shortcut.normalization.num_batches_tracked', 'resnet.encoder.stages.0.layers.0.shortcut.normalization.running_var', 'resnet.encoder.stages.3.layers.1.layer.2.normalization.running_var', 'resnet.encoder.stages.0.layers.2.layer.1.convolution.weight', 'resnet.encoder.stages.0.layers.0.layer.2.normalization.bias', 'resnet.encoder.stages.1.layers.1.layer.2.normalization.running_var', 'resnet.encoder.stages.2.layers.4.layer.1.normalization.weight', 'resnet.encoder.stages.3.layers.0.layer.0.convolution.weight', 'resnet.encoder.stages.1.layers.0.shortcut.normalization.weight', 'resnet.encoder.stages.2.layers.2.layer.1.normalization.weight', 'resnet.encoder.stages.3.layers.2.layer.1.convolution.weight', 'resnet.encoder.stages.3.layers.1.layer.0.normalization.num_batches_tracked', 'resnet.encoder.stages.0.layers.2.layer.2.normalization.weight', 'resnet.encoder.stages.2.layers.4.layer.0.convolution.weight', 'resnet.encoder.stages.3.layers.2.layer.0.normalization.num_batches_tracked', 'resnet.encoder.stages.2.layers.2.layer.1.normalization.running_var', 'resnet.encoder.stages.2.layers.4.layer.2.normalization.running_mean', 'resnet.encoder.stages.3.layers.2.layer.1.normalization.weight', 'resnet.encoder.stages.0.layers.1.layer.1.convolution.weight', 'resnet.encoder.stages.2.layers.2.layer.1.normalization.bias', 'resnet.encoder.stages.1.layers.1.layer.0.normalization.num_batches_tracked', 'resnet.encoder.stages.1.layers.2.layer.0.convolution.weight', 'resnet.encoder.stages.2.layers.3.layer.1.normalization.running_var', 'resnet.encoder.stages.1.layers.2.layer.2.convolution.weight', 'resnet.encoder.stages.1.layers.3.layer.1.normalization.num_batches_tracked', 'resnet.encoder.stages.1.layers.0.layer.2.normalization.weight', 'resnet.encoder.stages.1.layers.2.layer.0.normalization.bias', 'resnet.encoder.stages.0.layers.1.layer.0.normalization.running_var', 'resnet.encoder.stages.1.layers.0.layer.1.normalization.running_mean', 'resnet.encoder.stages.3.layers.2.layer.1.normalization.num_batches_tracked', 'resnet.encoder.stages.2.layers.5.layer.1.normalization.weight', 'resnet.encoder.stages.3.layers.2.layer.2.normalization.weight', 'resnet.encoder.stages.3.layers.1.layer.0.normalization.bias', 'resnet.encoder.stages.3.layers.1.layer.0.normalization.running_mean', 'resnet.encoder.stages.2.layers.1.layer.2.convolution.weight', 'resnet.encoder.stages.0.layers.2.layer.0.normalization.bias', 'resnet.encoder.stages.2.layers.4.layer.2.normalization.running_var', 'resnet.encoder.stages.1.layers.3.layer.0.convolution.weight', 'resnet.encoder.stages.2.layers.0.layer.1.normalization.running_mean', 'resnet.encoder.stages.2.layers.0.shortcut.normalization.running_mean', 'resnet.encoder.stages.0.layers.2.layer.0.normalization.running_var', 'resnet.encoder.stages.1.layers.2.layer.0.normalization.weight', 'resnet.encoder.stages.2.layers.3.layer.2.convolution.weight', 'resnet.encoder.stages.2.layers.4.layer.1.normalization.bias', 'resnet.encoder.stages.0.layers.0.layer.0.normalization.running_mean', 'resnet.encoder.stages.2.layers.1.layer.2.normalization.running_mean', 'resnet.encoder.stages.1.layers.0.layer.2.normalization.num_batches_tracked', 'resnet.encoder.stages.2.layers.0.shortcut.normalization.running_var', 'resnet.encoder.stages.2.layers.4.layer.2.normalization.weight', 'resnet.encoder.stages.2.layers.2.layer.1.normalization.num_batches_tracked', 'resnet.encoder.stages.1.layers.1.layer.1.convolution.weight', 'resnet.encoder.stages.0.layers.1.layer.2.convolution.weight', 'resnet.encoder.stages.1.layers.3.layer.0.normalization.num_batches_tracked', 'resnet.encoder.stages.2.layers.3.layer.0.normalization.weight', 'resnet.encoder.stages.0.layers.1.layer.2.normalization.num_batches_tracked', 'resnet.encoder.stages.2.layers.2.layer.0.normalization.weight', 'resnet.encoder.stages.2.layers.0.layer.0.normalization.running_var', 'resnet.encoder.stages.1.layers.2.layer.0.normalization.running_mean', 'resnet.encoder.stages.0.layers.1.layer.2.normalization.running_var', 'resnet.encoder.stages.0.layers.0.layer.0.normalization.bias', 'resnet.encoder.stages.3.layers.0.layer.0.normalization.num_batches_tracked', 'resnet.encoder.stages.0.layers.0.layer.2.normalization.weight', 'resnet.encoder.stages.3.layers.0.shortcut.normalization.bias', 'resnet.encoder.stages.2.layers.3.layer.2.normalization.running_mean', 'resnet.encoder.stages.1.layers.3.layer.1.normalization.running_var', 'resnet.encoder.stages.2.layers.3.layer.2.normalization.running_var', 'resnet.encoder.stages.1.layers.2.layer.0.normalization.num_batches_tracked', 'resnet.encoder.stages.3.layers.1.layer.2.normalization.running_mean', 'resnet.encoder.stages.2.layers.0.layer.0.normalization.bias', 'resnet.encoder.stages.2.layers.1.layer.0.normalization.running_var', 'resnet.encoder.stages.2.layers.3.layer.2.normalization.num_batches_tracked', 'resnet.encoder.stages.0.layers.1.layer.1.normalization.running_var', 'resnet.encoder.stages.2.layers.0.shortcut.normalization.num_batches_tracked', 'resnet.encoder.stages.0.layers.2.layer.1.normalization.running_var', 'resnet.encoder.stages.3.layers.0.layer.1.normalization.bias', 'resnet.encoder.stages.0.layers.0.layer.2.normalization.running_var', 'resnet.encoder.stages.2.layers.1.layer.2.normalization.running_var', 'resnet.encoder.stages.1.layers.3.layer.0.normalization.running_mean', 'resnet.encoder.stages.3.layers.0.shortcut.normalization.weight', 'resnet.encoder.stages.1.layers.2.layer.1.normalization.bias', 'resnet.encoder.stages.3.layers.0.layer.1.normalization.running_mean', 'resnet.encoder.stages.2.layers.3.layer.1.normalization.bias', 'resnet.encoder.stages.3.layers.0.layer.2.normalization.weight', 'resnet.encoder.stages.2.layers.4.layer.2.normalization.bias', 'resnet.encoder.stages.2.layers.0.layer.0.normalization.weight', 'resnet.encoder.stages.0.layers.1.layer.0.convolution.weight', 'resnet.encoder.stages.3.layers.1.layer.1.convolution.weight', 'resnet.encoder.stages.3.layers.1.layer.2.normalization.bias', 'resnet.encoder.stages.0.layers.1.layer.0.normalization.weight', 'resnet.encoder.stages.2.layers.2.layer.2.convolution.weight', 'resnet.encoder.stages.2.layers.5.layer.2.normalization.bias', 'resnet.encoder.stages.0.layers.2.layer.0.convolution.weight', 'resnet.encoder.stages.2.layers.5.layer.2.normalization.weight', 'resnet.encoder.stages.2.layers.4.layer.0.normalization.num_batches_tracked', 'resnet.encoder.stages.1.layers.2.layer.0.normalization.running_var', 'resnet.encoder.stages.3.layers.2.layer.2.normalization.running_var', 'resnet.encoder.stages.2.layers.5.layer.1.normalization.num_batches_tracked', 'resnet.encoder.stages.2.layers.5.layer.2.convolution.weight', 'resnet.encoder.stages.2.layers.1.layer.1.normalization.running_var', 'resnet.encoder.stages.1.layers.1.layer.0.normalization.running_mean', 'resnet.encoder.stages.2.layers.2.layer.1.convolution.weight', 'resnet.encoder.stages.2.layers.0.shortcut.normalization.weight', 'resnet.encoder.stages.2.layers.0.layer.2.normalization.running_mean', 'resnet.encoder.stages.0.layers.0.shortcut.normalization.weight', 'resnet.encoder.stages.0.layers.2.layer.1.normalization.bias', 'resnet.encoder.stages.3.layers.0.shortcut.normalization.num_batches_tracked', 'resnet.encoder.stages.3.layers.1.layer.2.convolution.weight', 'resnet.encoder.stages.1.layers.0.layer.1.convolution.weight', 'resnet.encoder.stages.3.layers.0.layer.1.normalization.running_var', 'resnet.encoder.stages.3.layers.1.layer.1.normalization.running_var', 'resnet.encoder.stages.3.layers.2.layer.2.convolution.weight', 'resnet.encoder.stages.2.layers.0.layer.2.normalization.bias', 'resnet.encoder.stages.1.layers.2.layer.1.normalization.num_batches_tracked', 'resnet.encoder.stages.1.layers.0.shortcut.normalization.num_batches_tracked', 'resnet.encoder.stages.1.layers.2.layer.2.normalization.weight', 'resnet.encoder.stages.2.layers.1.layer.1.normalization.running_mean', 'resnet.encoder.stages.1.layers.1.layer.0.normalization.weight', 'resnet.encoder.stages.3.layers.1.layer.1.normalization.weight', 'resnet.encoder.stages.2.layers.4.layer.1.normalization.running_var', 'resnet.encoder.stages.2.layers.5.layer.0.normalization.running_var', 'resnet.encoder.stages.2.layers.5.layer.1.normalization.bias', 'resnet.encoder.stages.2.layers.1.layer.1.convolution.weight', 'resnet.encoder.stages.2.layers.2.layer.2.normalization.bias', 'resnet.encoder.stages.1.layers.0.layer.2.normalization.running_var', 'resnet.encoder.stages.1.layers.0.layer.0.normalization.running_mean', 'resnet.encoder.stages.3.layers.2.layer.0.convolution.weight', 'resnet.encoder.stages.1.layers.0.layer.1.normalization.num_batches_tracked', 'resnet.encoder.stages.1.layers.2.layer.1.convolution.weight', 'resnet.encoder.stages.0.layers.0.layer.2.normalization.num_batches_tracked', 'resnet.encoder.stages.2.layers.1.layer.2.normalization.bias', 'resnet.encoder.stages.2.layers.5.layer.2.normalization.running_var', 'resnet.encoder.stages.1.layers.0.layer.2.convolution.weight', 'resnet.encoder.stages.3.layers.0.layer.2.convolution.weight', 'resnet.encoder.stages.2.layers.1.layer.0.normalization.running_mean', 'resnet.encoder.stages.1.layers.3.layer.2.normalization.bias', 'resnet.encoder.stages.2.layers.5.layer.0.normalization.running_mean', 'resnet.encoder.stages.1.layers.3.layer.2.convolution.weight', 'classifier.1.bias', 'resnet.encoder.stages.1.layers.2.layer.2.normalization.num_batches_tracked', 'resnet.encoder.stages.1.layers.0.layer.0.normalization.running_var', 'resnet.encoder.stages.2.layers.5.layer.1.convolution.weight', 'resnet.encoder.stages.0.layers.2.layer.2.normalization.running_var', 'resnet.encoder.stages.1.layers.0.layer.0.normalization.bias', 'resnet.encoder.stages.2.layers.0.layer.2.normalization.num_batches_tracked', 'resnet.encoder.stages.2.layers.5.layer.0.normalization.bias', 'resnet.encoder.stages.0.layers.2.layer.2.normalization.bias', 'resnet.encoder.stages.2.layers.2.layer.0.normalization.num_batches_tracked', 'resnet.encoder.stages.0.layers.1.layer.0.normalization.running_mean', 'resnet.encoder.stages.0.layers.2.layer.0.normalization.num_batches_tracked', 'classifier.1.weight', 'resnet.encoder.stages.0.layers.1.layer.1.normalization.bias', 'resnet.encoder.stages.0.layers.1.layer.2.normalization.weight', 'resnet.encoder.stages.0.layers.0.layer.1.normalization.bias', 'resnet.encoder.stages.0.layers.0.layer.0.normalization.running_var', 'resnet.encoder.stages.2.layers.2.layer.0.normalization.bias', 'resnet.encoder.stages.3.layers.2.layer.2.normalization.num_batches_tracked', 'resnet.encoder.stages.2.layers.4.layer.1.convolution.weight', 'resnet.encoder.stages.2.layers.3.layer.1.normalization.running_mean', 'resnet.encoder.stages.0.layers.2.layer.1.normalization.weight', 'resnet.encoder.stages.1.layers.2.layer.2.normalization.running_var', 'resnet.encoder.stages.0.layers.0.shortcut.normalization.bias', 'resnet.encoder.stages.1.layers.0.shortcut.normalization.running_mean', 'resnet.encoder.stages.1.layers.1.layer.1.normalization.running_mean', 'resnet.encoder.stages.0.layers.1.layer.0.normalization.num_batches_tracked', 'resnet.embedder.embedder.normalization.running_mean', 'resnet.encoder.stages.2.layers.2.layer.0.convolution.weight', 'resnet.encoder.stages.0.layers.0.layer.0.convolution.weight', 'resnet.encoder.stages.2.layers.3.layer.2.normalization.bias', 'resnet.encoder.stages.2.layers.5.layer.0.normalization.num_batches_tracked', 'resnet.encoder.stages.3.layers.1.layer.0.normalization.weight', 'resnet.encoder.stages.2.layers.0.shortcut.normalization.bias', 'resnet.encoder.stages.2.layers.3.layer.2.normalization.weight', 'resnet.encoder.stages.0.layers.0.shortcut.normalization.running_mean', 'resnet.encoder.stages.3.layers.0.layer.1.normalization.weight', 'resnet.encoder.stages.0.layers.2.layer.0.normalization.weight', 'resnet.encoder.stages.2.layers.4.layer.0.normalization.weight', 'resnet.encoder.stages.2.layers.2.layer.0.normalization.running_var', 'resnet.encoder.stages.0.layers.2.layer.2.convolution.weight', 'resnet.encoder.stages.2.layers.1.layer.2.normalization.weight', 'resnet.encoder.stages.1.layers.0.layer.2.normalization.bias', 'resnet.encoder.stages.1.layers.1.layer.0.normalization.bias', 'resnet.encoder.stages.2.layers.1.layer.1.normalization.weight', 'resnet.encoder.stages.2.layers.3.layer.0.normalization.num_batches_tracked', 'resnet.encoder.stages.3.layers.1.layer.0.normalization.running_var', 'resnet.encoder.stages.2.layers.1.layer.0.normalization.bias', 'resnet.encoder.stages.1.layers.2.layer.1.normalization.running_var', 'resnet.encoder.stages.0.layers.1.layer.0.normalization.bias', 'resnet.encoder.stages.0.layers.0.layer.1.convolution.weight', 'resnet.encoder.stages.3.layers.1.layer.2.normalization.num_batches_tracked', 'resnet.encoder.stages.0.layers.0.layer.0.normalization.weight', 'resnet.encoder.stages.1.layers.1.layer.1.normalization.num_batches_tracked', 'resnet.encoder.stages.1.layers.1.layer.2.normalization.running_mean', 'resnet.encoder.stages.2.layers.2.layer.1.normalization.running_mean', 'resnet.encoder.stages.2.layers.5.layer.1.normalization.running_var', 'resnet.encoder.stages.1.layers.2.layer.2.normalization.bias', 'resnet.encoder.stages.1.layers.0.layer.1.normalization.weight', 'resnet.encoder.stages.1.layers.1.layer.2.convolution.weight', 'resnet.encoder.stages.2.layers.1.layer.0.convolution.weight', 'resnet.encoder.stages.3.layers.1.layer.0.convolution.weight', 'resnet.encoder.stages.2.layers.5.layer.1.normalization.running_mean', 'resnet.encoder.stages.3.layers.0.shortcut.normalization.running_var', 'resnet.encoder.stages.0.layers.2.layer.2.normalization.num_batches_tracked', 'resnet.encoder.stages.1.layers.1.layer.1.normalization.weight', 'resnet.encoder.stages.0.layers.1.layer.1.normalization.num_batches_tracked', 'resnet.encoder.stages.3.layers.0.layer.2.normalization.bias', 'resnet.encoder.stages.2.layers.0.layer.0.normalization.running_mean', 'resnet.encoder.stages.0.layers.2.layer.1.normalization.num_batches_tracked', 'resnet.encoder.stages.2.layers.3.layer.1.normalization.weight', 'resnet.encoder.stages.1.layers.3.layer.1.normalization.running_mean', 'resnet.encoder.stages.1.layers.0.layer.2.normalization.running_mean', 'resnet.encoder.stages.2.layers.0.layer.2.normalization.weight', 'resnet.encoder.stages.2.layers.0.layer.0.convolution.weight', 'resnet.encoder.stages.1.layers.0.shortcut.normalization.bias', 'resnet.encoder.stages.2.layers.2.layer.2.normalization.weight', 'resnet.encoder.stages.0.layers.2.layer.2.normalization.running_mean', 'resnet.encoder.stages.1.layers.1.layer.2.normalization.num_batches_tracked', 'resnet.encoder.stages.2.layers.0.layer.1.convolution.weight', 'resnet.encoder.stages.1.layers.3.layer.2.normalization.running_mean', 'resnet.encoder.stages.3.layers.2.layer.2.normalization.running_mean', 'resnet.encoder.stages.2.layers.4.layer.0.normalization.running_var', 'resnet.encoder.stages.1.layers.1.layer.2.normalization.weight', 'resnet.embedder.embedder.normalization.bias', 'resnet.encoder.stages.1.layers.3.layer.0.normalization.weight', 'resnet.encoder.stages.3.layers.0.shortcut.normalization.running_mean', 'resnet.encoder.stages.1.layers.3.layer.1.normalization.bias', 'resnet.encoder.stages.0.layers.0.shortcut.convolution.weight', 'resnet.encoder.stages.0.layers.1.layer.1.normalization.running_mean', 'resnet.encoder.stages.2.layers.2.layer.2.normalization.running_var', 'resnet.encoder.stages.3.layers.2.layer.0.normalization.running_mean', 'resnet.encoder.stages.2.layers.1.layer.1.normalization.bias', 'resnet.encoder.stages.0.layers.0.layer.1.normalization.weight', 'resnet.encoder.stages.2.layers.0.layer.2.normalization.running_var', 'resnet.encoder.stages.2.layers.0.shortcut.convolution.weight', 'resnet.encoder.stages.1.layers.0.shortcut.convolution.weight', 'resnet.encoder.stages.2.layers.3.layer.1.normalization.num_batches_tracked', 'resnet.encoder.stages.2.layers.1.layer.0.normalization.weight', 'resnet.encoder.stages.1.layers.3.layer.2.normalization.running_var', 'resnet.encoder.stages.2.layers.2.layer.0.normalization.running_mean']\n",
            "- This IS expected if you are initializing ViTForImageClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing ViTForImageClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of ViTForImageClassification were not initialized from the model checkpoint at microsoft/resnet-50 and are newly initialized: ['encoder.layer.10.attention.output.dense.bias', 'encoder.layer.11.layernorm_after.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.3.layernorm_before.bias', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.4.output.dense.bias', 'encoder.layer.7.attention.attention.value.weight', 'encoder.layer.11.layernorm_before.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.8.attention.attention.query.bias', 'encoder.layer.1.attention.attention.key.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.8.attention.attention.value.bias', 'encoder.layer.11.layernorm_before.weight', 'encoder.layer.7.attention.attention.query.bias', 'encoder.layer.4.attention.attention.key.bias', 'encoder.layer.1.attention.attention.query.bias', 'encoder.layer.9.layernorm_before.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.10.attention.attention.value.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.9.attention.attention.key.bias', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.9.layernorm_before.bias', 'encoder.layer.8.attention.attention.query.weight', 'encoder.layer.9.output.dense.weight', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.5.attention.attention.query.weight', 'encoder.layer.7.attention.attention.query.weight', 'encoder.layer.4.layernorm_before.weight', 'embeddings.patch_embeddings.projection.weight', 'encoder.layer.0.layernorm_before.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.3.layernorm_before.weight', 'encoder.layer.6.attention.attention.query.weight', 'encoder.layer.3.attention.attention.query.weight', 'encoder.layer.7.output.dense.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.5.attention.attention.value.bias', 'encoder.layer.10.attention.attention.key.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.7.layernorm_after.weight', 'embeddings.position_embeddings', 'encoder.layer.2.layernorm_after.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.11.attention.attention.query.weight', 'encoder.layer.5.attention.attention.query.bias', 'encoder.layer.5.layernorm_after.bias', 'encoder.layer.9.attention.attention.query.weight', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.2.layernorm_before.bias', 'encoder.layer.2.output.dense.bias', 'encoder.layer.5.attention.attention.key.bias', 'encoder.layer.8.output.dense.bias', 'encoder.layer.1.layernorm_after.weight', 'encoder.layer.5.attention.attention.value.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.10.attention.attention.key.weight', 'encoder.layer.2.attention.attention.value.bias', 'encoder.layer.11.output.dense.bias', 'encoder.layer.9.attention.attention.value.weight', 'encoder.layer.5.layernorm_before.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.2.attention.attention.query.bias', 'encoder.layer.9.attention.attention.key.weight', 'encoder.layer.6.layernorm_after.weight', 'encoder.layer.6.attention.attention.value.bias', 'encoder.layer.11.attention.attention.value.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.6.output.dense.weight', 'encoder.layer.0.layernorm_after.weight', 'encoder.layer.4.attention.attention.key.weight', 'classifier.weight', 'encoder.layer.4.attention.attention.query.bias', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.9.attention.attention.query.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.4.layernorm_after.weight', 'encoder.layer.0.attention.attention.query.bias', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.11.layernorm_after.bias', 'encoder.layer.9.layernorm_after.bias', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.0.output.dense.bias', 'encoder.layer.8.layernorm_after.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.3.attention.attention.value.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.6.layernorm_before.weight', 'encoder.layer.8.attention.attention.key.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.3.layernorm_after.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.4.attention.attention.value.bias', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.9.output.dense.bias', 'encoder.layer.1.attention.attention.query.weight', 'encoder.layer.2.attention.attention.key.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.6.attention.attention.query.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.9.layernorm_after.weight', 'encoder.layer.3.attention.attention.key.bias', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.0.attention.attention.query.weight', 'encoder.layer.1.attention.attention.value.weight', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.8.layernorm_before.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.6.layernorm_before.bias', 'encoder.layer.2.attention.attention.value.weight', 'encoder.layer.1.attention.attention.value.bias', 'encoder.layer.7.attention.attention.key.bias', 'encoder.layer.6.output.dense.bias', 'layernorm.weight', 'encoder.layer.3.layernorm_after.bias', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.3.attention.attention.query.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.11.attention.attention.key.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.2.attention.attention.query.weight', 'encoder.layer.0.layernorm_after.bias', 'encoder.layer.6.layernorm_after.bias', 'encoder.layer.7.output.dense.bias', 'encoder.layer.6.attention.attention.key.weight', 'encoder.layer.5.layernorm_after.weight', 'embeddings.cls_token', 'encoder.layer.7.layernorm_before.weight', 'encoder.layer.8.layernorm_before.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.0.attention.attention.value.bias', 'classifier.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.1.attention.attention.key.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.11.attention.attention.value.bias', 'encoder.layer.10.layernorm_after.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.2.layernorm_before.weight', 'encoder.layer.10.layernorm_before.bias', 'encoder.layer.5.attention.attention.key.weight', 'encoder.layer.3.attention.attention.value.weight', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.7.attention.attention.key.weight', 'encoder.layer.0.layernorm_before.weight', 'encoder.layer.4.attention.attention.query.weight', 'embeddings.patch_embeddings.projection.bias', 'encoder.layer.7.layernorm_before.bias', 'encoder.layer.7.layernorm_after.bias', 'encoder.layer.3.output.dense.bias', 'encoder.layer.7.attention.attention.value.bias', 'encoder.layer.6.attention.attention.value.weight', 'encoder.layer.2.attention.attention.key.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.10.layernorm_after.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.4.layernorm_after.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.4.attention.attention.value.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.2.layernorm_after.weight', 'encoder.layer.0.attention.attention.value.weight', 'encoder.layer.2.intermediate.dense.bias', 'layernorm.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.5.output.dense.weight', 'encoder.layer.9.attention.attention.value.bias', 'encoder.layer.8.attention.attention.value.weight', 'encoder.layer.8.layernorm_after.bias', 'encoder.layer.1.layernorm_after.bias', 'encoder.layer.10.layernorm_before.weight', 'encoder.layer.0.attention.attention.key.bias', 'encoder.layer.11.attention.attention.key.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.10.attention.attention.query.weight', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.10.attention.attention.query.bias', 'encoder.layer.3.attention.attention.key.weight', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.8.attention.attention.key.bias', 'encoder.layer.10.attention.attention.value.bias', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.4.layernorm_before.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.1.layernorm_before.weight', 'encoder.layer.6.attention.attention.key.bias', 'encoder.layer.1.layernorm_before.bias', 'encoder.layer.0.attention.attention.key.weight', 'encoder.layer.11.attention.attention.query.bias', 'encoder.layer.5.layernorm_before.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_results = trainer.train()\n",
        "trainer.save_model()\n",
        "trainer.log_metrics(\"train\", train_results.metrics)\n",
        "trainer.save_metrics(\"train\", train_results.metrics)\n",
        "trainer.save_state()\n",
        "\n",
        "wandb.finish()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "fe15697ef5e440978bf3a7d292eee167",
            "08f9bf8e83d14bed971527a6b42de233",
            "5ce6178a83a7417caa7d12edd8f605f0",
            "a2989b368b6f4714a54e3bf44883dea9",
            "f3716049b140430390a8d75741b8a175",
            "aaa7f0909c614786aa66319c466c44d7",
            "8cf4b273c4e144eba1cda2c7a1a1aac1",
            "870a405122b049b8baf6fafce5e7f32f"
          ]
        },
        "id": "hJnxpA-GU0UM",
        "outputId": "3fc480e2-87f0-41b3-af39-9a24b6148e71"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You are using a model of type resnet to instantiate a model of type vit. This is not supported for all configurations of models and can yield errors.\n",
            "Some weights of the model checkpoint at microsoft/resnet-50 were not used when initializing ViTForImageClassification: ['resnet.embedder.embedder.normalization.running_var', 'resnet.encoder.stages.3.layers.2.layer.1.normalization.running_mean', 'resnet.encoder.stages.1.layers.0.layer.0.normalization.num_batches_tracked', 'resnet.encoder.stages.2.layers.3.layer.0.normalization.running_mean', 'resnet.encoder.stages.1.layers.3.layer.2.normalization.weight', 'resnet.encoder.stages.1.layers.3.layer.1.convolution.weight', 'resnet.encoder.stages.2.layers.1.layer.2.normalization.num_batches_tracked', 'resnet.encoder.stages.2.layers.0.layer.1.normalization.running_var', 'resnet.encoder.stages.2.layers.2.layer.2.normalization.running_mean', 'resnet.encoder.stages.3.layers.2.layer.0.normalization.bias', 'resnet.encoder.stages.1.layers.1.layer.0.normalization.running_var', 'resnet.embedder.embedder.convolution.weight', 'resnet.embedder.embedder.normalization.weight', 'resnet.encoder.stages.3.layers.2.layer.0.normalization.weight', 'resnet.encoder.stages.2.layers.0.layer.1.normalization.bias', 'resnet.encoder.stages.2.layers.4.layer.0.normalization.running_mean', 'resnet.encoder.stages.0.layers.2.layer.0.normalization.running_mean', 'resnet.encoder.stages.0.layers.0.layer.2.normalization.running_mean', 'resnet.encoder.stages.1.layers.1.layer.1.normalization.running_var', 'resnet.encoder.stages.1.layers.0.layer.0.convolution.weight', 'resnet.encoder.stages.1.layers.3.layer.0.normalization.bias', 'resnet.encoder.stages.3.layers.2.layer.2.normalization.bias', 'resnet.encoder.stages.2.layers.4.layer.1.normalization.running_mean', 'resnet.encoder.stages.3.layers.0.layer.1.normalization.num_batches_tracked', 'resnet.encoder.stages.3.layers.2.layer.1.normalization.running_var', 'resnet.encoder.stages.0.layers.1.layer.1.normalization.weight', 'resnet.encoder.stages.3.layers.1.layer.1.normalization.bias', 'resnet.encoder.stages.2.layers.5.layer.0.convolution.weight', 'resnet.encoder.stages.0.layers.2.layer.1.normalization.running_mean', 'resnet.encoder.stages.0.layers.1.layer.2.normalization.running_mean', 'resnet.encoder.stages.0.layers.0.layer.0.normalization.num_batches_tracked', 'resnet.encoder.stages.1.layers.1.layer.1.normalization.bias', 'resnet.encoder.stages.3.layers.0.layer.0.normalization.bias', 'resnet.encoder.stages.2.layers.4.layer.2.convolution.weight', 'resnet.encoder.stages.0.layers.1.layer.2.normalization.bias', 'resnet.encoder.stages.2.layers.5.layer.2.normalization.num_batches_tracked', 'resnet.encoder.stages.0.layers.0.layer.1.normalization.num_batches_tracked', 'resnet.encoder.stages.3.layers.0.layer.2.normalization.num_batches_tracked', 'resnet.encoder.stages.2.layers.1.layer.0.normalization.num_batches_tracked', 'resnet.encoder.stages.3.layers.1.layer.1.normalization.running_mean', 'resnet.encoder.stages.1.layers.0.layer.1.normalization.bias', 'resnet.encoder.stages.2.layers.3.layer.1.convolution.weight', 'resnet.encoder.stages.2.layers.3.layer.0.normalization.running_var', 'resnet.encoder.stages.3.layers.0.shortcut.convolution.weight', 'resnet.encoder.stages.0.layers.0.layer.2.convolution.weight', 'resnet.encoder.stages.1.layers.3.layer.2.normalization.num_batches_tracked', 'resnet.encoder.stages.3.layers.0.layer.1.convolution.weight', 'resnet.encoder.stages.3.layers.0.layer.2.normalization.running_mean', 'resnet.encoder.stages.3.layers.0.layer.0.normalization.weight', 'resnet.encoder.stages.2.layers.5.layer.0.normalization.weight', 'resnet.encoder.stages.1.layers.1.layer.2.normalization.bias', 'resnet.encoder.stages.2.layers.0.layer.0.normalization.num_batches_tracked', 'resnet.embedder.embedder.normalization.num_batches_tracked', 'resnet.encoder.stages.1.layers.2.layer.1.normalization.weight', 'resnet.encoder.stages.1.layers.3.layer.1.normalization.weight', 'resnet.encoder.stages.2.layers.5.layer.2.normalization.running_mean', 'resnet.encoder.stages.1.layers.0.layer.1.normalization.running_var', 'resnet.encoder.stages.3.layers.2.layer.0.normalization.running_var', 'resnet.encoder.stages.0.layers.0.layer.1.normalization.running_mean', 'resnet.encoder.stages.2.layers.0.layer.2.convolution.weight', 'resnet.encoder.stages.1.layers.2.layer.1.normalization.running_mean', 'resnet.encoder.stages.3.layers.1.layer.1.normalization.num_batches_tracked', 'resnet.encoder.stages.1.layers.0.shortcut.normalization.running_var', 'resnet.encoder.stages.2.layers.2.layer.2.normalization.num_batches_tracked', 'resnet.encoder.stages.3.layers.2.layer.1.normalization.bias', 'resnet.encoder.stages.1.layers.3.layer.0.normalization.running_var', 'resnet.encoder.stages.2.layers.1.layer.1.normalization.num_batches_tracked', 'resnet.encoder.stages.1.layers.1.layer.0.convolution.weight', 'resnet.encoder.stages.3.layers.0.layer.0.normalization.running_var', 'resnet.encoder.stages.2.layers.0.layer.1.normalization.num_batches_tracked', 'resnet.encoder.stages.1.layers.2.layer.2.normalization.running_mean', 'resnet.encoder.stages.2.layers.4.layer.0.normalization.bias', 'resnet.encoder.stages.2.layers.3.layer.0.convolution.weight', 'resnet.encoder.stages.2.layers.4.layer.1.normalization.num_batches_tracked', 'resnet.encoder.stages.3.layers.1.layer.2.normalization.weight', 'resnet.encoder.stages.3.layers.0.layer.2.normalization.running_var', 'resnet.encoder.stages.3.layers.0.layer.0.normalization.running_mean', 'resnet.encoder.stages.1.layers.0.layer.0.normalization.weight', 'resnet.encoder.stages.0.layers.0.layer.1.normalization.running_var', 'resnet.encoder.stages.2.layers.4.layer.2.normalization.num_batches_tracked', 'resnet.encoder.stages.2.layers.0.layer.1.normalization.weight', 'resnet.encoder.stages.2.layers.3.layer.0.normalization.bias', 'resnet.encoder.stages.0.layers.0.shortcut.normalization.num_batches_tracked', 'resnet.encoder.stages.0.layers.0.shortcut.normalization.running_var', 'resnet.encoder.stages.3.layers.1.layer.2.normalization.running_var', 'resnet.encoder.stages.0.layers.2.layer.1.convolution.weight', 'resnet.encoder.stages.0.layers.0.layer.2.normalization.bias', 'resnet.encoder.stages.1.layers.1.layer.2.normalization.running_var', 'resnet.encoder.stages.2.layers.4.layer.1.normalization.weight', 'resnet.encoder.stages.3.layers.0.layer.0.convolution.weight', 'resnet.encoder.stages.1.layers.0.shortcut.normalization.weight', 'resnet.encoder.stages.2.layers.2.layer.1.normalization.weight', 'resnet.encoder.stages.3.layers.2.layer.1.convolution.weight', 'resnet.encoder.stages.3.layers.1.layer.0.normalization.num_batches_tracked', 'resnet.encoder.stages.0.layers.2.layer.2.normalization.weight', 'resnet.encoder.stages.2.layers.4.layer.0.convolution.weight', 'resnet.encoder.stages.3.layers.2.layer.0.normalization.num_batches_tracked', 'resnet.encoder.stages.2.layers.2.layer.1.normalization.running_var', 'resnet.encoder.stages.2.layers.4.layer.2.normalization.running_mean', 'resnet.encoder.stages.3.layers.2.layer.1.normalization.weight', 'resnet.encoder.stages.0.layers.1.layer.1.convolution.weight', 'resnet.encoder.stages.2.layers.2.layer.1.normalization.bias', 'resnet.encoder.stages.1.layers.1.layer.0.normalization.num_batches_tracked', 'resnet.encoder.stages.1.layers.2.layer.0.convolution.weight', 'resnet.encoder.stages.2.layers.3.layer.1.normalization.running_var', 'resnet.encoder.stages.1.layers.2.layer.2.convolution.weight', 'resnet.encoder.stages.1.layers.3.layer.1.normalization.num_batches_tracked', 'resnet.encoder.stages.1.layers.0.layer.2.normalization.weight', 'resnet.encoder.stages.1.layers.2.layer.0.normalization.bias', 'resnet.encoder.stages.0.layers.1.layer.0.normalization.running_var', 'resnet.encoder.stages.1.layers.0.layer.1.normalization.running_mean', 'resnet.encoder.stages.3.layers.2.layer.1.normalization.num_batches_tracked', 'resnet.encoder.stages.2.layers.5.layer.1.normalization.weight', 'resnet.encoder.stages.3.layers.2.layer.2.normalization.weight', 'resnet.encoder.stages.3.layers.1.layer.0.normalization.bias', 'resnet.encoder.stages.3.layers.1.layer.0.normalization.running_mean', 'resnet.encoder.stages.2.layers.1.layer.2.convolution.weight', 'resnet.encoder.stages.0.layers.2.layer.0.normalization.bias', 'resnet.encoder.stages.2.layers.4.layer.2.normalization.running_var', 'resnet.encoder.stages.1.layers.3.layer.0.convolution.weight', 'resnet.encoder.stages.2.layers.0.layer.1.normalization.running_mean', 'resnet.encoder.stages.2.layers.0.shortcut.normalization.running_mean', 'resnet.encoder.stages.0.layers.2.layer.0.normalization.running_var', 'resnet.encoder.stages.1.layers.2.layer.0.normalization.weight', 'resnet.encoder.stages.2.layers.3.layer.2.convolution.weight', 'resnet.encoder.stages.2.layers.4.layer.1.normalization.bias', 'resnet.encoder.stages.0.layers.0.layer.0.normalization.running_mean', 'resnet.encoder.stages.2.layers.1.layer.2.normalization.running_mean', 'resnet.encoder.stages.1.layers.0.layer.2.normalization.num_batches_tracked', 'resnet.encoder.stages.2.layers.0.shortcut.normalization.running_var', 'resnet.encoder.stages.2.layers.4.layer.2.normalization.weight', 'resnet.encoder.stages.2.layers.2.layer.1.normalization.num_batches_tracked', 'resnet.encoder.stages.1.layers.1.layer.1.convolution.weight', 'resnet.encoder.stages.0.layers.1.layer.2.convolution.weight', 'resnet.encoder.stages.1.layers.3.layer.0.normalization.num_batches_tracked', 'resnet.encoder.stages.2.layers.3.layer.0.normalization.weight', 'resnet.encoder.stages.0.layers.1.layer.2.normalization.num_batches_tracked', 'resnet.encoder.stages.2.layers.2.layer.0.normalization.weight', 'resnet.encoder.stages.2.layers.0.layer.0.normalization.running_var', 'resnet.encoder.stages.1.layers.2.layer.0.normalization.running_mean', 'resnet.encoder.stages.0.layers.1.layer.2.normalization.running_var', 'resnet.encoder.stages.0.layers.0.layer.0.normalization.bias', 'resnet.encoder.stages.3.layers.0.layer.0.normalization.num_batches_tracked', 'resnet.encoder.stages.0.layers.0.layer.2.normalization.weight', 'resnet.encoder.stages.3.layers.0.shortcut.normalization.bias', 'resnet.encoder.stages.2.layers.3.layer.2.normalization.running_mean', 'resnet.encoder.stages.1.layers.3.layer.1.normalization.running_var', 'resnet.encoder.stages.2.layers.3.layer.2.normalization.running_var', 'resnet.encoder.stages.1.layers.2.layer.0.normalization.num_batches_tracked', 'resnet.encoder.stages.3.layers.1.layer.2.normalization.running_mean', 'resnet.encoder.stages.2.layers.0.layer.0.normalization.bias', 'resnet.encoder.stages.2.layers.1.layer.0.normalization.running_var', 'resnet.encoder.stages.2.layers.3.layer.2.normalization.num_batches_tracked', 'resnet.encoder.stages.0.layers.1.layer.1.normalization.running_var', 'resnet.encoder.stages.2.layers.0.shortcut.normalization.num_batches_tracked', 'resnet.encoder.stages.0.layers.2.layer.1.normalization.running_var', 'resnet.encoder.stages.3.layers.0.layer.1.normalization.bias', 'resnet.encoder.stages.0.layers.0.layer.2.normalization.running_var', 'resnet.encoder.stages.2.layers.1.layer.2.normalization.running_var', 'resnet.encoder.stages.1.layers.3.layer.0.normalization.running_mean', 'resnet.encoder.stages.3.layers.0.shortcut.normalization.weight', 'resnet.encoder.stages.1.layers.2.layer.1.normalization.bias', 'resnet.encoder.stages.3.layers.0.layer.1.normalization.running_mean', 'resnet.encoder.stages.2.layers.3.layer.1.normalization.bias', 'resnet.encoder.stages.3.layers.0.layer.2.normalization.weight', 'resnet.encoder.stages.2.layers.4.layer.2.normalization.bias', 'resnet.encoder.stages.2.layers.0.layer.0.normalization.weight', 'resnet.encoder.stages.0.layers.1.layer.0.convolution.weight', 'resnet.encoder.stages.3.layers.1.layer.1.convolution.weight', 'resnet.encoder.stages.3.layers.1.layer.2.normalization.bias', 'resnet.encoder.stages.0.layers.1.layer.0.normalization.weight', 'resnet.encoder.stages.2.layers.2.layer.2.convolution.weight', 'resnet.encoder.stages.2.layers.5.layer.2.normalization.bias', 'resnet.encoder.stages.0.layers.2.layer.0.convolution.weight', 'resnet.encoder.stages.2.layers.5.layer.2.normalization.weight', 'resnet.encoder.stages.2.layers.4.layer.0.normalization.num_batches_tracked', 'resnet.encoder.stages.1.layers.2.layer.0.normalization.running_var', 'resnet.encoder.stages.3.layers.2.layer.2.normalization.running_var', 'resnet.encoder.stages.2.layers.5.layer.1.normalization.num_batches_tracked', 'resnet.encoder.stages.2.layers.5.layer.2.convolution.weight', 'resnet.encoder.stages.2.layers.1.layer.1.normalization.running_var', 'resnet.encoder.stages.1.layers.1.layer.0.normalization.running_mean', 'resnet.encoder.stages.2.layers.2.layer.1.convolution.weight', 'resnet.encoder.stages.2.layers.0.shortcut.normalization.weight', 'resnet.encoder.stages.2.layers.0.layer.2.normalization.running_mean', 'resnet.encoder.stages.0.layers.0.shortcut.normalization.weight', 'resnet.encoder.stages.0.layers.2.layer.1.normalization.bias', 'resnet.encoder.stages.3.layers.0.shortcut.normalization.num_batches_tracked', 'resnet.encoder.stages.3.layers.1.layer.2.convolution.weight', 'resnet.encoder.stages.1.layers.0.layer.1.convolution.weight', 'resnet.encoder.stages.3.layers.0.layer.1.normalization.running_var', 'resnet.encoder.stages.3.layers.1.layer.1.normalization.running_var', 'resnet.encoder.stages.3.layers.2.layer.2.convolution.weight', 'resnet.encoder.stages.2.layers.0.layer.2.normalization.bias', 'resnet.encoder.stages.1.layers.2.layer.1.normalization.num_batches_tracked', 'resnet.encoder.stages.1.layers.0.shortcut.normalization.num_batches_tracked', 'resnet.encoder.stages.1.layers.2.layer.2.normalization.weight', 'resnet.encoder.stages.2.layers.1.layer.1.normalization.running_mean', 'resnet.encoder.stages.1.layers.1.layer.0.normalization.weight', 'resnet.encoder.stages.3.layers.1.layer.1.normalization.weight', 'resnet.encoder.stages.2.layers.4.layer.1.normalization.running_var', 'resnet.encoder.stages.2.layers.5.layer.0.normalization.running_var', 'resnet.encoder.stages.2.layers.5.layer.1.normalization.bias', 'resnet.encoder.stages.2.layers.1.layer.1.convolution.weight', 'resnet.encoder.stages.2.layers.2.layer.2.normalization.bias', 'resnet.encoder.stages.1.layers.0.layer.2.normalization.running_var', 'resnet.encoder.stages.1.layers.0.layer.0.normalization.running_mean', 'resnet.encoder.stages.3.layers.2.layer.0.convolution.weight', 'resnet.encoder.stages.1.layers.0.layer.1.normalization.num_batches_tracked', 'resnet.encoder.stages.1.layers.2.layer.1.convolution.weight', 'resnet.encoder.stages.0.layers.0.layer.2.normalization.num_batches_tracked', 'resnet.encoder.stages.2.layers.1.layer.2.normalization.bias', 'resnet.encoder.stages.2.layers.5.layer.2.normalization.running_var', 'resnet.encoder.stages.1.layers.0.layer.2.convolution.weight', 'resnet.encoder.stages.3.layers.0.layer.2.convolution.weight', 'resnet.encoder.stages.2.layers.1.layer.0.normalization.running_mean', 'resnet.encoder.stages.1.layers.3.layer.2.normalization.bias', 'resnet.encoder.stages.2.layers.5.layer.0.normalization.running_mean', 'resnet.encoder.stages.1.layers.3.layer.2.convolution.weight', 'classifier.1.bias', 'resnet.encoder.stages.1.layers.2.layer.2.normalization.num_batches_tracked', 'resnet.encoder.stages.1.layers.0.layer.0.normalization.running_var', 'resnet.encoder.stages.2.layers.5.layer.1.convolution.weight', 'resnet.encoder.stages.0.layers.2.layer.2.normalization.running_var', 'resnet.encoder.stages.1.layers.0.layer.0.normalization.bias', 'resnet.encoder.stages.2.layers.0.layer.2.normalization.num_batches_tracked', 'resnet.encoder.stages.2.layers.5.layer.0.normalization.bias', 'resnet.encoder.stages.0.layers.2.layer.2.normalization.bias', 'resnet.encoder.stages.2.layers.2.layer.0.normalization.num_batches_tracked', 'resnet.encoder.stages.0.layers.1.layer.0.normalization.running_mean', 'resnet.encoder.stages.0.layers.2.layer.0.normalization.num_batches_tracked', 'classifier.1.weight', 'resnet.encoder.stages.0.layers.1.layer.1.normalization.bias', 'resnet.encoder.stages.0.layers.1.layer.2.normalization.weight', 'resnet.encoder.stages.0.layers.0.layer.1.normalization.bias', 'resnet.encoder.stages.0.layers.0.layer.0.normalization.running_var', 'resnet.encoder.stages.2.layers.2.layer.0.normalization.bias', 'resnet.encoder.stages.3.layers.2.layer.2.normalization.num_batches_tracked', 'resnet.encoder.stages.2.layers.4.layer.1.convolution.weight', 'resnet.encoder.stages.2.layers.3.layer.1.normalization.running_mean', 'resnet.encoder.stages.0.layers.2.layer.1.normalization.weight', 'resnet.encoder.stages.1.layers.2.layer.2.normalization.running_var', 'resnet.encoder.stages.0.layers.0.shortcut.normalization.bias', 'resnet.encoder.stages.1.layers.0.shortcut.normalization.running_mean', 'resnet.encoder.stages.1.layers.1.layer.1.normalization.running_mean', 'resnet.encoder.stages.0.layers.1.layer.0.normalization.num_batches_tracked', 'resnet.embedder.embedder.normalization.running_mean', 'resnet.encoder.stages.2.layers.2.layer.0.convolution.weight', 'resnet.encoder.stages.0.layers.0.layer.0.convolution.weight', 'resnet.encoder.stages.2.layers.3.layer.2.normalization.bias', 'resnet.encoder.stages.2.layers.5.layer.0.normalization.num_batches_tracked', 'resnet.encoder.stages.3.layers.1.layer.0.normalization.weight', 'resnet.encoder.stages.2.layers.0.shortcut.normalization.bias', 'resnet.encoder.stages.2.layers.3.layer.2.normalization.weight', 'resnet.encoder.stages.0.layers.0.shortcut.normalization.running_mean', 'resnet.encoder.stages.3.layers.0.layer.1.normalization.weight', 'resnet.encoder.stages.0.layers.2.layer.0.normalization.weight', 'resnet.encoder.stages.2.layers.4.layer.0.normalization.weight', 'resnet.encoder.stages.2.layers.2.layer.0.normalization.running_var', 'resnet.encoder.stages.0.layers.2.layer.2.convolution.weight', 'resnet.encoder.stages.2.layers.1.layer.2.normalization.weight', 'resnet.encoder.stages.1.layers.0.layer.2.normalization.bias', 'resnet.encoder.stages.1.layers.1.layer.0.normalization.bias', 'resnet.encoder.stages.2.layers.1.layer.1.normalization.weight', 'resnet.encoder.stages.2.layers.3.layer.0.normalization.num_batches_tracked', 'resnet.encoder.stages.3.layers.1.layer.0.normalization.running_var', 'resnet.encoder.stages.2.layers.1.layer.0.normalization.bias', 'resnet.encoder.stages.1.layers.2.layer.1.normalization.running_var', 'resnet.encoder.stages.0.layers.1.layer.0.normalization.bias', 'resnet.encoder.stages.0.layers.0.layer.1.convolution.weight', 'resnet.encoder.stages.3.layers.1.layer.2.normalization.num_batches_tracked', 'resnet.encoder.stages.0.layers.0.layer.0.normalization.weight', 'resnet.encoder.stages.1.layers.1.layer.1.normalization.num_batches_tracked', 'resnet.encoder.stages.1.layers.1.layer.2.normalization.running_mean', 'resnet.encoder.stages.2.layers.2.layer.1.normalization.running_mean', 'resnet.encoder.stages.2.layers.5.layer.1.normalization.running_var', 'resnet.encoder.stages.1.layers.2.layer.2.normalization.bias', 'resnet.encoder.stages.1.layers.0.layer.1.normalization.weight', 'resnet.encoder.stages.1.layers.1.layer.2.convolution.weight', 'resnet.encoder.stages.2.layers.1.layer.0.convolution.weight', 'resnet.encoder.stages.3.layers.1.layer.0.convolution.weight', 'resnet.encoder.stages.2.layers.5.layer.1.normalization.running_mean', 'resnet.encoder.stages.3.layers.0.shortcut.normalization.running_var', 'resnet.encoder.stages.0.layers.2.layer.2.normalization.num_batches_tracked', 'resnet.encoder.stages.1.layers.1.layer.1.normalization.weight', 'resnet.encoder.stages.0.layers.1.layer.1.normalization.num_batches_tracked', 'resnet.encoder.stages.3.layers.0.layer.2.normalization.bias', 'resnet.encoder.stages.2.layers.0.layer.0.normalization.running_mean', 'resnet.encoder.stages.0.layers.2.layer.1.normalization.num_batches_tracked', 'resnet.encoder.stages.2.layers.3.layer.1.normalization.weight', 'resnet.encoder.stages.1.layers.3.layer.1.normalization.running_mean', 'resnet.encoder.stages.1.layers.0.layer.2.normalization.running_mean', 'resnet.encoder.stages.2.layers.0.layer.2.normalization.weight', 'resnet.encoder.stages.2.layers.0.layer.0.convolution.weight', 'resnet.encoder.stages.1.layers.0.shortcut.normalization.bias', 'resnet.encoder.stages.2.layers.2.layer.2.normalization.weight', 'resnet.encoder.stages.0.layers.2.layer.2.normalization.running_mean', 'resnet.encoder.stages.1.layers.1.layer.2.normalization.num_batches_tracked', 'resnet.encoder.stages.2.layers.0.layer.1.convolution.weight', 'resnet.encoder.stages.1.layers.3.layer.2.normalization.running_mean', 'resnet.encoder.stages.3.layers.2.layer.2.normalization.running_mean', 'resnet.encoder.stages.2.layers.4.layer.0.normalization.running_var', 'resnet.encoder.stages.1.layers.1.layer.2.normalization.weight', 'resnet.embedder.embedder.normalization.bias', 'resnet.encoder.stages.1.layers.3.layer.0.normalization.weight', 'resnet.encoder.stages.3.layers.0.shortcut.normalization.running_mean', 'resnet.encoder.stages.1.layers.3.layer.1.normalization.bias', 'resnet.encoder.stages.0.layers.0.shortcut.convolution.weight', 'resnet.encoder.stages.0.layers.1.layer.1.normalization.running_mean', 'resnet.encoder.stages.2.layers.2.layer.2.normalization.running_var', 'resnet.encoder.stages.3.layers.2.layer.0.normalization.running_mean', 'resnet.encoder.stages.2.layers.1.layer.1.normalization.bias', 'resnet.encoder.stages.0.layers.0.layer.1.normalization.weight', 'resnet.encoder.stages.2.layers.0.layer.2.normalization.running_var', 'resnet.encoder.stages.2.layers.0.shortcut.convolution.weight', 'resnet.encoder.stages.1.layers.0.shortcut.convolution.weight', 'resnet.encoder.stages.2.layers.3.layer.1.normalization.num_batches_tracked', 'resnet.encoder.stages.2.layers.1.layer.0.normalization.weight', 'resnet.encoder.stages.1.layers.3.layer.2.normalization.running_var', 'resnet.encoder.stages.2.layers.2.layer.0.normalization.running_mean']\n",
            "- This IS expected if you are initializing ViTForImageClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing ViTForImageClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of ViTForImageClassification were not initialized from the model checkpoint at microsoft/resnet-50 and are newly initialized: ['encoder.layer.10.attention.output.dense.bias', 'encoder.layer.11.layernorm_after.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.3.layernorm_before.bias', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.4.output.dense.bias', 'encoder.layer.7.attention.attention.value.weight', 'encoder.layer.11.layernorm_before.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.8.attention.attention.query.bias', 'encoder.layer.1.attention.attention.key.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.8.attention.attention.value.bias', 'encoder.layer.11.layernorm_before.weight', 'encoder.layer.7.attention.attention.query.bias', 'encoder.layer.4.attention.attention.key.bias', 'encoder.layer.1.attention.attention.query.bias', 'encoder.layer.9.layernorm_before.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.10.attention.attention.value.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.9.attention.attention.key.bias', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.9.layernorm_before.bias', 'encoder.layer.8.attention.attention.query.weight', 'encoder.layer.9.output.dense.weight', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.5.attention.attention.query.weight', 'encoder.layer.7.attention.attention.query.weight', 'encoder.layer.4.layernorm_before.weight', 'embeddings.patch_embeddings.projection.weight', 'encoder.layer.0.layernorm_before.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.3.layernorm_before.weight', 'encoder.layer.6.attention.attention.query.weight', 'encoder.layer.3.attention.attention.query.weight', 'encoder.layer.7.output.dense.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.5.attention.attention.value.bias', 'encoder.layer.10.attention.attention.key.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.7.layernorm_after.weight', 'embeddings.position_embeddings', 'encoder.layer.2.layernorm_after.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.11.attention.attention.query.weight', 'encoder.layer.5.attention.attention.query.bias', 'encoder.layer.5.layernorm_after.bias', 'encoder.layer.9.attention.attention.query.weight', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.2.layernorm_before.bias', 'encoder.layer.2.output.dense.bias', 'encoder.layer.5.attention.attention.key.bias', 'encoder.layer.8.output.dense.bias', 'encoder.layer.1.layernorm_after.weight', 'encoder.layer.5.attention.attention.value.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.10.attention.attention.key.weight', 'encoder.layer.2.attention.attention.value.bias', 'encoder.layer.11.output.dense.bias', 'encoder.layer.9.attention.attention.value.weight', 'encoder.layer.5.layernorm_before.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.2.attention.attention.query.bias', 'encoder.layer.9.attention.attention.key.weight', 'encoder.layer.6.layernorm_after.weight', 'encoder.layer.6.attention.attention.value.bias', 'encoder.layer.11.attention.attention.value.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.6.output.dense.weight', 'encoder.layer.0.layernorm_after.weight', 'encoder.layer.4.attention.attention.key.weight', 'classifier.weight', 'encoder.layer.4.attention.attention.query.bias', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.9.attention.attention.query.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.4.layernorm_after.weight', 'encoder.layer.0.attention.attention.query.bias', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.11.layernorm_after.bias', 'encoder.layer.9.layernorm_after.bias', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.0.output.dense.bias', 'encoder.layer.8.layernorm_after.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.3.attention.attention.value.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.6.layernorm_before.weight', 'encoder.layer.8.attention.attention.key.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.3.layernorm_after.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.4.attention.attention.value.bias', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.9.output.dense.bias', 'encoder.layer.1.attention.attention.query.weight', 'encoder.layer.2.attention.attention.key.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.6.attention.attention.query.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.9.layernorm_after.weight', 'encoder.layer.3.attention.attention.key.bias', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.0.attention.attention.query.weight', 'encoder.layer.1.attention.attention.value.weight', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.8.layernorm_before.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.6.layernorm_before.bias', 'encoder.layer.2.attention.attention.value.weight', 'encoder.layer.1.attention.attention.value.bias', 'encoder.layer.7.attention.attention.key.bias', 'encoder.layer.6.output.dense.bias', 'layernorm.weight', 'encoder.layer.3.layernorm_after.bias', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.3.attention.attention.query.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.11.attention.attention.key.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.2.attention.attention.query.weight', 'encoder.layer.0.layernorm_after.bias', 'encoder.layer.6.layernorm_after.bias', 'encoder.layer.7.output.dense.bias', 'encoder.layer.6.attention.attention.key.weight', 'encoder.layer.5.layernorm_after.weight', 'embeddings.cls_token', 'encoder.layer.7.layernorm_before.weight', 'encoder.layer.8.layernorm_before.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.0.attention.attention.value.bias', 'classifier.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.1.attention.attention.key.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.11.attention.attention.value.bias', 'encoder.layer.10.layernorm_after.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.2.layernorm_before.weight', 'encoder.layer.10.layernorm_before.bias', 'encoder.layer.5.attention.attention.key.weight', 'encoder.layer.3.attention.attention.value.weight', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.7.attention.attention.key.weight', 'encoder.layer.0.layernorm_before.weight', 'encoder.layer.4.attention.attention.query.weight', 'embeddings.patch_embeddings.projection.bias', 'encoder.layer.7.layernorm_before.bias', 'encoder.layer.7.layernorm_after.bias', 'encoder.layer.3.output.dense.bias', 'encoder.layer.7.attention.attention.value.bias', 'encoder.layer.6.attention.attention.value.weight', 'encoder.layer.2.attention.attention.key.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.10.layernorm_after.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.4.layernorm_after.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.4.attention.attention.value.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.2.layernorm_after.weight', 'encoder.layer.0.attention.attention.value.weight', 'encoder.layer.2.intermediate.dense.bias', 'layernorm.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.5.output.dense.weight', 'encoder.layer.9.attention.attention.value.bias', 'encoder.layer.8.attention.attention.value.weight', 'encoder.layer.8.layernorm_after.bias', 'encoder.layer.1.layernorm_after.bias', 'encoder.layer.10.layernorm_before.weight', 'encoder.layer.0.attention.attention.key.bias', 'encoder.layer.11.attention.attention.key.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.10.attention.attention.query.weight', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.10.attention.attention.query.bias', 'encoder.layer.3.attention.attention.key.weight', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.8.attention.attention.key.bias', 'encoder.layer.10.attention.attention.value.bias', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.4.layernorm_before.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.1.layernorm_before.weight', 'encoder.layer.6.attention.attention.key.bias', 'encoder.layer.1.layernorm_before.bias', 'encoder.layer.0.attention.attention.key.weight', 'encoder.layer.11.attention.attention.query.bias', 'encoder.layer.5.layernorm_before.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='770' max='770' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [770/770 06:42, Epoch 5/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>1.058200</td>\n",
              "      <td>1.007590</td>\n",
              "      <td>0.494290</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.944900</td>\n",
              "      <td>0.962383</td>\n",
              "      <td>0.481240</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>0.906700</td>\n",
              "      <td>0.788853</td>\n",
              "      <td>0.675367</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.822800</td>\n",
              "      <td>0.882754</td>\n",
              "      <td>0.598695</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>0.945000</td>\n",
              "      <td>0.752794</td>\n",
              "      <td>0.667210</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>0.990200</td>\n",
              "      <td>0.718278</td>\n",
              "      <td>0.657423</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>350</td>\n",
              "      <td>0.643100</td>\n",
              "      <td>0.625987</td>\n",
              "      <td>0.719413</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>0.781800</td>\n",
              "      <td>0.814145</td>\n",
              "      <td>0.655791</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>450</td>\n",
              "      <td>0.598300</td>\n",
              "      <td>0.783055</td>\n",
              "      <td>0.690049</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.574400</td>\n",
              "      <td>0.606931</td>\n",
              "      <td>0.732463</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>550</td>\n",
              "      <td>0.564200</td>\n",
              "      <td>0.623138</td>\n",
              "      <td>0.730832</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>0.600200</td>\n",
              "      <td>0.628611</td>\n",
              "      <td>0.730832</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>650</td>\n",
              "      <td>0.508100</td>\n",
              "      <td>0.609853</td>\n",
              "      <td>0.750408</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>0.603800</td>\n",
              "      <td>0.712227</td>\n",
              "      <td>0.712887</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>750</td>\n",
              "      <td>0.519700</td>\n",
              "      <td>0.615449</td>\n",
              "      <td>0.743883</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "***** train metrics *****\n",
            "  epoch                    =         5.0\n",
            "  total_flos               = 884451758GF\n",
            "  train_loss               =      0.7807\n",
            "  train_runtime            =  0:06:42.59\n",
            "  train_samples_per_second =       30.44\n",
            "  train_steps_per_second   =       1.913\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(Label(value='0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max‚Ä¶"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fe15697ef5e440978bf3a7d292eee167"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>‚ñÅ‚ñÅ‚ñÜ‚ñÑ‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñà‚ñá‚ñá‚ñà‚ñá‚ñà</td></tr><tr><td>eval/loss</td><td>‚ñà‚ñá‚ñÑ‚ñÜ‚ñÑ‚ñÉ‚ñÅ‚ñÖ‚ñÑ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÉ‚ñÅ</td></tr><tr><td>eval/runtime</td><td>‚ñÇ‚ñÇ‚ñÑ‚ñÉ‚ñÉ‚ñÅ‚ñÅ‚ñÉ‚ñà‚ñÖ‚ñÜ‚ñÉ‚ñÇ‚ñÖ‚ñÑ</td></tr><tr><td>eval/samples_per_second</td><td>‚ñÜ‚ñá‚ñÖ‚ñÜ‚ñÜ‚ñà‚ñà‚ñÜ‚ñÅ‚ñÑ‚ñÉ‚ñÜ‚ñá‚ñÉ‚ñÖ</td></tr><tr><td>eval/steps_per_second</td><td>‚ñÜ‚ñá‚ñÖ‚ñÜ‚ñÜ‚ñà‚ñà‚ñÜ‚ñÅ‚ñÑ‚ñÉ‚ñÜ‚ñá‚ñÉ‚ñÖ</td></tr><tr><td>train/epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>train/global_step</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>train/learning_rate</td><td>‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>train/loss</td><td>‚ñà‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>train/total_flos</td><td>‚ñÅ</td></tr><tr><td>train/train_loss</td><td>‚ñÅ</td></tr><tr><td>train/train_runtime</td><td>‚ñÅ</td></tr><tr><td>train/train_samples_per_second</td><td>‚ñÅ</td></tr><tr><td>train/train_steps_per_second</td><td>‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>0.74388</td></tr><tr><td>eval/loss</td><td>0.61545</td></tr><tr><td>eval/runtime</td><td>8.7986</td></tr><tr><td>eval/samples_per_second</td><td>69.67</td></tr><tr><td>eval/steps_per_second</td><td>8.751</td></tr><tr><td>train/epoch</td><td>5.0</td></tr><tr><td>train/global_step</td><td>770</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.5562</td></tr><tr><td>train/total_flos</td><td>9.496728442678579e+17</td></tr><tr><td>train/train_loss</td><td>0.78066</td></tr><tr><td>train/train_runtime</td><td>402.5993</td></tr><tr><td>train/train_samples_per_second</td><td>30.44</td></tr><tr><td>train/train_steps_per_second</td><td>1.913</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">feasible-sun-2</strong> at: <a href='https://wandb.ai/deep_bekeif/cnn_with_aug/runs/9zzbef2a' target=\"_blank\">https://wandb.ai/deep_bekeif/cnn_with_aug/runs/9zzbef2a</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20230702_112647-9zzbef2a/logs</code>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#display Confusion Matrix\n",
        "pred = trainer.predict(test_dataset)\n",
        "\n",
        "cm_display = met.ConfusionMatrixDisplay(confusion_matrix = compute_metrics(pred), display_labels = labels_names)\n",
        "\n",
        "cm_display.plot()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "id": "p8k46NAXXsdk",
        "outputId": "1e438153-6ff2-4303-8e29-4d0107b0753f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#EfficientFormer"
      ],
      "metadata": {
        "id": "35MYeAQp5MRs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Import and create model"
      ],
      "metadata": {
        "id": "9EXj9vb45cun"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import EfficientFormerForImageClassification\n",
        "from transformers import AutoImageProcessor\n",
        "\n",
        "# define the EfficientFormer Model\n",
        "def model_init():\n",
        "  model = EfficientFormerForImageClassification.from_pretrained(\"snap-research/efficientformer-l1-300\")\n",
        "  model.config.label2id = {tumor_type: i for i, tumor_type in enumerate(labels_names)}\n",
        "  model.config.num_labels = len(labels_names)\n",
        "  model.config.id2label = {str(i): tumor_type for i, tumor_type in enumerate(labels_names)}\n",
        "  return model"
      ],
      "metadata": {
        "id": "uQb9OvgC5NST"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Create Transforms"
      ],
      "metadata": {
        "id": "v0l6ll_l6FdW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()\n",
        "# Define the augmentation pipeline with imgaug\n",
        "augmenter = iaa.Sequential([\n",
        "    iaa.Fliplr(0.5),  # horizontally flip with probability of 0.5\n",
        "    iaa.Sometimes(0.3, iaa.GaussianBlur((0, 1.0))),  # apply Gaussian blur with probability of 0.3\n",
        "    iaa.Sometimes(0.5, iaa.SaltAndPepper(0.05)),  # Add Salt and Pepper noise with probability of 0.5\n",
        "    iaa.Sometimes(0.3, iaa.AdditivePoissonNoise(lam=(0, 30))),  # Add Poisson noise with probability of 0.3\n",
        "])\n",
        "\n",
        "# imgaug works with numpy images (HWC) but torchvision with PIL images, so we have to convert\n",
        "imgaug_transform = Lambda(lambda img: augmenter.augment_image(np.array(img)))\n",
        "\n",
        "# Define transformations for the train dataset (without augmentation)\n",
        "train_transforms_without_aug = transforms.Compose([\n",
        "    transforms.Resize((224, 224),interpolation=Image.BILINEAR),  # Resize 512x512 images to 224x224 using bilinear interpolation\n",
        "    transforms.ToTensor(),  # Convert image to PyTorch tensor and automatically rescale to [0,1] by dividing by 255\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize image tensor to range [-1,1]\n",
        "])\n",
        "\n",
        "# Define transformations for the train dataset (including augmentation)\n",
        "train_transforms_with_aug = transforms.Compose([\n",
        "    transforms.Resize((224, 224),interpolation=Image.BILINEAR),  # Resize 512x512 images to 224x224 using bilinear interpolation\n",
        "    imgaug_transform,\n",
        "    transforms.ToTensor(),  # Convert image to PyTorch tensor and automatically rescale to [0,1] by dividing by 255\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize image tensor to range [-1,1]\n",
        "])\n",
        "\n",
        "# Define transformations for the test dataset\n",
        "test_transforms = transforms.Compose([\n",
        "    transforms.Resize((224, 224),interpolation=Image.BILINEAR),  # Resize 512x512 images to 224x224 using bilinear interpolation\n",
        "    transforms.ToTensor(),  # Convert image to PyTorch tensor and automatically rescale to [0,1] by dividing by 255\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize image tensor to range [-1,1]\n",
        "])"
      ],
      "metadata": {
        "id": "Kkpepp-U5nHM"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Without Augmentation"
      ],
      "metadata": {
        "id": "KN_LdgEo6gFh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#This needs to be re-defined when the notebook is this long:\n",
        "def collate_fn(batch):\n",
        "    return {\n",
        "        'pixel_values': torch.stack([x['pixel_values'] for x in batch]),\n",
        "        'labels': torch.tensor([x['labels'] for x in batch])\n",
        "    }\n",
        "\n",
        "metric = load_metric(\"accuracy\")\n",
        "\n",
        "def compute_metrics(p):\n",
        "    return metric.compute(predictions=np.argmax(p.predictions, axis=1), references=p.label_ids)"
      ],
      "metadata": {
        "id": "nNxjVzTU-8Et"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the datasets from folders and process the dataset with the transform\n",
        "train_dataset = ImageFolder(data_path + '/brain_dataset/train', transform=train_transforms_without_aug)\n",
        "test_dataset = ImageFolder(data_path + '/brain_dataset/test', transform=test_transforms)"
      ],
      "metadata": {
        "id": "usHAE5GE6gfD"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize a new run\n",
        "\n",
        "# Define the TrainingArguments\n",
        "training_args = TrainingArguments(\n",
        "  output_dir=output_vit_no_aug,\n",
        "  per_device_train_batch_size=16,\n",
        "  evaluation_strategy=\"steps\",\n",
        "  num_train_epochs=15,\n",
        "  fp16=True,\n",
        "  save_steps=100,#100\n",
        "  eval_steps=50,#100\n",
        "  logging_steps=10,\n",
        "  learning_rate=2e-4,\n",
        "  save_total_limit=2,\n",
        "  remove_unused_columns=False,\n",
        "  push_to_hub=False,\n",
        "  report_to='wandb',\n",
        "  load_best_model_at_end=True,\n",
        "  seed = seed\n",
        ")\n",
        "\n",
        "\n",
        "trainer = Trainer(\n",
        "    model_init=model_init,\n",
        "    args=training_args,\n",
        "    compute_metrics=compute_metrics,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset,\n",
        "    data_collator= lambda samples: {'pixel_values': torch.stack([sample[0] for sample in samples]),\n",
        "                                   'labels': torch.tensor([sample[1] for sample in samples])},\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OylaRYOP6xDF",
        "outputId": "586e9c1f-7f33-4a69-828f-df3108dd6ace"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at snap-research/efficientformer-l1-300 were not used when initializing EfficientFormerForImageClassification: ['distillation_classifier.bias', 'distillation_classifier.weight']\n",
            "- This IS expected if you are initializing EfficientFormerForImageClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing EfficientFormerForImageClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.init(project=\"eff_vit_with_aug\")\n",
        "#Train model:\n",
        "train_results = trainer.train()\n",
        "trainer.save_model()\n",
        "trainer.log_metrics(\"train\", train_results.metrics)\n",
        "trainer.save_metrics(\"train\", train_results.metrics)\n",
        "trainer.save_state()\n",
        "wandb.finish()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "d84568dfd5154dc28bf0b55684e317a0",
            "0b071901c80a442a8f77d43712f8bfd0",
            "f182459aef0e4bd0aa38bfd5b9e6e1fb",
            "86ec1480b3bc41d098970a75294ab83d",
            "daae331cb81c493da12822750e7cb8ba",
            "55d3d2c481564c5db094b016db76ff4b",
            "0b146b7f58a44e9aa5f2644f0aeea8bd",
            "6d82c55b9be745cba015af009c81f364"
          ]
        },
        "id": "LEkPJOsV0UYl",
        "outputId": "864b6ded-7499-468c-a2eb-55ffddc17e54"
      },
      "execution_count": 19,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mlitvingil\u001b[0m (\u001b[33mdeep_bekeif\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.15.5"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20230706_113607-3npklywb</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/deep_bekeif/eff_vit_with_aug/runs/3npklywb' target=\"_blank\">sparkling-violet-3</a></strong> to <a href='https://wandb.ai/deep_bekeif/eff_vit_with_aug' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/deep_bekeif/eff_vit_with_aug' target=\"_blank\">https://wandb.ai/deep_bekeif/eff_vit_with_aug</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/deep_bekeif/eff_vit_with_aug/runs/3npklywb' target=\"_blank\">https://wandb.ai/deep_bekeif/eff_vit_with_aug/runs/3npklywb</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at snap-research/efficientformer-l1-300 were not used when initializing EfficientFormerForImageClassification: ['distillation_classifier.bias', 'distillation_classifier.weight']\n",
            "- This IS expected if you are initializing EfficientFormerForImageClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing EfficientFormerForImageClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='515' max='2310' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 515/2310 03:15 < 11:25, 2.62 it/s, Epoch 3.34/15]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.497700</td>\n",
              "      <td>1.165688</td>\n",
              "      <td>0.649266</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.426300</td>\n",
              "      <td>0.329744</td>\n",
              "      <td>0.882545</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>0.221000</td>\n",
              "      <td>0.308642</td>\n",
              "      <td>0.903752</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.197200</td>\n",
              "      <td>0.184380</td>\n",
              "      <td>0.960848</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>0.166100</td>\n",
              "      <td>0.507530</td>\n",
              "      <td>0.838499</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>0.057100</td>\n",
              "      <td>0.495160</td>\n",
              "      <td>0.897227</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>350</td>\n",
              "      <td>0.024100</td>\n",
              "      <td>0.116779</td>\n",
              "      <td>0.977162</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>0.002900</td>\n",
              "      <td>0.345643</td>\n",
              "      <td>0.949429</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>450</td>\n",
              "      <td>0.025400</td>\n",
              "      <td>0.251672</td>\n",
              "      <td>0.959217</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.019300</td>\n",
              "      <td>0.119560</td>\n",
              "      <td>0.977162</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2310' max='2310' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2310/2310 14:48, Epoch 15/15]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.497700</td>\n",
              "      <td>1.165688</td>\n",
              "      <td>0.649266</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.426300</td>\n",
              "      <td>0.329744</td>\n",
              "      <td>0.882545</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>0.221000</td>\n",
              "      <td>0.308642</td>\n",
              "      <td>0.903752</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.197200</td>\n",
              "      <td>0.184380</td>\n",
              "      <td>0.960848</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>0.166100</td>\n",
              "      <td>0.507530</td>\n",
              "      <td>0.838499</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>0.057100</td>\n",
              "      <td>0.495160</td>\n",
              "      <td>0.897227</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>350</td>\n",
              "      <td>0.024100</td>\n",
              "      <td>0.116779</td>\n",
              "      <td>0.977162</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>0.002900</td>\n",
              "      <td>0.345643</td>\n",
              "      <td>0.949429</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>450</td>\n",
              "      <td>0.025400</td>\n",
              "      <td>0.251672</td>\n",
              "      <td>0.959217</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.019300</td>\n",
              "      <td>0.119560</td>\n",
              "      <td>0.977162</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>550</td>\n",
              "      <td>0.006200</td>\n",
              "      <td>0.113984</td>\n",
              "      <td>0.978793</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>0.041300</td>\n",
              "      <td>0.169076</td>\n",
              "      <td>0.967374</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>650</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.128103</td>\n",
              "      <td>0.980424</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>0.000300</td>\n",
              "      <td>0.137957</td>\n",
              "      <td>0.975530</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>750</td>\n",
              "      <td>0.000500</td>\n",
              "      <td>0.076592</td>\n",
              "      <td>0.982055</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>0.000100</td>\n",
              "      <td>0.208434</td>\n",
              "      <td>0.973899</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>850</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.152724</td>\n",
              "      <td>0.977162</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>900</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.121889</td>\n",
              "      <td>0.970636</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>950</td>\n",
              "      <td>0.011400</td>\n",
              "      <td>0.068255</td>\n",
              "      <td>0.988581</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.000500</td>\n",
              "      <td>0.180906</td>\n",
              "      <td>0.975530</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1050</td>\n",
              "      <td>0.000100</td>\n",
              "      <td>0.181584</td>\n",
              "      <td>0.980424</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1100</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.150022</td>\n",
              "      <td>0.982055</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1150</td>\n",
              "      <td>0.000100</td>\n",
              "      <td>0.153902</td>\n",
              "      <td>0.980424</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1200</td>\n",
              "      <td>0.000100</td>\n",
              "      <td>0.130018</td>\n",
              "      <td>0.980424</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1250</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.151560</td>\n",
              "      <td>0.982055</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1300</td>\n",
              "      <td>0.000400</td>\n",
              "      <td>0.152844</td>\n",
              "      <td>0.983687</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1350</td>\n",
              "      <td>0.002000</td>\n",
              "      <td>0.207546</td>\n",
              "      <td>0.978793</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1400</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.142784</td>\n",
              "      <td>0.982055</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1450</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.156371</td>\n",
              "      <td>0.977162</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.141886</td>\n",
              "      <td>0.978793</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1550</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.173622</td>\n",
              "      <td>0.977162</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1600</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.176020</td>\n",
              "      <td>0.975530</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1650</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.147501</td>\n",
              "      <td>0.980424</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1700</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.140108</td>\n",
              "      <td>0.983687</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1750</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.127862</td>\n",
              "      <td>0.985318</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1800</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.119874</td>\n",
              "      <td>0.986949</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1850</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.142275</td>\n",
              "      <td>0.983687</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1900</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.122612</td>\n",
              "      <td>0.988581</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1950</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.128685</td>\n",
              "      <td>0.986949</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.127153</td>\n",
              "      <td>0.986949</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2050</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.123158</td>\n",
              "      <td>0.988581</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2100</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.130948</td>\n",
              "      <td>0.986949</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2150</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.115738</td>\n",
              "      <td>0.990212</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2200</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.140264</td>\n",
              "      <td>0.983687</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2250</td>\n",
              "      <td>0.000500</td>\n",
              "      <td>0.130639</td>\n",
              "      <td>0.985318</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2300</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.121154</td>\n",
              "      <td>0.990212</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "***** train metrics *****\n",
            "  epoch                    =        15.0\n",
            "  total_flos               = 366175246GF\n",
            "  train_loss               =      0.0792\n",
            "  train_runtime            =  0:14:48.86\n",
            "  train_samples_per_second =      41.362\n",
            "  train_steps_per_second   =       2.599\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(Label(value='0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max‚Ä¶"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d84568dfd5154dc28bf0b55684e317a0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>‚ñÅ‚ñÜ‚ñÜ‚ñá‚ñÖ‚ñÜ‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>eval/loss</td><td>‚ñà‚ñÉ‚ñÉ‚ñÇ‚ñÑ‚ñÑ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>eval/runtime</td><td>‚ñÉ‚ñÑ‚ñá‚ñÇ‚ñÖ‚ñÜ‚ñÇ‚ñÑ‚ñÇ‚ñà‚ñÉ‚ñÖ‚ñÖ‚ñÑ‚ñÇ‚ñÖ‚ñÜ‚ñÅ‚ñÜ‚ñÉ‚ñÖ‚ñÇ‚ñÑ‚ñÜ‚ñÇ‚ñÜ‚ñÅ‚ñÜ‚ñÉ‚ñÖ‚ñÜ‚ñÇ‚ñá‚ñÜ‚ñÖ‚ñÅ‚ñÖ‚ñÅ‚ñÑ‚ñÜ</td></tr><tr><td>eval/samples_per_second</td><td>‚ñÜ‚ñÖ‚ñÇ‚ñá‚ñÉ‚ñÉ‚ñá‚ñÖ‚ñá‚ñÅ‚ñÜ‚ñÑ‚ñÑ‚ñÖ‚ñá‚ñÑ‚ñÉ‚ñà‚ñÉ‚ñÜ‚ñÑ‚ñá‚ñÑ‚ñÉ‚ñá‚ñÉ‚ñà‚ñÉ‚ñÜ‚ñÉ‚ñÉ‚ñá‚ñÇ‚ñÉ‚ñÑ‚ñà‚ñÑ‚ñà‚ñÖ‚ñÉ</td></tr><tr><td>eval/steps_per_second</td><td>‚ñÜ‚ñÖ‚ñÇ‚ñá‚ñÉ‚ñÉ‚ñá‚ñÖ‚ñá‚ñÅ‚ñÜ‚ñÑ‚ñÑ‚ñÖ‚ñá‚ñÑ‚ñÉ‚ñà‚ñÉ‚ñÜ‚ñÑ‚ñá‚ñÖ‚ñÉ‚ñá‚ñÉ‚ñà‚ñÉ‚ñÜ‚ñÉ‚ñÉ‚ñá‚ñÇ‚ñÉ‚ñÑ‚ñà‚ñÑ‚ñà‚ñÖ‚ñÉ</td></tr><tr><td>train/epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>train/global_step</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>train/learning_rate</td><td>‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>train/loss</td><td>‚ñà‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>train/total_flos</td><td>‚ñÅ</td></tr><tr><td>train/train_loss</td><td>‚ñÅ</td></tr><tr><td>train/train_runtime</td><td>‚ñÅ</td></tr><tr><td>train/train_samples_per_second</td><td>‚ñÅ</td></tr><tr><td>train/train_steps_per_second</td><td>‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>0.99021</td></tr><tr><td>eval/loss</td><td>0.12115</td></tr><tr><td>eval/runtime</td><td>7.6554</td></tr><tr><td>eval/samples_per_second</td><td>80.074</td></tr><tr><td>eval/steps_per_second</td><td>10.058</td></tr><tr><td>train/epoch</td><td>15.0</td></tr><tr><td>train/global_step</td><td>2310</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.0</td></tr><tr><td>train/total_flos</td><td>3.9317767701037056e+17</td></tr><tr><td>train/train_loss</td><td>0.0792</td></tr><tr><td>train/train_runtime</td><td>888.8612</td></tr><tr><td>train/train_samples_per_second</td><td>41.362</td></tr><tr><td>train/train_steps_per_second</td><td>2.599</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">sparkling-violet-3</strong> at: <a href='https://wandb.ai/deep_bekeif/eff_vit_with_aug/runs/3npklywb' target=\"_blank\">https://wandb.ai/deep_bekeif/eff_vit_with_aug/runs/3npklywb</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20230706_113607-3npklywb/logs</code>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## With Augmentation"
      ],
      "metadata": {
        "id": "QkWza8S27Di_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the datasets from folders and process the dataset with the transform\n",
        "train_dataset = ImageFolder(data_path + '/brain_dataset/train', transform=train_transforms_with_aug)\n",
        "test_dataset = ImageFolder(data_path + '/brain_dataset/test', transform=test_transforms)"
      ],
      "metadata": {
        "id": "7JXZl0q87I6e"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize a new run\n",
        "\n",
        "# Define the TrainingArguments\n",
        "training_args = TrainingArguments(\n",
        "  output_dir=output_vit_no_aug,\n",
        "  per_device_train_batch_size=16,\n",
        "  evaluation_strategy=\"steps\",\n",
        "  num_train_epochs=15,\n",
        "  fp16=True,\n",
        "  save_steps=100,#100\n",
        "  eval_steps=50,#100\n",
        "  logging_steps=10,\n",
        "  learning_rate=2e-4,\n",
        "  save_total_limit=2,\n",
        "  remove_unused_columns=False,\n",
        "  push_to_hub=False,\n",
        "  report_to='wandb',\n",
        "  load_best_model_at_end=True,\n",
        "  seed = seed\n",
        ")\n",
        "\n",
        "\n",
        "trainer = Trainer(\n",
        "    model_init=model_init,\n",
        "    args=training_args,\n",
        "    compute_metrics=compute_metrics,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset,\n",
        "    data_collator= lambda samples: {'pixel_values': torch.stack([sample[0] for sample in samples]),\n",
        "                                   'labels': torch.tensor([sample[1] for sample in samples])},\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OOw_iq5D7Ob3",
        "outputId": "39d59350-4bdb-4265-f2b3-9e9d0b75366d"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at snap-research/efficientformer-l1-300 were not used when initializing EfficientFormerForImageClassification: ['distillation_classifier.bias', 'distillation_classifier.weight']\n",
            "- This IS expected if you are initializing EfficientFormerForImageClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing EfficientFormerForImageClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.init(project=\"eff_vit_with_aug\")\n",
        "#Train model:\n",
        "train_results = trainer.train()\n",
        "trainer.save_model()\n",
        "trainer.log_metrics(\"train\", train_results.metrics)\n",
        "trainer.save_metrics(\"train\", train_results.metrics)\n",
        "trainer.save_state()\n",
        "wandb.finish()"
      ],
      "metadata": {
        "id": "6J9ZcKOV7Q5V",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "590a3126-da27-469e-8f29-b23f0315cf27"
      },
      "execution_count": 22,
      "outputs": [
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.15.5"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20230706_115243-p2yt98i0</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/deep_bekeif/eff_vit_with_aug/runs/p2yt98i0' target=\"_blank\">expert-paper-4</a></strong> to <a href='https://wandb.ai/deep_bekeif/eff_vit_with_aug' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/deep_bekeif/eff_vit_with_aug' target=\"_blank\">https://wandb.ai/deep_bekeif/eff_vit_with_aug</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/deep_bekeif/eff_vit_with_aug/runs/p2yt98i0' target=\"_blank\">https://wandb.ai/deep_bekeif/eff_vit_with_aug/runs/p2yt98i0</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at snap-research/efficientformer-l1-300 were not used when initializing EfficientFormerForImageClassification: ['distillation_classifier.bias', 'distillation_classifier.weight']\n",
            "- This IS expected if you are initializing EfficientFormerForImageClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing EfficientFormerForImageClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1951' max='2310' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1951/2310 14:28 < 02:40, 2.24 it/s, Epoch 12.66/15]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.520500</td>\n",
              "      <td>0.653517</td>\n",
              "      <td>0.787928</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.498200</td>\n",
              "      <td>0.269240</td>\n",
              "      <td>0.910277</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>0.332700</td>\n",
              "      <td>0.560298</td>\n",
              "      <td>0.817292</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.195800</td>\n",
              "      <td>0.319811</td>\n",
              "      <td>0.893964</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>0.324300</td>\n",
              "      <td>0.201889</td>\n",
              "      <td>0.923328</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>0.200900</td>\n",
              "      <td>0.265060</td>\n",
              "      <td>0.923328</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>350</td>\n",
              "      <td>0.262600</td>\n",
              "      <td>0.291674</td>\n",
              "      <td>0.920065</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>0.118300</td>\n",
              "      <td>0.097557</td>\n",
              "      <td>0.959217</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>450</td>\n",
              "      <td>0.132900</td>\n",
              "      <td>0.211417</td>\n",
              "      <td>0.941272</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.069200</td>\n",
              "      <td>0.105135</td>\n",
              "      <td>0.973899</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>550</td>\n",
              "      <td>0.090000</td>\n",
              "      <td>0.169374</td>\n",
              "      <td>0.960848</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>0.060800</td>\n",
              "      <td>0.166781</td>\n",
              "      <td>0.957586</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>650</td>\n",
              "      <td>0.115800</td>\n",
              "      <td>0.081715</td>\n",
              "      <td>0.978793</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>0.047400</td>\n",
              "      <td>0.091550</td>\n",
              "      <td>0.980424</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>750</td>\n",
              "      <td>0.067400</td>\n",
              "      <td>0.166789</td>\n",
              "      <td>0.960848</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>0.025800</td>\n",
              "      <td>0.130996</td>\n",
              "      <td>0.973899</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>850</td>\n",
              "      <td>0.030500</td>\n",
              "      <td>0.126761</td>\n",
              "      <td>0.972268</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>900</td>\n",
              "      <td>0.067600</td>\n",
              "      <td>0.161172</td>\n",
              "      <td>0.965742</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>950</td>\n",
              "      <td>0.003800</td>\n",
              "      <td>0.159876</td>\n",
              "      <td>0.969005</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.002000</td>\n",
              "      <td>0.181216</td>\n",
              "      <td>0.967374</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1050</td>\n",
              "      <td>0.044800</td>\n",
              "      <td>0.147842</td>\n",
              "      <td>0.969005</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1100</td>\n",
              "      <td>0.027500</td>\n",
              "      <td>0.147781</td>\n",
              "      <td>0.975530</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1150</td>\n",
              "      <td>0.059500</td>\n",
              "      <td>0.127596</td>\n",
              "      <td>0.975530</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1200</td>\n",
              "      <td>0.001700</td>\n",
              "      <td>0.131664</td>\n",
              "      <td>0.975530</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1250</td>\n",
              "      <td>0.059700</td>\n",
              "      <td>0.117599</td>\n",
              "      <td>0.972268</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1300</td>\n",
              "      <td>0.013900</td>\n",
              "      <td>0.094734</td>\n",
              "      <td>0.980424</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1350</td>\n",
              "      <td>0.012100</td>\n",
              "      <td>0.100977</td>\n",
              "      <td>0.978793</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1400</td>\n",
              "      <td>0.000700</td>\n",
              "      <td>0.095023</td>\n",
              "      <td>0.985318</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1450</td>\n",
              "      <td>0.000300</td>\n",
              "      <td>0.088430</td>\n",
              "      <td>0.983687</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>0.000900</td>\n",
              "      <td>0.089859</td>\n",
              "      <td>0.983687</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1550</td>\n",
              "      <td>0.000400</td>\n",
              "      <td>0.112063</td>\n",
              "      <td>0.980424</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1600</td>\n",
              "      <td>0.000600</td>\n",
              "      <td>0.084463</td>\n",
              "      <td>0.983687</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1650</td>\n",
              "      <td>0.002800</td>\n",
              "      <td>0.086946</td>\n",
              "      <td>0.980424</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1700</td>\n",
              "      <td>0.034500</td>\n",
              "      <td>0.094177</td>\n",
              "      <td>0.980424</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1750</td>\n",
              "      <td>0.004100</td>\n",
              "      <td>0.103019</td>\n",
              "      <td>0.977162</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1800</td>\n",
              "      <td>0.014300</td>\n",
              "      <td>0.109625</td>\n",
              "      <td>0.973899</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1850</td>\n",
              "      <td>0.219400</td>\n",
              "      <td>0.092211</td>\n",
              "      <td>0.982055</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1900</td>\n",
              "      <td>0.000500</td>\n",
              "      <td>0.108257</td>\n",
              "      <td>0.980424</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>\n",
              "    <div>\n",
              "      \n",
              "      <progress value='10' max='77' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [10/77 00:00 < 00:06, 10.26 it/s]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2310' max='2310' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2310/2310 17:17, Epoch 15/15]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.520500</td>\n",
              "      <td>0.653517</td>\n",
              "      <td>0.787928</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.498200</td>\n",
              "      <td>0.269240</td>\n",
              "      <td>0.910277</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>0.332700</td>\n",
              "      <td>0.560298</td>\n",
              "      <td>0.817292</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.195800</td>\n",
              "      <td>0.319811</td>\n",
              "      <td>0.893964</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>0.324300</td>\n",
              "      <td>0.201889</td>\n",
              "      <td>0.923328</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>0.200900</td>\n",
              "      <td>0.265060</td>\n",
              "      <td>0.923328</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>350</td>\n",
              "      <td>0.262600</td>\n",
              "      <td>0.291674</td>\n",
              "      <td>0.920065</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>0.118300</td>\n",
              "      <td>0.097557</td>\n",
              "      <td>0.959217</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>450</td>\n",
              "      <td>0.132900</td>\n",
              "      <td>0.211417</td>\n",
              "      <td>0.941272</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.069200</td>\n",
              "      <td>0.105135</td>\n",
              "      <td>0.973899</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>550</td>\n",
              "      <td>0.090000</td>\n",
              "      <td>0.169374</td>\n",
              "      <td>0.960848</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>0.060800</td>\n",
              "      <td>0.166781</td>\n",
              "      <td>0.957586</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>650</td>\n",
              "      <td>0.115800</td>\n",
              "      <td>0.081715</td>\n",
              "      <td>0.978793</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>0.047400</td>\n",
              "      <td>0.091550</td>\n",
              "      <td>0.980424</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>750</td>\n",
              "      <td>0.067400</td>\n",
              "      <td>0.166789</td>\n",
              "      <td>0.960848</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>0.025800</td>\n",
              "      <td>0.130996</td>\n",
              "      <td>0.973899</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>850</td>\n",
              "      <td>0.030500</td>\n",
              "      <td>0.126761</td>\n",
              "      <td>0.972268</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>900</td>\n",
              "      <td>0.067600</td>\n",
              "      <td>0.161172</td>\n",
              "      <td>0.965742</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>950</td>\n",
              "      <td>0.003800</td>\n",
              "      <td>0.159876</td>\n",
              "      <td>0.969005</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.002000</td>\n",
              "      <td>0.181216</td>\n",
              "      <td>0.967374</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1050</td>\n",
              "      <td>0.044800</td>\n",
              "      <td>0.147842</td>\n",
              "      <td>0.969005</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1100</td>\n",
              "      <td>0.027500</td>\n",
              "      <td>0.147781</td>\n",
              "      <td>0.975530</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1150</td>\n",
              "      <td>0.059500</td>\n",
              "      <td>0.127596</td>\n",
              "      <td>0.975530</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1200</td>\n",
              "      <td>0.001700</td>\n",
              "      <td>0.131664</td>\n",
              "      <td>0.975530</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1250</td>\n",
              "      <td>0.059700</td>\n",
              "      <td>0.117599</td>\n",
              "      <td>0.972268</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1300</td>\n",
              "      <td>0.013900</td>\n",
              "      <td>0.094734</td>\n",
              "      <td>0.980424</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1350</td>\n",
              "      <td>0.012100</td>\n",
              "      <td>0.100977</td>\n",
              "      <td>0.978793</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1400</td>\n",
              "      <td>0.000700</td>\n",
              "      <td>0.095023</td>\n",
              "      <td>0.985318</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1450</td>\n",
              "      <td>0.000300</td>\n",
              "      <td>0.088430</td>\n",
              "      <td>0.983687</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>0.000900</td>\n",
              "      <td>0.089859</td>\n",
              "      <td>0.983687</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1550</td>\n",
              "      <td>0.000400</td>\n",
              "      <td>0.112063</td>\n",
              "      <td>0.980424</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1600</td>\n",
              "      <td>0.000600</td>\n",
              "      <td>0.084463</td>\n",
              "      <td>0.983687</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1650</td>\n",
              "      <td>0.002800</td>\n",
              "      <td>0.086946</td>\n",
              "      <td>0.980424</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1700</td>\n",
              "      <td>0.034500</td>\n",
              "      <td>0.094177</td>\n",
              "      <td>0.980424</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1750</td>\n",
              "      <td>0.004100</td>\n",
              "      <td>0.103019</td>\n",
              "      <td>0.977162</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1800</td>\n",
              "      <td>0.014300</td>\n",
              "      <td>0.109625</td>\n",
              "      <td>0.973899</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1850</td>\n",
              "      <td>0.219400</td>\n",
              "      <td>0.092211</td>\n",
              "      <td>0.982055</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1900</td>\n",
              "      <td>0.000500</td>\n",
              "      <td>0.108257</td>\n",
              "      <td>0.980424</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1950</td>\n",
              "      <td>0.000200</td>\n",
              "      <td>0.091216</td>\n",
              "      <td>0.982055</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>0.000100</td>\n",
              "      <td>0.088663</td>\n",
              "      <td>0.985318</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2050</td>\n",
              "      <td>0.005700</td>\n",
              "      <td>0.085337</td>\n",
              "      <td>0.982055</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2100</td>\n",
              "      <td>0.034100</td>\n",
              "      <td>0.087938</td>\n",
              "      <td>0.982055</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2150</td>\n",
              "      <td>0.022500</td>\n",
              "      <td>0.078990</td>\n",
              "      <td>0.983687</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2200</td>\n",
              "      <td>0.017600</td>\n",
              "      <td>0.078148</td>\n",
              "      <td>0.982055</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2250</td>\n",
              "      <td>0.000100</td>\n",
              "      <td>0.072188</td>\n",
              "      <td>0.986949</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2300</td>\n",
              "      <td>0.000400</td>\n",
              "      <td>0.073737</td>\n",
              "      <td>0.980424</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "***** train metrics *****\n",
            "  epoch                    =        15.0\n",
            "  total_flos               = 366175246GF\n",
            "  train_loss               =      0.1281\n",
            "  train_runtime            =  0:17:18.24\n",
            "  train_samples_per_second =      35.411\n",
            "  train_steps_per_second   =       2.225\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>‚ñÅ‚ñÖ‚ñÇ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñà‚ñá‚ñá‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>eval/loss</td><td>‚ñà‚ñÉ‚ñá‚ñÑ‚ñÉ‚ñÉ‚ñÑ‚ñÉ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>eval/runtime</td><td>‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÉ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñà‚ñÇ‚ñÉ‚ñÇ‚ñÉ‚ñÑ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÇ</td></tr><tr><td>eval/samples_per_second</td><td>‚ñà‚ñá‚ñá‚ñà‚ñá‚ñà‚ñÜ‚ñá‚ñÜ‚ñá‚ñÖ‚ñÜ‚ñá‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñÜ‚ñá‚ñÜ‚ñà‚ñá‚ñÜ‚ñà‚ñÅ‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñÖ‚ñá‚ñÜ‚ñá‚ñÖ‚ñá‚ñÜ‚ñá‚ñá</td></tr><tr><td>eval/steps_per_second</td><td>‚ñà‚ñá‚ñá‚ñà‚ñá‚ñà‚ñÜ‚ñá‚ñÜ‚ñá‚ñÖ‚ñÜ‚ñá‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñÜ‚ñá‚ñÜ‚ñà‚ñá‚ñÜ‚ñà‚ñÅ‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñÖ‚ñá‚ñÜ‚ñá‚ñÖ‚ñá‚ñÜ‚ñá‚ñá</td></tr><tr><td>train/epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>train/global_step</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>train/learning_rate</td><td>‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>train/loss</td><td>‚ñà‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>train/total_flos</td><td>‚ñÅ</td></tr><tr><td>train/train_loss</td><td>‚ñÅ</td></tr><tr><td>train/train_runtime</td><td>‚ñÅ</td></tr><tr><td>train/train_samples_per_second</td><td>‚ñÅ</td></tr><tr><td>train/train_steps_per_second</td><td>‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>0.98042</td></tr><tr><td>eval/loss</td><td>0.07374</td></tr><tr><td>eval/runtime</td><td>7.9677</td></tr><tr><td>eval/samples_per_second</td><td>76.936</td></tr><tr><td>eval/steps_per_second</td><td>9.664</td></tr><tr><td>train/epoch</td><td>15.0</td></tr><tr><td>train/global_step</td><td>2310</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.0</td></tr><tr><td>train/total_flos</td><td>3.9317767701037056e+17</td></tr><tr><td>train/train_loss</td><td>0.12808</td></tr><tr><td>train/train_runtime</td><td>1038.2438</td></tr><tr><td>train/train_samples_per_second</td><td>35.411</td></tr><tr><td>train/train_steps_per_second</td><td>2.225</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">expert-paper-4</strong> at: <a href='https://wandb.ai/deep_bekeif/eff_vit_with_aug/runs/p2yt98i0' target=\"_blank\">https://wandb.ai/deep_bekeif/eff_vit_with_aug/runs/p2yt98i0</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20230706_115243-p2yt98i0/logs</code>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "e8pOlK68_21b",
        "GtJNGP6yS9I8",
        "5C9ZT2fPUbqc",
        "4cZvzMmILYsi",
        "krzSz6X_UuhZ",
        "B0QUfYorUxfu"
      ],
      "provenance": [],
      "gpuType": "A100",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "7b0b645b32144ccf810f3a15796ef66c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1e4a25437b634ef894b21972431432b4",
              "IPY_MODEL_e936fd6952fe41dca9ffe96a6450787b"
            ],
            "layout": "IPY_MODEL_06ce33f8bd3048248a7076bd1515d46b"
          }
        },
        "1e4a25437b634ef894b21972431432b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7a130b04a5ce4dce99eb61334816e362",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_a08c3126a6684d97a0c2f6517ade5eed",
            "value": "0.002 MB of 0.019 MB uploaded (0.000 MB deduped)\r"
          }
        },
        "e936fd6952fe41dca9ffe96a6450787b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_19061d6836ed4fa697b1f8697e7d46d9",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9b5bf20b83cc4d7496466d045034ea9d",
            "value": 0.10976948408342481
          }
        },
        "06ce33f8bd3048248a7076bd1515d46b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7a130b04a5ce4dce99eb61334816e362": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a08c3126a6684d97a0c2f6517ade5eed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "19061d6836ed4fa697b1f8697e7d46d9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9b5bf20b83cc4d7496466d045034ea9d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fe15697ef5e440978bf3a7d292eee167": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_08f9bf8e83d14bed971527a6b42de233",
              "IPY_MODEL_5ce6178a83a7417caa7d12edd8f605f0"
            ],
            "layout": "IPY_MODEL_a2989b368b6f4714a54e3bf44883dea9"
          }
        },
        "08f9bf8e83d14bed971527a6b42de233": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f3716049b140430390a8d75741b8a175",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_aaa7f0909c614786aa66319c466c44d7",
            "value": "0.002 MB of 0.020 MB uploaded (0.000 MB deduped)\r"
          }
        },
        "5ce6178a83a7417caa7d12edd8f605f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8cf4b273c4e144eba1cda2c7a1a1aac1",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_870a405122b049b8baf6fafce5e7f32f",
            "value": 0.10532867333748265
          }
        },
        "a2989b368b6f4714a54e3bf44883dea9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f3716049b140430390a8d75741b8a175": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aaa7f0909c614786aa66319c466c44d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8cf4b273c4e144eba1cda2c7a1a1aac1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "870a405122b049b8baf6fafce5e7f32f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c274518a78be489c8e37f2d713adce2e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_66ceb6d5524248d9a321cd6a6014c2ff",
              "IPY_MODEL_b1fef4bc421d465caa1dbeb97a65bba3",
              "IPY_MODEL_6f8e010856cd4940b859b5d1a06ad0a3"
            ],
            "layout": "IPY_MODEL_a9f7f4897e95436897024426d20d324e"
          }
        },
        "66ceb6d5524248d9a321cd6a6014c2ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7bfc198d6c554b219865e00e2ba5cc45",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_058757c72182488e8e5ee9db427adc81",
            "value": "Downloading builder script: "
          }
        },
        "b1fef4bc421d465caa1dbeb97a65bba3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b281060b8107458bac092186336962d9",
            "max": 1652,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0a1492b49ca749b38f43804366a93761",
            "value": 1652
          }
        },
        "6f8e010856cd4940b859b5d1a06ad0a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b6e37bdf317b4f1d82afd5cd1d276dbf",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_376501286ad042419cd58399eef26929",
            "value": " 4.21k/? [00:00&lt;00:00, 275kB/s]"
          }
        },
        "a9f7f4897e95436897024426d20d324e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7bfc198d6c554b219865e00e2ba5cc45": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "058757c72182488e8e5ee9db427adc81": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b281060b8107458bac092186336962d9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0a1492b49ca749b38f43804366a93761": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b6e37bdf317b4f1d82afd5cd1d276dbf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "376501286ad042419cd58399eef26929": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d84568dfd5154dc28bf0b55684e317a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0b071901c80a442a8f77d43712f8bfd0",
              "IPY_MODEL_f182459aef0e4bd0aa38bfd5b9e6e1fb"
            ],
            "layout": "IPY_MODEL_86ec1480b3bc41d098970a75294ab83d"
          }
        },
        "0b071901c80a442a8f77d43712f8bfd0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_daae331cb81c493da12822750e7cb8ba",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_55d3d2c481564c5db094b016db76ff4b",
            "value": "0.035 MB of 0.035 MB uploaded (0.000 MB deduped)\r"
          }
        },
        "f182459aef0e4bd0aa38bfd5b9e6e1fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0b146b7f58a44e9aa5f2644f0aeea8bd",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6d82c55b9be745cba015af009c81f364",
            "value": 1
          }
        },
        "86ec1480b3bc41d098970a75294ab83d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "daae331cb81c493da12822750e7cb8ba": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "55d3d2c481564c5db094b016db76ff4b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0b146b7f58a44e9aa5f2644f0aeea8bd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6d82c55b9be745cba015af009c81f364": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}